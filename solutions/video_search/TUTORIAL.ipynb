{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a213cfee",
   "metadata": {},
   "source": [
    "# Video search\n",
    "In this example we will be going over the code required to perform video search. This example uses a Vgg model to extract video features that are then used with Milvus to build a system that can perform the searches. \n",
    "## Data\n",
    "\n",
    "This example uses Tumblr's 10 animated gifs as an example to build an end-to-end solution that uses image search video. Readers can use your own video files to build the system.\n",
    "\n",
    "Download location: https://drive.google.com/file/d/1hS4ANTQx9xNr9AByiLVeA1rEnxycdxtZ/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102f770",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "| Python Packages | Docker Servers |\n",
    "| --------------- | -------------- |\n",
    "| PyMilvus        | Milvus-1.0.0   |\n",
    "| Redis           | Redis          |\n",
    "| Minio           | Minio          |\n",
    "\n",
    "We will assume that you have familiarity with libraries including PyTorch, TorchVision, MatPlotLib, and PIL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d0fe1",
   "metadata": {},
   "source": [
    "## Up and Running\n",
    "\n",
    "\n",
    "### 1. Start Milvus Server\n",
    "\n",
    "```bash\n",
    "$  docker run -d --name milvus_cpu_1.0.0 --network my-net --ip 10.0.0.2 \\\n",
    "-p 19530:19530 \\\n",
    "-p 19121:19121 \\\n",
    "-v /home/$USER/milvus/db:/var/lib/milvus/db \\\n",
    "-v /home/$USER/milvus/conf:/var/lib/milvus/conf \\\n",
    "-v /home/$USER/milvus/logs:/var/lib/milvus/logs \\\n",
    "-v /home/$USER/milvus/wal:/var/lib/milvus/wal \\\n",
    "milvusdb/milvus:1.0.0-cpu-d030521-1ea92e\n",
    "```\n",
    "\n",
    "This demo uses Milvus 1.0. Refer to the [Install Milvus](https://milvus.io/docs/v1.0.0/milvus_docker-cpu.md) for how to install Milvus docker. \n",
    "\n",
    "### 2. Start Redis Server\n",
    "\n",
    "```bash\n",
    "$ docker run --name some-redis -d redis\n",
    "```\n",
    "\n",
    "We are using Redis as a metadata storage service. Code can easily be modified to use python dictionary, but that usually does not work in any use case outside of quick examples.\n",
    "\n",
    "### 3. Start Minio Server\n",
    "```bash\n",
    "$ docker run --name some-minio -d minio -p 9000:9000\n",
    "```\n",
    "We are using Minio to save raw videos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b80af",
   "metadata": {},
   "source": [
    "## Code Overview\n",
    "### Connecting to Servers\n",
    "\n",
    "We first start off by connecting to the servers. In this case the docker containers are running on localhost and the ports are the default ports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d15825-1a18-45d0-9b11-559886c2e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connectings to Milvus and Redis\n",
    "\n",
    "import redis\n",
    "import milvus\n",
    "\n",
    "milv = milvus.Milvus(host = '127.0.0.1', port = 19530)\n",
    "red = redis.Redis(host = '127.0.0.1', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85801f40",
   "metadata": {},
   "source": [
    "### Building Collection and Setting Index\n",
    "\n",
    "The next step involves creating a collection. A collection in Milvus is similar to a table in a relational database, and is used for storing all the vectors. To create a collection, we first must select a name, the dimension of the vectors being stored within, the index_file_size, and metric_type. The index_file_size corresponds to how large each data segmet will be within the collection. More information on this can be found here. The metric_type is the distance formula being used to calculate similarity. In this example we are using the Euclidean distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db730ba7-618e-40c4-9f9f-a45e7dbf3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating collection\n",
    "\n",
    "import time\n",
    "\n",
    "collection_name = \"test_collection\"\n",
    "milv.drop_collection(collection_name) \n",
    "red.flushdb()\n",
    "time.sleep(.1)\n",
    "\n",
    "collection_param = {\n",
    "            'collection_name': collection_name,\n",
    "            'dimension': 512,\n",
    "            'index_file_size': 1024,  # optional\n",
    "            'metric_type': milvus.MetricType.L2  # optional\n",
    "            }\n",
    "\n",
    "status, ok = milv.has_collection(collection_name)\n",
    "\n",
    "if not ok:\n",
    "    status = milv.create_collection(collection_param)\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cf359",
   "metadata": {},
   "source": [
    "After creating the collection we want to assign it an index type. This can be done before or after inserting the data. When done before, indexes will be made as data comes in and fills the data segments. In this example we are using IVF_SQ8 which requires the 'nlist' parameter. Each index types carries its own parameters. More info about this param can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe53c8-4752-4872-914f-4d49096992d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing collection\n",
    "\n",
    "index_param = {\n",
    "    'nlist': 512\n",
    "}\n",
    "\n",
    "status = milv.create_index(collection_name, milvus.IndexType.IVF_SQ8, index_param)\n",
    "status, index = milv.get_index_info(collection_name)\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36cdc6",
   "metadata": {},
   "source": [
    "### Processing and Storing Videos\n",
    "\n",
    "In order to store the videos in Milvus, We first need to cut the frame of the video, here we choose the opencv method.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24664b23-94df-41ab-ab7a-73c12ba75d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tumblr_ll0clfX9hx1qdnpeio1_500.gif', 'tumblr_lhrmcm5Wcs1qas1v4o1_400.gif', 'tumblr_lkrdoicnct1qh7pxno1_500.gif', 'tumblr_liirx1mwHB1qbyvbvo1_500.gif', 'tumblr_lkkipqqeoc1qa7tfso1_500.gif', 'tumblr_lgs5577Bmq1qf1oxoo1_500.gif', 'tumblr_lhctxtXNwX1qavsh3o1_500.gif', 'tumblr_lkji6jwrTV1qerwoao1_250.gif', 'tumblr_lhns1x9P9d1qc3h7bo1_400.gif', 'tumblr_lixf5lzpf61qesrtzo1_250.gif']\n",
      "tumblr_ll0clfX9hx1qdnpeio1_500.gif\n",
      "tumblr_ll0clfX9hx1qdnpeio1_500\n",
      "tumblr_ll0clfX9hx1qdnpeio1_500\n",
      "tumblr_ll0clfX9hx1qdnpeio1_500\n",
      "tumblr_ll0clfX9hx1qdnpeio1_500\n",
      "Total frames:  4\n",
      "tumblr_lhrmcm5Wcs1qas1v4o1_400.gif\n",
      "tumblr_lhrmcm5Wcs1qas1v4o1_400\n",
      "tumblr_lhrmcm5Wcs1qas1v4o1_400\n",
      "tumblr_lhrmcm5Wcs1qas1v4o1_400\n",
      "tumblr_lhrmcm5Wcs1qas1v4o1_400\n",
      "Total frames:  4\n",
      "tumblr_lkrdoicnct1qh7pxno1_500.gif\n",
      "tumblr_lkrdoicnct1qh7pxno1_500\n",
      "tumblr_lkrdoicnct1qh7pxno1_500\n",
      "tumblr_lkrdoicnct1qh7pxno1_500\n",
      "Total frames:  3\n",
      "tumblr_liirx1mwHB1qbyvbvo1_500.gif\n",
      "tumblr_liirx1mwHB1qbyvbvo1_500\n",
      "tumblr_liirx1mwHB1qbyvbvo1_500\n",
      "tumblr_liirx1mwHB1qbyvbvo1_500\n",
      "Total frames:  3\n",
      "tumblr_lkkipqqeoc1qa7tfso1_500.gif\n",
      "tumblr_lkkipqqeoc1qa7tfso1_500\n",
      "tumblr_lkkipqqeoc1qa7tfso1_500\n",
      "tumblr_lkkipqqeoc1qa7tfso1_500\n",
      "tumblr_lkkipqqeoc1qa7tfso1_500\n",
      "Total frames:  4\n",
      "tumblr_lgs5577Bmq1qf1oxoo1_500.gif\n",
      "tumblr_lgs5577Bmq1qf1oxoo1_500\n",
      "tumblr_lgs5577Bmq1qf1oxoo1_500\n",
      "tumblr_lgs5577Bmq1qf1oxoo1_500\n",
      "tumblr_lgs5577Bmq1qf1oxoo1_500\n",
      "Total frames:  4\n",
      "tumblr_lhctxtXNwX1qavsh3o1_500.gif\n",
      "tumblr_lhctxtXNwX1qavsh3o1_500\n",
      "tumblr_lhctxtXNwX1qavsh3o1_500\n",
      "tumblr_lhctxtXNwX1qavsh3o1_500\n",
      "tumblr_lhctxtXNwX1qavsh3o1_500\n",
      "Total frames:  4\n",
      "tumblr_lkji6jwrTV1qerwoao1_250.gif\n",
      "tumblr_lkji6jwrTV1qerwoao1_250\n",
      "tumblr_lkji6jwrTV1qerwoao1_250\n",
      "tumblr_lkji6jwrTV1qerwoao1_250\n",
      "tumblr_lkji6jwrTV1qerwoao1_250\n",
      "tumblr_lkji6jwrTV1qerwoao1_250\n",
      "tumblr_lkji6jwrTV1qerwoao1_250\n",
      "tumblr_lkji6jwrTV1qerwoao1_250\n",
      "Total frames:  7\n",
      "tumblr_lhns1x9P9d1qc3h7bo1_400.gif\n",
      "tumblr_lhns1x9P9d1qc3h7bo1_400\n",
      "tumblr_lhns1x9P9d1qc3h7bo1_400\n",
      "tumblr_lhns1x9P9d1qc3h7bo1_400\n",
      "tumblr_lhns1x9P9d1qc3h7bo1_400\n",
      "tumblr_lhns1x9P9d1qc3h7bo1_400\n",
      "Total frames:  5\n",
      "tumblr_lixf5lzpf61qesrtzo1_250.gif\n",
      "tumblr_lixf5lzpf61qesrtzo1_250\n",
      "tumblr_lixf5lzpf61qesrtzo1_250\n",
      "tumblr_lixf5lzpf61qesrtzo1_250\n",
      "tumblr_lixf5lzpf61qesrtzo1_250\n",
      "Total frames:  4\n"
     ]
    }
   ],
   "source": [
    "import cv2,os\n",
    "save_path = \"/data1/lcl/test_video_bootcamp/frame_res/\"      #存储的位置\n",
    "path = \"/data1/lcl/test_video_bootcamp/examle-gif-10/10-gif/\"    #要截取视频的文件夹\n",
    "\n",
    "filelist = os.listdir(path)     #读取文件夹下的全部文件\n",
    "print(filelist)     \n",
    "for item in filelist:  \n",
    "    if item.endswith('.gif'):     #根据本身的视频文件后缀来写，个人视频文件是gif格式\n",
    "        print(item)\n",
    "        try:\n",
    "            src = os.path.join(path, item)\n",
    "            vid_cap = cv2.VideoCapture(src)    #传入视频的路径\n",
    "            success, image = vid_cap.read()\n",
    "            count = 0\n",
    "            while success:\n",
    "                vid_cap.set(cv2.CAP_PROP_POS_MSEC, 1 * 1000 * count)   #截取图片的方法  此处是 1 秒截取一个  能够改变参数设置截取间隔的时间\n",
    "                video_to_picture_path= os.path.join(save_path, item.split(\".\")[0])    #视频文件夹的命名\n",
    "                if not os.path.exists(video_to_picture_path):   #建立每个视频存储图片对应的文件夹\n",
    "                    os.makedirs(video_to_picture_path)\n",
    "                cv2.imwrite(video_to_picture_path+\"/\" + str(item.split(\".\")[0]) + \"#\" + str(count) + \".jpg\", image)       #存储图片的地址 以及对图片的命名\n",
    "                success, image = vid_cap.read()\n",
    "                count += 1\n",
    "            print('Total frames: ', count)     #打印截取的图片数目 \n",
    "        except:\n",
    "            print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a3567",
   "metadata": {},
   "source": [
    "In this example we are also using a slightly modified dataloader that also returns the file path of the image. With this dataloader we are also transforming the images into what ResNet model takes as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a689e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        return super(ImageFolderWithPaths, self).__getitem__(index) + (self.imgs[index][0],)\n",
    "\n",
    "dataset = ImageFolderWithPaths(data_dir, transform=transforms.Compose([\n",
    "                                                transforms.Resize(256),\n",
    "                                                transforms.CenterCrop(224),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, num_workers=0, batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c8168",
   "metadata": {},
   "source": [
    "Inputting the data involves three major steps. First, the images need to be run through the model. This outputs vectors for each image. Second, these vectors are pushed into Milvus. Milvus then returns the corresponding IDs for the vectors. Third, these IDs and the image filepaths are used as the key and value for storage in Redis. Redis is used so that we can return the original image as a result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ac471-f13f-4bad-8077-f85256b6cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing and storing\n",
    "\n",
    "for inputs, labels, paths in dataloader:\n",
    "    with torch.no_grad():\n",
    "        output = encoder(inputs).squeeze()\n",
    "        output = output.numpy()\n",
    "\n",
    "    status, ids = milv.insert(collection_name=collection_name, records=output)\n",
    "\n",
    "    if not status.OK():\n",
    "        print(\"Insert failed: {}\".format(status))\n",
    "    else:\n",
    "        for x in range(len(ids)):\n",
    "            red.set(str(ids[x]), paths[x])\n",
    "        print(\"Added: \" + str(len(ids)) + \" vectors into: \" + collection_name + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc8e22-8f5b-4bfc-96e0-22eafe0bc0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper display function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def show_results(query, results, distances):\n",
    "    \n",
    "    fig_query, ax_query = plt.subplots(1,1, figsize=(5,5))\n",
    "    ax_query.imshow(Image.open(query))\n",
    "    ax_query.axis('off')\n",
    "    ax_query.set_title(\"Searched Image\")\n",
    "    \n",
    "    res_count = len(results)\n",
    "    fig, ax = plt.subplots(1,res_count,figsize=(5,5))\n",
    "    for x in range(res_count):\n",
    "        ax[x].imshow(Image.open(results[x]))\n",
    "        ax[x].axis('off')\n",
    "        dist =  str(distances[x])\n",
    "        dist = dist[0:dist.find('.')+4]\n",
    "        ax[x].set_title(\"D: \" +dist)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7d637-73ca-4928-afa4-eeaf12d2ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling out random images to search\n",
    "\n",
    "random_ids = [int(red.randomkey()) for x in range(10)]\n",
    "search_images = [x.decode(\"utf-8\") for x in red.mget(random_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62bcbc",
   "metadata": {},
   "source": [
    "### Searching\n",
    "\n",
    "When searching for an image, we first put the image through the same transformations as the ones used for storing the images. Once transformed, we run the image through the ResNet to get the corresponding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d58d45-775e-453d-a4e2-80ebfefda180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing query images and searching\n",
    "\n",
    "transform_ops = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "embeddings = [transform_ops(Image.open(x)) for x in search_images]\n",
    "embeddings = torch.stack(embeddings, dim=0)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    embeddings = encoder(embeddings).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68544d93",
   "metadata": {},
   "source": [
    "Then we can use these embeddings in a search. The search requires a few arguments. It needs the name of the collection, the vectors being searched for, how many closest vectors to be returned, and the parameters for the index, in this case nprobe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sub_param = {\n",
    "        \"nprobe\": 16\n",
    "    }\n",
    "\n",
    "search_param = {\n",
    "    'collection_name': collection_name,\n",
    "    'query_records': embeddings,\n",
    "    'top_k': 3,\n",
    "    'params': search_sub_param,\n",
    "    }\n",
    "\n",
    "start = time.time()\n",
    "status, results = milv.search(**search_param)\n",
    "end = time.time() - start\n",
    "\n",
    "print(\"Search took a total of: \", end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b735af",
   "metadata": {},
   "source": [
    "The result of this search contains the IDs and corresponding distances of the top_k closes vectors. We can use the IDs in Redis to get the original image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if status.OK():\n",
    "    for x in range(len(results)):\n",
    "        query_file = search_images[x]\n",
    "        result_files = [red.get(y.id).decode('utf-8') for y in results[x]]\n",
    "        distances = [y.distance for y in results[x]]\n",
    "        show_results(query_file, result_files, distances)\n",
    "else:\n",
    "    print(\"Search Failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1ac1c6",
   "metadata": {},
   "source": [
    "This is the basic way to do a reverse image search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
