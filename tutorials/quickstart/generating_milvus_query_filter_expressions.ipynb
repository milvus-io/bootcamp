{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec848c59-80b7-4044-909e-c180549f6a05",
   "metadata": {},
   "source": [
    "# Generating Milvus Query Filter Expressions with Large Language Models\n",
    "\n",
    "In this tutorial, we will demonstrate how to use Large Language Models (LLMs) to automatically generate Milvus filter expressions from natural language queries. This approach makes vector database querying more accessible by allowing users to express complex filtering conditions in plain English, which are then converted to proper Milvus syntax.\n",
    "\n",
    "Milvus supports sophisticated filtering capabilities including:\n",
    "\n",
    "* **Basic Operators**: Comparison operators like `==`, `!=`, `>`, `<`, `>=`, `<=`\n",
    "* **Boolean Operators**: Logical operators like `and`, `or`, `not` for complex conditions\n",
    "* **String Operations**: Pattern matching with `like` and other string functions\n",
    "* **Array Operations**: Working with array fields using `array_contains`, `array_length`, etc.\n",
    "* **JSON Operations**: Querying JSON fields with specialized operators\n",
    "\n",
    "By integrating LLMs with Milvus documentation, we can create an intelligent system that understands natural language queries and generates syntactically correct filter expressions. This tutorial will walk through the process of setting up this system, highlighting its effectiveness in various filtering scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a03be4-1206-4eae-9712-422dc539178a",
   "metadata": {},
   "source": [
    "## Dependencies and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f770d54-3153-4069-80fe-36a5157c0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pymilvus openai requests docling beautifulsoup4\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c80f0-fabe-470c-b6ba-5f82a378c077",
   "metadata": {},
   "source": [
    "## Set up environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba410c-7177-41ad-b0f6-80d8f74eeb01",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Configure your OpenAI API credentials to enable embedding generation and LLM-based filter expression creation. Replace `'your_openai_api_key'` with your actual OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec824b5d-f182-4ccd-b0db-b8096032817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable!\")\n",
    "\n",
    "openai.api_key = api_key\n",
    "print(\"API key loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfafbe-23b1-4bc1-a31c-de1b29dc20e8",
   "metadata": {},
   "source": [
    "## Create a Sample Collection\n",
    "\n",
    "Now let's create a sample Milvus collection with user data. This collection will contain both scalar fields (for filtering) and vector embeddings (for semantic search). We'll use OpenAI's text embedding model to generate vector representations of user information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cb4de-8778-4a25-9084-34c9b9ce389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import uuid\n",
    "\n",
    "client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "embedding_dim = 1536\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(\n",
    "        name=\"pk\",\n",
    "        dtype=DataType.VARCHAR,\n",
    "        is_primary=True,\n",
    "        auto_id=False,\n",
    "        max_length=100,\n",
    "    ),\n",
    "    FieldSchema(name=\"name\", dtype=DataType.VARCHAR, max_length=128),\n",
    "    FieldSchema(name=\"age\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"city\", dtype=DataType.VARCHAR, max_length=128),\n",
    "    FieldSchema(name=\"hobby\", dtype=DataType.VARCHAR, max_length=128),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim),\n",
    "]\n",
    "schema = CollectionSchema(fields=fields, description=\"User data embedding example\")\n",
    "collection_name = \"user_data_collection\"\n",
    "\n",
    "if client.has_collection(collection_name):\n",
    "    client.drop_collection(collection_name)\n",
    "# Strong consistency waits for all loads to complete, adding latency with large datasets\n",
    "# client.create_collection(\n",
    "#     collection_name=collection_name, schema=schema, consistency_level=\"Strong\"\n",
    "# )\n",
    "client.create_collection(collection_name=collection_name, schema=schema)\n",
    "\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_type=\"IVF_FLAT\",\n",
    "    metric_type=\"COSINE\",\n",
    "    params={\"nlist\": 128},\n",
    ")\n",
    "client.create_index(collection_name=collection_name, index_params=index_params)\n",
    "\n",
    "data_to_insert = [\n",
    "    {\"name\": \"John\", \"age\": 23, \"city\": \"Shanghai\", \"hobby\": \"Drinking coffee\"},\n",
    "    {\"name\": \"Alice\", \"age\": 29, \"city\": \"New York\", \"hobby\": \"Reading books\"},\n",
    "    {\"name\": \"Bob\", \"age\": 31, \"city\": \"London\", \"hobby\": \"Playing chess\"},\n",
    "    {\"name\": \"Eve\", \"age\": 27, \"city\": \"Paris\", \"hobby\": \"Painting\"},\n",
    "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Tokyo\", \"hobby\": \"Cycling\"},\n",
    "    {\"name\": \"Grace\", \"age\": 22, \"city\": \"Berlin\", \"hobby\": \"Photography\"},\n",
    "    {\"name\": \"David\", \"age\": 40, \"city\": \"Toronto\", \"hobby\": \"Watching movies\"},\n",
    "    {\"name\": \"Helen\", \"age\": 30, \"city\": \"Sydney\", \"hobby\": \"Cooking\"},\n",
    "    {\"name\": \"Frank\", \"age\": 28, \"city\": \"Beijing\", \"hobby\": \"Hiking\"},\n",
    "    {\"name\": \"Ivy\", \"age\": 26, \"city\": \"Seoul\", \"hobby\": \"Dancing\"},\n",
    "    {\"name\": \"Tom\", \"age\": 33, \"city\": \"Madrid\", \"hobby\": \"Writing\"},\n",
    "]\n",
    "\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    return [\n",
    "        rec.embedding\n",
    "        for rec in openai_client.embeddings.create(\n",
    "            input=texts, model=embedding_model, dimensions=embedding_dim\n",
    "        ).data\n",
    "    ]\n",
    "\n",
    "\n",
    "texts = [\n",
    "    f\"{item['name']} from {item['city']} is {item['age']} years old and likes {item['hobby']}.\"\n",
    "    for item in data_to_insert\n",
    "]\n",
    "embeddings = get_embeddings(texts)\n",
    "\n",
    "insert_data = []\n",
    "for item, embedding in zip(data_to_insert, embeddings):\n",
    "    item_with_embedding = {\n",
    "        \"pk\": str(uuid.uuid4()),\n",
    "        \"name\": item[\"name\"],\n",
    "        \"age\": item[\"age\"],\n",
    "        \"city\": item[\"city\"],\n",
    "        \"hobby\": item[\"hobby\"],\n",
    "        \"embedding\": embedding,\n",
    "    }\n",
    "    insert_data.append(item_with_embedding)\n",
    "\n",
    "client.insert(collection_name=collection_name, data=insert_data)\n",
    "\n",
    "print(f\"Collection '{collection_name}' has been created and data has been inserted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86620203-2e40-49e9-8207-d6cf60089562",
   "metadata": {},
   "source": [
    "## Print 3 sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963c871-24c2-4276-b75a-fe0d36a843da",
   "metadata": {},
   "source": [
    "The code above creates a Milvus collection with the following structure:\n",
    "\n",
    "- **pk**: Primary key field (VARCHAR)\n",
    "- **name**: User name (VARCHAR) \n",
    "- **age**: User age (INT64)\n",
    "- **city**: User city (VARCHAR)\n",
    "- **hobby**: User hobby (VARCHAR)\n",
    "- **embedding**: Vector embedding (FLOAT_VECTOR, 1536 dimensions)\n",
    "\n",
    "We have inserted 11 sample users with their personal information and generate embeddings for semantic search capabilities. Each user's information is converted into a descriptive text that captures their name, location, age, and interests before being embedded. Let's verify that our collection was created successfully and contains the expected data by querying a few sample records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40083557-ada4-4585-92aa-d77f647934bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "collection_name = \"user_data_collection\"\n",
    "\n",
    "client.load_collection(collection_name=collection_name)\n",
    "\n",
    "result = client.query(\n",
    "    collection_name=collection_name,\n",
    "    filter=\"\",\n",
    "    output_fields=[\"name\", \"age\", \"city\", \"hobby\"],\n",
    "    limit=3,\n",
    ")\n",
    "\n",
    "for record in result:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada73d92-c583-4baf-bc72-8049d9925f74",
   "metadata": {},
   "source": [
    "## Collecting Milvus Filter Expression Documentation\n",
    "\n",
    "To help the large language model better understand Milvus's filter expression syntax, we need to provide it with relevant official documentation. We'll use the `docling` library to scrape several key pages from the official Milvus website.\n",
    "\n",
    "These pages contain detailed information about:\n",
    "- **Boolean operators**: `and`, `or`, `not` for complex logical conditions\n",
    "- **Basic operators**: Comparison operators like `==`, `!=`, `>`, `<`, `>=`, `<=`\n",
    "- **Filtering templates**: Advanced filtering patterns and syntax\n",
    "- **String matching**: Pattern matching with `like` and other string operations\n",
    "\n",
    "This documentation will serve as the knowledge base for our LLM to generate accurate filter expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9fc499-032d-4c9c-bc5f-9736e57d96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docling\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "converter = DocumentConverter()\n",
    "docs = [\n",
    "    converter.convert(url)\n",
    "    for url in [\n",
    "        \"https://milvus.io/docs/boolean.md\",\n",
    "        \"https://milvus.io/docs/basic-operators.md\",\n",
    "        \"https://milvus.io/docs/filtering-templating.md\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "for doc in docs[:3]:\n",
    "    print(doc.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf4952-6d1f-4b29-8db7-3b3f5c463b61",
   "metadata": {},
   "source": [
    "The documentation scraping provides comprehensive coverage of Milvus filter syntax. This knowledge base will enable our LLM to understand the nuances of filter expression construction, including proper operator usage, field referencing, and complex condition combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54dfc5c-a4b7-4733-b29b-939864f9e89e",
   "metadata": {},
   "source": [
    "## LLM-Powered Filter Generation\n",
    "\n",
    "Now that we have the documentation context, let's set up the LLM system to generate filter expressions. We'll create a structured prompt that combines the scraped documentation with user queries to produce syntactically correct Milvus filter expressions.\n",
    "\n",
    "Our filter generation system uses a carefully crafted prompt that:\n",
    "\n",
    "1. **Provides context**: Includes the complete Milvus documentation as reference material\n",
    "2. **Sets constraints**: Ensures the LLM only uses documented syntax and features  \n",
    "3. **Enforces accuracy**: Requires syntactically correct expressions\n",
    "4. **Maintains focus**: Returns only the filter expression without explanations\n",
    "\n",
    "Let's test this with a natural language query and see how well the LLM performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5738b8c-7abb-4d1c-81c9-91c3cbd7c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "context = \"\\n\".join([doc.document.export_to_markdown() for doc in docs])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an expert Milvus vector database engineer. Your task is to convert a user's natural language query into a valid Milvus filter expression, using the provided Milvus documentation as your knowledge base.\n",
    "\n",
    "Follow these rules strictly:\n",
    "1. Only use the provided documents as your source of knowledge.\n",
    "2. Ensure the generated filter expression is syntactically correct.\n",
    "3. If there isn't enough information in the documents to create an expression, state that directly.\n",
    "4. Only return the final filter expression. Do not include any explanations or extra text.\n",
    "\n",
    "---\n",
    "**Milvus Documentation Context:**\n",
    "{context}\n",
    "\n",
    "---\n",
    "**User Query:**\n",
    "{user_query}\n",
    "\n",
    "---\n",
    "**Filter Expression:**\n",
    "\"\"\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def generate_filter_expr(user_query):\n",
    "    \"\"\"\n",
    "    Generates a Milvus filter expression from a user query using GPT-4o-mini.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": user_query},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "user_query = \"Find people older than 30 who live in London, Tokyo, or Toronto\"\n",
    "\n",
    "filter_expr = generate_filter_expr(user_query)\n",
    "\n",
    "print(f\"Generated filter expression: {filter_expr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a1ba7-1ee2-4df2-9b20-0538d710378f",
   "metadata": {},
   "source": [
    "The LLM successfully generated a filter expression that combines multiple conditions:\n",
    "- Age comparison using `>`\n",
    "- Multiple city matching using `in` operator  \n",
    "- Proper field referencing and syntax\n",
    "\n",
    "This demonstrates the power of providing comprehensive documentation context to guide LLM filter generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc32bb-6efc-44cd-ba65-9dae9411f9b5",
   "metadata": {},
   "source": [
    "## Test the Generated Filter\n",
    "\n",
    "Now let's test our generated filter expression by using it in an actual Milvus search operation. We'll combine semantic search with precise filtering to find users that match both the query intent and the specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910943c8-b723-4d70-b6a2-e16987b7a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "clean_filter = (\n",
    "    filter_expr.replace(\"```\", \"\").replace('filter=\"', \"\").replace('\"', \"\").strip()\n",
    ")\n",
    "print(f\"Using filter: {clean_filter}\")\n",
    "\n",
    "query_embedding = (\n",
    "    openai_client.embeddings.create(\n",
    "        input=[user_query], model=\"text-embedding-3-small\", dimensions=1536\n",
    "    )\n",
    "    .data[0]\n",
    "    .embedding\n",
    ")\n",
    "\n",
    "search_results = client.search(\n",
    "    collection_name=\"user_data_collection\",\n",
    "    data=[query_embedding],\n",
    "    limit=10,\n",
    "    filter=clean_filter,\n",
    "    output_fields=[\"pk\", \"name\", \"age\", \"city\", \"hobby\"],\n",
    "    search_params={\n",
    "        \"metric_type\": \"COSINE\",\n",
    "        \"params\": {\"nprobe\": 10},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Search results:\")\n",
    "for i, hits in enumerate(search_results):\n",
    "    print(f\"Query {i}:\")\n",
    "    for hit in hits:\n",
    "        print(f\"  - {hit}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562e2e9-d452-4985-a00a-bc6ef575517a",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "The search results demonstrate successful integration of LLM-generated filters with Milvus vector search. The filter correctly identified users who:\n",
    "\n",
    "- Are older than 30 years\n",
    "- Live in London, Tokyo, or Toronto\n",
    "- Match the semantic context of the query\n",
    "\n",
    "This approach combines the precision of structured filtering with the flexibility of natural language input, making vector databases more accessible to users who may not be familiar with specific query syntax."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
