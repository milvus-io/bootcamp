{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b869e8-ca2a-4a85-b3e0-8c8727d8c9bb",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/integration/langchain/langchain_milvus_dido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>   <a href=\"https://github.com/milvus-io/bootcamp/blob/master/integration/langchain/langchain_milvus_dido.ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/View%20on%20GitHub-555555?style=flat&logo=github&logoColor=white\" alt=\"GitHub Repository\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c572bea",
   "metadata": {},
   "source": [
    "# Integrating Milvus Text Embedding Function with LangChain\n",
    "\n",
    "This guide demonstrates how to use Milvus 2.6's **Text Embedding Function** (also known as Data In Data Out) with LangChain. This feature allows the Milvus server to automatically convert raw text into vector embeddings, simplifying client-side code and centralizing API key management.\n",
    "\n",
    "[Milvus](https://milvus.io/) is the world's most advanced open-source vector database, built specifically to support embedding similarity search and AI applications. [LangChain](https://www.langchain.com/) is a framework for developing applications powered by large language models (LLMs). By integrating Milvus's Text Embedding Function, you can achieve a simpler and more efficient vector search solution in your LangChain applications.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this tutorial, ensure you have installed the following dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b9aed-e724-479c-8985-a0c708430648",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade langchain-milvus langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c456549-e3de-498e-9724-9ac28bf0faf0",
   "metadata": {},
   "source": [
    "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu).\n",
    "\n",
    "### Configuring the Milvus Server\n",
    "\n",
    "**Important**: The Text Embedding Function (Data In Data Out) feature is only available in **Milvus Server**. **Milvus Lite does not support this feature**. You need to use a Milvus server deployed with Docker/Kubernetes.\n",
    "\n",
    "Before using the Text Embedding Function, you need to configure credentials for embedding service providers on the Milvus server.\n",
    "\n",
    "**Declare your keys under credential:**\n",
    "\n",
    "You may list one or many API keys—give each a label you invent and will reference later.\n",
    "\n",
    "```yaml\n",
    "# milvus.yaml\n",
    "\n",
    "credential:\n",
    "  apikey_dev:\n",
    "    apikey: <YOUR_OPENAI_API_KEY>\n",
    "```\n",
    "\n",
    "**Tell Milvus which key to use for OpenAI calls**\n",
    "\n",
    "In the same file, point the OpenAI provider at the label you want it to use.\n",
    "\n",
    "```yaml\n",
    "function:\n",
    "  textEmbedding:\n",
    "    providers:\n",
    "      openai:\n",
    "        credential: apikey_dev\n",
    "        # url: https://api.openai.com/v1/embeddings   # (optional) custom url\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f17a7-5e20-4eba-8437-042e5be9b190",
   "metadata": {},
   "source": [
    "For more configuration methods, please refer to the [Milvus Embedding Function documentation](https://milvus.io/docs/embedding-function-overview.md).\n",
    "\n",
    "### Starting the Milvus Service\n",
    "\n",
    "Ensure that Milvus Server is running and the embedding feature is enabled. You can deploy Milvus server using [Docker](https://milvus.io/docs/install_standalone-docker.md) or [Kubernetes](https://milvus.io/docs/install_cluster-helm.md). Note: **Milvus Lite does not support Text Embedding Function**.\n",
    "\n",
    "## Understanding Embedding: Client-side vs Server-side\n",
    "\n",
    "Before diving into usage, let's first understand the differences between the two embedding approaches.\n",
    "\n",
    "### Embedding using LangChain's `Embeddings` class (Client-side)\n",
    "\n",
    "In the traditional LangChain approach, embedding generation happens on the client side by using the [`Embeddings` class](https://python.langchain.com/docs/api_reference/embeddings/langchain_core.embeddings.Embeddings). Your application needs to use the `embed_query` method of the class to call the embedding API, then store the generated vectors in Milvus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8178cca-c888-4205-b43a-20be38808cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "# Generate embedding on client side\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector = embeddings.embed_query(\"Hello, world!\")\n",
    "# [0.123, -0.456, ...] A vector of floats\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": \"http://localhost:19530\"},\n",
    "    collection_name=\"traditional_approach_collection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b8d78-e1bf-4b40-82b5-690317281725",
   "metadata": {},
   "source": [
    "**Sequence Diagram:**\n",
    "\n",
    "![](../../pics/langchain_milvus_dito_langchain_embedding.png)\n",
    "\n",
    "**Characteristics:**\n",
    "- Client directly calls embedding API\n",
    "- Need to manage API keys on the client side\n",
    "- Data flow: Text → Client → Embedding API → Vector → Milvus\n",
    "\n",
    "### Milvus Text Embedding Function (Server-side Data In Data Out)\n",
    "\n",
    "Milvus 2.6's Text Embedding Function (Data In Data Out) allows the Milvus server to automatically convert raw text into vector embeddings. The client only needs to provide text, and Milvus will automatically handle embedding generation.\n",
    "\n",
    "**Sequence Diagram:**\n",
    "\n",
    "![](../../pics/langchain_milvus_dito_milvus_embedding.png)\n",
    "\n",
    "**Characteristics:**\n",
    "- Milvus server calls embedding API\n",
    "- API keys are centrally managed on the server side\n",
    "- Data flow: Text → Milvus → Embedding API → Vector (stored in Milvus)\n",
    "\n",
    "### Comparison of the Two Methods\n",
    "\n",
    "| Feature | LangChain Embedding (Client-side) | Milvus Text Embedding Function (Server-side) |\n",
    "|------|------------------------------|------------------------------------------|\n",
    "| **Processing Location** | Client application | Milvus server |\n",
    "| **API Calls** | Client directly calls embedding API | Milvus server calls embedding API |\n",
    "| **API Key Management** | Need to manage on client side | Centrally managed on server side, more secure |\n",
    "| **Code Complexity** | Need to manage API keys and calls on client side | Only need to configure once in Milvus configuration |\n",
    "| **Use Cases** | • Need client-side control over embedding process<br>• Need to cache embedding results on client side<br>• Need to support multiple embedding model switching | • Simplify client-side code<br>• Centrally manage API keys on server side<br>• Need to batch process large volumes of documents<br>• Want to reduce client-side interactions with external APIs<br>• Need to combine with Milvus built-in features like BM25 |\n",
    "| **Milvus Version Requirements** | All versions (including Milvus Lite) | Milvus Lite not supported |\n",
    "\n",
    "**This tutorial primarily introduces the Milvus server-side Text Embedding Function (Data In Data Out) method**, which is a new feature introduced in Milvus 2.6 that can significantly simplify client-side code and improve security.\n",
    "\n",
    "## Using Text Embedding Function\n",
    "\n",
    "### Example 1: Server-side Embedding Only\n",
    "\n",
    "This is the simplest use case, completely relying on the Milvus server to generate embeddings. The client does not need any embedding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009e70e-ad50-4f08-875c-85f08e476b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "from langchain_milvus.function import TextEmbeddingBuiltInFunction\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create Text Embedding Function\n",
    "text_embedding_func = TextEmbeddingBuiltInFunction(\n",
    "    input_field_names=\"text\",  # Input field name (field containing text)\n",
    "    output_field_names=\"vector\",  # Output field name (field storing vectors)\n",
    "    dim=1536,  # Vector dimension (must specify)\n",
    "    params={\n",
    "        \"provider\": \"openai\",  # Service provider\n",
    "        \"model_name\": \"text-embedding-3-small\",  # Model name\n",
    "        \"credential\": \"apikey_dev\",    # Optional: use credential label configured in milvus.yaml\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create Milvus vector store\n",
    "# Note: embedding_function=None, because embedding is done on server side\n",
    "vector_store = Milvus(\n",
    "    embedding_function=None,  # Do not use client-side embedding\n",
    "    builtin_function=text_embedding_func,\n",
    "    connection_args={\"uri\": \"http://localhost:19530\"},\n",
    "    collection_name=\"my_collection\",\n",
    "    # consistency_level=\"Strong\",    # Strong consistency level, default is \"Session\"\n",
    "    auto_id=True,\n",
    "    # drop_old=True,  # If you want to drop old collection and create a new one\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7546a6-bb25-4954-8842-d800952afd2d",
   "metadata": {},
   "source": [
    "> For `connection_args`:\n",
    "> - **Must use Milvus Server**: The Text Embedding Function feature is only available in Milvus Server, Milvus Lite is not supported.\n",
    "> - Use server uri, such as `http://localhost:19530` (local Docker deployment) or `http://your-server:19530` (remote server).\n",
    "> - If using [Zilliz Cloud](https://zilliz.com/cloud), use the Public Endpoint as `uri` and set the `token` parameter.\n",
    "\n",
    "When adding documents, you only need to provide text, no need to pre-compute vectors. Milvus will automatically call the OpenAI API to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d4c19be-5596-48de-975f-9f2761c6f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[462726375729313252, 462726375729313253, 462726375729313254]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add documents (only need to provide text, no need to pre-compute vectors)\n",
    "documents = [\n",
    "    Document(page_content=\"Milvus simplifies semantic search through embeddings.\"),\n",
    "    Document(\n",
    "        page_content=\"Vector embeddings convert text into searchable numeric data.\"\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Semantic search helps users find relevant information quickly.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c76b02-b25e-4166-9307-b8f90193b9db",
   "metadata": {},
   "source": [
    "During search, directly use text queries, and Milvus will automatically convert the query text to vectors for search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e4dd43-89dc-49fc-8187-e7ac3d961a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765186679.227345 12227536 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Milvus simplifies semantic search through embeddings.\n",
      "Metadata: {'pk': 462726375729313252}\n",
      "\n",
      "Content: Semantic search helps users find relevant information quickly.\n",
      "Metadata: {'pk': 462726375729313254}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search (directly use text query)\n",
    "results = vector_store.similarity_search(\n",
    "    query=\"How does Milvus handle semantic search?\", k=2\n",
    ")\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bec31a-0a71-4336-bb63-c2830776661b",
   "metadata": {},
   "source": [
    "### Example 2: Combining Text Embedding and BM25 (Hybrid Search)\n",
    "\n",
    "Combining semantic search (Text Embedding) and keyword search (BM25) enables more powerful hybrid search capabilities. Semantic search excels at understanding query intent, while keyword search excels at exact matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e310d11-728c-4d37-8331-8b79b2f6680f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[462726375729313255, 462726375729313256]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_milvus import Milvus\n",
    "from langchain_milvus.function import TextEmbeddingBuiltInFunction, BM25BuiltInFunction\n",
    "\n",
    "# Text Embedding Function (semantic search)\n",
    "text_embedding_func = TextEmbeddingBuiltInFunction(\n",
    "    input_field_names=\"text\",\n",
    "    output_field_names=\"vector_dense\",\n",
    "    dim=1536,\n",
    "    params={\n",
    "        \"provider\": \"openai\",\n",
    "        \"model_name\": \"text-embedding-3-small\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# BM25 Function (keyword search)\n",
    "bm25_func = BM25BuiltInFunction(\n",
    "    input_field_names=\"text\",\n",
    "    output_field_names=\"vector_sparse\",\n",
    ")\n",
    "\n",
    "# Create Milvus vector store\n",
    "vector_store = Milvus(\n",
    "    embedding_function=None,\n",
    "    builtin_function=[text_embedding_func, bm25_func],\n",
    "    connection_args={\"uri\": \"http://localhost:19530\"},\n",
    "    vector_field=[\"vector_dense\", \"vector_sparse\"],\n",
    "    collection_name=\"hybrid_search_collection\",\n",
    "    # consistency_level=\"Strong\",    # Strong consistency level, default is \"Session\"\n",
    "    auto_id=True,\n",
    "    # drop_old=True,  # If you want to drop old collection and create a new one\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "documents = [\n",
    "    Document(page_content=\"Machine learning and artificial intelligence\"),\n",
    "    Document(page_content=\"The cat sat on the mat\"),\n",
    "]\n",
    "\n",
    "vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33769f2-c9fc-4faa-9cbb-b64948c030f1",
   "metadata": {},
   "source": [
    "Use `WeightedRanker` to control the weights of semantic search and keyword search. When dense weight is higher, results are more biased towards semantic similarity; when sparse weight is higher, results are more biased towards keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcfb7203-e11f-4713-ae53-c6e24c9b2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid search, use WeightedRanker to control weights\n",
    "# 70% semantic search, 30% keyword search\n",
    "results = vector_store.similarity_search(\n",
    "    query=\"AI technology\",\n",
    "    k=2,\n",
    "    ranker_type=\"weighted\",\n",
    "    ranker_params={\"weights\": [0.7, 0.3]},\n",
    ")\n",
    "\n",
    "# If you want to be more biased towards keyword matching, you can adjust weights\n",
    "# 30% semantic search, 70% keyword search\n",
    "results_keyword_focused = vector_store.similarity_search(\n",
    "    query=\"cat mat\",\n",
    "    k=2,\n",
    "    ranker_type=\"weighted\",\n",
    "    ranker_params={\"weights\": [0.3, 0.7]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ee933a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'pk': 462726375729313255}, page_content='Machine learning and artificial intelligence'),\n",
       " Document(metadata={'pk': 462726375729313256}, page_content='The cat sat on the mat')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67d55b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'pk': 462726375729313256}, page_content='The cat sat on the mat'),\n",
       " Document(metadata={'pk': 462726375729313255}, page_content='Machine learning and artificial intelligence')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_keyword_focused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133083a0-c7a4-4335-8ed2-5ad759829148",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You have learned how to use Milvus's Text Embedding Function (Data In Data Out) feature with LangChain. By moving embedding generation to the server side, you can simplify client-side code, centrally manage API keys, and easily implement hybrid search. Combined with Text Embedding Function and BM25, Milvus provides you with powerful vector search capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
