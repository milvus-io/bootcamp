{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/integration/evaluation_with_contextual_ai_lmunit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<a href=\"https://github.com/milvus-io/bootcamp/blob/master/integration/evaluation_with_contextual_ai_lmunit.ipynb\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/View%20on%20GitHub-555555?style=flat&logo=github&logoColor=white\" alt=\"GitHub Repository\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation with Contextual AI LMUnit\n",
        "\n",
        "This guide demonstrates how to use [Contextual AI's LMUnit](https://docs.contextual.ai/api-reference/lmunit/lmunit?utm_campaign=lmunit-integration&utm_source=milvus&utm_medium=github&utm_content=notebook) to evaluate a Retrieval-Augmented Generation (RAG) pipeline built upon [Milvus](https://milvus.io/).\n",
        "\n",
        "The RAG system combines a retrieval system with a generative model to generate new text based on a given prompt. The system first retrieves relevant documents from a corpus using Milvus, and then uses a generative model to generate new text based on the retrieved documents.\n",
        "\n",
        "[LMUnit](https://contextual.ai/research/lmunit?utm_campaign=lmunit-integration&utm_source=milvus&utm_medium=github&utm_content=notebook) is a specialized model developed by Contextual AI for evaluating LLM response quality through natural language unit testing. It achieves state-of-the-art performance in assessing generative AI outputs by providing a structured method to test responses.\n",
        "\n",
        "## Why Natural Language Unit Testing?\n",
        "\n",
        "Traditional LLM evaluation methods often face several challenges:\n",
        "- Human evaluations are inconsistent and costly, while metrics like ROUGE fail to capture nuanced quality measures\n",
        "- General-purpose LLMs may not provide fine-grained feedback\n",
        "- Simple yes/no evaluations miss important nuances\n",
        "\n",
        "Natural language unit tests address these challenges by:\n",
        "- Breaking down evaluation into specific, testable criteria\n",
        "- Providing granular feedback on different quality aspects\n",
        "- Enabling systematic improvement of LLM outputs\n",
        "- Supporting domain-specific quality requirements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, make sure you have the following dependencies installed:\n",
        "\n",
        "**Important**: We install `pymilvus[milvus_lite]` to enable local Milvus database connections. The `[milvus_lite]` extra is required for using Milvus Lite.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install --upgrade \"pymilvus[milvus_lite]\" contextual-client openai requests tqdm pandas matplotlib seaborn scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu).\n",
        "\n",
        "You will need API keys for both Contextual AI and OpenAI:\n",
        "- [Contextual AI API key](https://docs.contextual.ai/user-guides/beginner-guide?utm_campaign=lmunit-integration&utm_source=milvus&utm_medium=github&utm_content=notebook) for LMUnit evaluation\n",
        "- [OpenAI API key](https://platform.openai.com/docs/quickstart) for the RAG pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CONTEXTUAL_API_KEY\"] = \"your-contextual-api-key\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-***********\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the RAG Pipeline\n",
        "\n",
        "We will define a RAG class that uses Milvus as the vector store and OpenAI as the LLM. The class contains the `load` method, which loads text data into Milvus, the `retrieve` method, which retrieves the most similar text data to the given question, and the `answer` method, which answers the given question with the retrieved knowledge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pymilvus import MilvusClient\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self, collection_name: str = \"evaluation_collection\"):\n",
        "        self.milvus_client = MilvusClient(uri=\"./milvus_evaluation.db\")\n",
        "        self.openai_client = OpenAI()\n",
        "        self.collection_name = collection_name\n",
        "        \n",
        "    def emb_text(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embeddings using OpenAI\"\"\"\n",
        "        return (\n",
        "            self.openai_client.embeddings.create(\n",
        "                input=text, model=\"text-embedding-3-small\"\n",
        "            )\n",
        "            .data[0]\n",
        "            .embedding\n",
        "        )\n",
        "    \n",
        "    def load(self, texts: List[str]):\n",
        "        \"\"\"Load texts into Milvus collection\"\"\"\n",
        "        # Create collection if it doesn't exist\n",
        "        if self.milvus_client.has_collection(self.collection_name):\n",
        "            self.milvus_client.drop_collection(self.collection_name)\n",
        "            \n",
        "        # Get embedding dimension\n",
        "        test_embedding = self.emb_text(\"test\")\n",
        "        embedding_dim = len(test_embedding)\n",
        "        \n",
        "        self.milvus_client.create_collection(\n",
        "            collection_name=self.collection_name,\n",
        "            dimension=embedding_dim,\n",
        "            metric_type=\"IP\",\n",
        "            consistency_level=\"Bounded\",\n",
        "        )\n",
        "        \n",
        "        # Insert data\n",
        "        data = []\n",
        "        for i, text in enumerate(texts):\n",
        "            embedding = self.emb_text(text)\n",
        "            data.append({\"id\": i, \"vector\": embedding, \"text\": text})\n",
        "        \n",
        "        self.milvus_client.insert(collection_name=self.collection_name, data=data)\n",
        "        print(f\"Loaded {len(texts)} documents into Milvus\")\n",
        "    \n",
        "    def retrieve(self, question: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieve relevant documents for a question\"\"\"\n",
        "        search_res = self.milvus_client.search(\n",
        "            collection_name=self.collection_name,\n",
        "            data=[self.emb_text(question)],\n",
        "            limit=top_k,\n",
        "            search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
        "            output_fields=[\"text\"],\n",
        "        )\n",
        "        \n",
        "        return [res[\"entity\"][\"text\"] for res in search_res[0]]\n",
        "    \n",
        "    def answer(self, question: str, top_k: int = 3) -> str:\n",
        "        \"\"\"Generate answer using retrieved context\"\"\"\n",
        "        context = self.retrieve(question, top_k)\n",
        "        context_str = \"\\n\".join(context)\n",
        "        \n",
        "        response = self.openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful assistant. Answer the question based on the provided context. If the context doesn't contain enough information, say so.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Context:\\n{context_str}\\n\\nQuestion: {question}\"\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "        \n",
        "        return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Sample Data\n",
        "\n",
        "We'll use a sample dataset of financial documents to demonstrate LMUnit evaluation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample financial documents for evaluation\n",
        "sample_documents = [\n",
        "    \"The Federal Reserve raised interest rates by 0.25% to combat inflation. This marks the third consecutive rate increase this year, bringing the federal funds rate to 5.25%.\",\n",
        "    \"Apple Inc. reported record quarterly revenue of $94.8 billion, driven by strong iPhone sales and services growth. The company's stock price increased by 8% following the earnings announcement.\",\n",
        "    \"The Consumer Price Index (CPI) rose 3.2% year-over-year in October, slightly below expectations. Core inflation, excluding food and energy, increased by 4.0%.\",\n",
        "    \"Tesla's autonomous driving technology has received regulatory approval in California. The company plans to roll out Full Self-Driving (FSD) features to all eligible vehicles by Q2 2024.\",\n",
        "    \"The European Central Bank maintained its key interest rate at 4.5% while signaling potential future cuts if inflation continues to decline. The euro strengthened against the dollar following the announcement.\"\n",
        "]\n",
        "\n",
        "# Sample questions and expected answers for evaluation\n",
        "evaluation_data = [\n",
        "    {\n",
        "        \"question\": \"What is the current federal funds rate?\",\n",
        "        \"expected_answer\": \"The federal funds rate is currently 5.25% after the Federal Reserve raised it by 0.25%.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How much revenue did Apple report in their latest quarter?\",\n",
        "        \"expected_answer\": \"Apple reported record quarterly revenue of $94.8 billion in their latest quarter.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What was the year-over-year CPI increase in October?\",\n",
        "        \"expected_answer\": \"The Consumer Price Index (CPI) rose 3.2% year-over-year in October.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Initialize and load data into RAG pipeline\n",
        "rag = RAGPipeline()\n",
        "rag.load(sample_documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up LMUnit Evaluation\n",
        "\n",
        "Now we'll set up LMUnit for evaluating our RAG pipeline responses:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from contextual import ContextualAI\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize Contextual AI client\n",
        "contextual_client = ContextualAI(api_key=os.environ[\"CONTEXTUAL_API_KEY\"])\n",
        "\n",
        "# Define unit tests for financial domain evaluation\n",
        "unit_tests = [\n",
        "    \"Does the response provide specific numerical data when available?\",\n",
        "    \"Is the response factually accurate based on the provided context?\",\n",
        "    \"Does the response avoid speculation or unsupported claims?\",\n",
        "    \"Is the response clear and well-structured?\",\n",
        "    \"Does the response directly answer the question asked?\"\n",
        "]\n",
        "\n",
        "def run_lmunit_evaluation(rag_pipeline, evaluation_data, unit_tests):\n",
        "    \"\"\"Run LMUnit evaluation on RAG pipeline responses\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for item in tqdm(evaluation_data, desc=\"Evaluating responses\"):\n",
        "        question = item[\"question\"]\n",
        "        expected_answer = item[\"expected_answer\"]\n",
        "        \n",
        "        # Generate response using RAG pipeline\n",
        "        generated_response = rag_pipeline.answer(question)\n",
        "        \n",
        "        # Run unit tests on the generated response\n",
        "        test_results = []\n",
        "        for test in unit_tests:\n",
        "            try:\n",
        "                result = contextual_client.lmunit.create(\n",
        "                    query=question,\n",
        "                    response=generated_response,\n",
        "                    unit_test=test\n",
        "                )\n",
        "                test_results.append({\n",
        "                    'test': test,\n",
        "                    'score': result.score,\n",
        "                    'metadata': result.metadata if hasattr(result, 'metadata') else None\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error with test '{test}': {e}\")\n",
        "                test_results.append({\n",
        "                    'test': test,\n",
        "                    'score': None,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "        \n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'expected_answer': expected_answer,\n",
        "            'generated_response': generated_response,\n",
        "            'test_results': test_results\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = run_lmunit_evaluation(rag, evaluation_data, unit_tests)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Results\n",
        "\n",
        "Let's analyze the evaluation results and visualize the performance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Process results for analysis\n",
        "analysis_data = []\n",
        "for result in evaluation_results:\n",
        "    for test_result in result['test_results']:\n",
        "        if test_result['score'] is not None:\n",
        "            analysis_data.append({\n",
        "                'question': result['question'][:50] + \"...\",  # Truncate for display\n",
        "                'test': test_result['test'][:40] + \"...\",  # Truncate for display\n",
        "                'score': test_result['score']\n",
        "            })\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "df = pd.DataFrame(analysis_data)\n",
        "\n",
        "# Calculate average scores by test\n",
        "test_scores = df.groupby('test')['score'].agg(['mean', 'std', 'count']).reset_index()\n",
        "test_scores = test_scores.sort_values('mean', ascending=False)\n",
        "\n",
        "print(\"Average Scores by Test:\")\n",
        "print(test_scores)\n",
        "\n",
        "# Visualize results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Bar plot of average scores by test\n",
        "test_scores.plot(x='test', y='mean', kind='bar', ax=ax1, color='skyblue')\n",
        "ax1.set_title('Average LMUnit Scores by Test')\n",
        "ax1.set_xlabel('Test')\n",
        "ax1.set_ylabel('Average Score')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.set_ylim(0, 5)\n",
        "\n",
        "# Heatmap of scores by question and test\n",
        "pivot_df = df.pivot(index='question', columns='test', values='score')\n",
        "sns.heatmap(pivot_df, annot=True, cmap='RdYlGn', center=2.5, ax=ax2)\n",
        "ax2.set_title('LMUnit Scores Heatmap')\n",
        "ax2.set_xlabel('Test')\n",
        "ax2.set_ylabel('Question')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Evaluation with Custom Scoring Rubrics\n",
        "\n",
        "LMUnit supports custom scoring rubrics for more detailed evaluation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced evaluation with custom scoring rubric\n",
        "advanced_unit_test = \"\"\"\n",
        "Does the response provide comprehensive financial information?\n",
        "Scoring Scale:\n",
        "Score 1: No specific financial data provided; vague or general statements only\n",
        "Score 2: Limited financial data provided; either numbers or context discussed, but not both\n",
        "Score 3: Basic financial data provided with surface-level analysis\n",
        "Score 4: Clear financial data provided with detailed analysis and context\n",
        "Score 5: Comprehensive financial data with in-depth analysis, context, and implications\n",
        "\"\"\"\n",
        "\n",
        "# Run advanced evaluation\n",
        "advanced_results = []\n",
        "for item in evaluation_data:\n",
        "    question = item[\"question\"]\n",
        "    generated_response = rag.answer(question)\n",
        "    \n",
        "    try:\n",
        "        result = contextual_client.lmunit.create(\n",
        "            query=question,\n",
        "            response=generated_response,\n",
        "            unit_test=advanced_unit_test\n",
        "        )\n",
        "        advanced_results.append({\n",
        "            'question': question,\n",
        "            'response': generated_response,\n",
        "            'score': result.score,\n",
        "            'metadata': result.metadata if hasattr(result, 'metadata') else None\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error with advanced evaluation: {e}\")\n",
        "\n",
        "# Display advanced results\n",
        "print(\"Advanced Evaluation Results:\")\n",
        "for result in advanced_results:\n",
        "    print(f\"\\nQuestion: {result['question']}\")\n",
        "    print(f\"Score: {result['score']}/5\")\n",
        "    print(f\"Response: {result['response'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Analysis and Insights\n",
        "\n",
        "Let's analyze the overall performance and identify areas for improvement:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate overall statistics\n",
        "all_scores = [score for result in evaluation_results for test_result in result['test_results'] \n",
        "              if test_result['score'] is not None for score in [test_result['score']]]\n",
        "\n",
        "if all_scores:\n",
        "    print(f\"Overall Performance Statistics:\")\n",
        "    print(f\"Average Score: {np.mean(all_scores):.2f}\")\n",
        "    print(f\"Median Score: {np.median(all_scores):.2f}\")\n",
        "    print(f\"Standard Deviation: {np.std(all_scores):.2f}\")\n",
        "    print(f\"Min Score: {np.min(all_scores):.2f}\")\n",
        "    print(f\"Max Score: {np.max(all_scores):.2f}\")\n",
        "    \n",
        "    # Identify low-performing areas\n",
        "    low_scores = [score for score in all_scores if score < 3.0]\n",
        "    print(f\"\\nLow Scores (< 3.0): {len(low_scores)} out of {len(all_scores)} ({len(low_scores)/len(all_scores)*100:.1f}%)\")\n",
        "    \n",
        "    # Performance by test type\n",
        "    print(f\"\\nPerformance by Test Type:\")\n",
        "    for test in unit_tests:\n",
        "        test_scores = [test_result['score'] for result in evaluation_results \n",
        "                      for test_result in result['test_results'] \n",
        "                      if test_result['test'] == test and test_result['score'] is not None]\n",
        "        if test_scores:\n",
        "            avg_score = np.mean(test_scores)\n",
        "            print(f\"  {test[:50]}...: {avg_score:.2f}\")\n",
        "\n",
        "# Display detailed results\n",
        "print(f\"\\nDetailed Evaluation Results:\")\n",
        "for i, result in enumerate(evaluation_results):\n",
        "    print(f\"\\n--- Evaluation {i+1} ---\")\n",
        "    print(f\"Question: {result['question']}\")\n",
        "    print(f\"Generated Response: {result['generated_response']}\")\n",
        "    print(f\"Test Scores:\")\n",
        "    for test_result in result['test_results']:\n",
        "        if test_result['score'] is not None:\n",
        "            print(f\"  - {test_result['test'][:50]}...: {test_result['score']}/5\")\n",
        "        else:\n",
        "            print(f\"  - {test_result['test'][:50]}...: Error - {test_result.get('error', 'Unknown error')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This guide demonstrated how to use Contextual AI's LMUnit to evaluate a RAG pipeline built with Milvus. Key takeaways:\n",
        "\n",
        "### LMUnit Advantages:\n",
        "- **Natural Language Unit Testing**: Provides structured, testable criteria for LLM responses\n",
        "- **Granular Feedback**: Breaks down evaluation into specific quality aspects\n",
        "- **Domain-Specific Evaluation**: Supports custom scoring rubrics for specialized domains\n",
        "- **State-of-the-Art Performance**: Achieves superior evaluation accuracy compared to traditional methods\n",
        "\n",
        "### Evaluation Workflow:\n",
        "1. **Setup**: Initialize RAG pipeline with Milvus and OpenAI\n",
        "2. **Data Loading**: Load sample documents into vector database\n",
        "3. **Response Generation**: Generate responses using RAG pipeline\n",
        "4. **Unit Testing**: Apply LMUnit tests to evaluate response quality\n",
        "5. **Analysis**: Visualize and analyze evaluation results\n",
        "6. **Improvement**: Identify areas for RAG pipeline enhancement\n",
        "\n",
        "### Key Benefits:\n",
        "- **Systematic Evaluation**: Structured approach to assessing LLM output quality\n",
        "- **Actionable Insights**: Detailed feedback for improving RAG performance\n",
        "- **Scalable Testing**: Easy to add new test cases and evaluation criteria\n",
        "- **Production Ready**: Robust evaluation framework for real-world applications\n",
        "\n",
        "### Next Steps:\n",
        "- Experiment with different unit test designs for your specific domain\n",
        "- Integrate LMUnit evaluation into your RAG pipeline development workflow\n",
        "- Use evaluation results to fine-tune retrieval and generation parameters\n",
        "- Implement continuous evaluation monitoring for production RAG systems\n",
        "\n",
        "This evaluation framework provides a solid foundation for building high-quality, reliable RAG applications with Milvus and Contextual AI.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
