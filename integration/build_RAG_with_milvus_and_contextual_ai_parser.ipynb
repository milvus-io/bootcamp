{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ9WzgE3slzy"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/integration/build_RAG_with_milvus_and_contextual_ai_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>   <a href=\"https://github.com/milvus-io/bootcamp/blob/master/integration/build_RAG_with_milvus_and_contextual_ai_parser.ipynb\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/View%20on%20GitHub-555555?style=flat&logo=github&logoColor=white\" alt=\"GitHub Repository\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_pmmN4slzz"
      },
      "source": [
        "# Build RAG with Milvus and Contextual AI\n",
        "\n",
        "**Versions used:**\n",
        "- Milvus version `2.6.3`\n",
        "- Contextual AI client `0.9.0`\n",
        "\n",
        "[Contextual AI Parser](https://docs.contextual.ai/api-reference/parse/parse-file?utm_campaign=Parse-api-integration&utm_source=milvus&utm_medium=github&utm_content=notebook) is a cloud-based document parsing service that excels at extracting structured information from PDFs, DOC/DOCX, and PPT/PPTX files. It provides high-quality markdown extraction with document hierarchy preservation and advanced table extraction, making it ideal for RAG applications.\n",
        "\n",
        "In this tutorial, we'll show you how to build a Retrieval-Augmented Generation (RAG) pipeline using Milvus and Contextual AI Parser. The pipeline integrates Contextual AI Parser for document parsing, Milvus for vector storage, and OpenAI for generating insightful, context-aware responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZOVxoRCslzz"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "### Dependencies and Environment\n",
        "\n",
        "To start, install the required dependencies by running the following command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBrTzXtCslz0"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade \"pymilvus[milvus_lite]\" contextual-client openai requests rich\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCIQylEJslz0"
      },
      "source": [
        "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxbh-M36slz0"
      },
      "source": [
        "### Setting Up API Keys\n",
        "\n",
        "We will use Contextual AI for document parsing and OpenAI as the LLM in this example. You should prepare the [CONTEXTUAL_API_KEY](https://docs.contextual.ai/user-guides/beginner-guide?utm_campaign=Parse-api-integration&utm_source=milvus&utm_medium=github&utm_content=notebook) and [OPENAI_API_KEY](https://platform.openai.com/docs/quickstart) as environment variables.\n",
        "\n",
        "If you're running this notebook in Google Colab, you can add your API keys as secrets. The code below dynamically handles both Colab secrets and environment variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QMoGBzQHslz0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# API key variable names\n",
        "contextual_api_key_var = \"CONTEXTUAL_API_KEY\"\n",
        "openai_api_key_var = \"OPENAI_API_KEY\"\n",
        "\n",
        "# Fetch API keys\n",
        "try:\n",
        "    # If running in Colab, fetch API keys from Secrets\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "    contextual_api_key = userdata.get(contextual_api_key_var)\n",
        "    openai_api_key = userdata.get(openai_api_key_var)\n",
        "\n",
        "    if not contextual_api_key:\n",
        "        raise ValueError(f\"Secret '{contextual_api_key_var}' not found in Colab secrets.\")\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(f\"Secret '{openai_api_key_var}' not found in Colab secrets.\")\n",
        "except ImportError:\n",
        "    # If not running in Colab, fetch API keys from environment variables\n",
        "    contextual_api_key = os.getenv(contextual_api_key_var)\n",
        "    openai_api_key = os.getenv(openai_api_key_var)\n",
        "\n",
        "    if not contextual_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{contextual_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "    if not openai_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{openai_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "\n",
        "os.environ[\"CONTEXTUAL_API_KEY\"] = contextual_api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr8bNz00slz0"
      },
      "source": [
        "### Prepare the LLM and Embedding Model\n",
        "\n",
        "We initialize the OpenAI client for embeddings and Contextual AI client for GLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNRHYLXEslz0"
      },
      "source": [
        "Define a function to generate text embeddings using OpenAI client. We use the [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings) model as an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "axlV6oJVslz1"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from contextual import ContextualAI\n",
        "\n",
        "openai_client = OpenAI()\n",
        "contextual_client = ContextualAI(api_key=contextual_api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "deKVeHLqslz1"
      },
      "outputs": [],
      "source": [
        "def emb_text(text):\n",
        "    return (\n",
        "        openai_client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
        "        .data[0]\n",
        "        .embedding\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGWrzTFqslz1"
      },
      "source": [
        "Generate a test embedding and print its dimension and first few elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTPAzj5Qslz1",
        "outputId": "08654532-7400-46dd-fcae-8a93849eb191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1536\n",
            "[0.009889289736747742, -0.005578675772994757, 0.00683477520942688, -0.03805781528353691, -0.01824733428657055, -0.04121600463986397, -0.007636285852640867, 0.03225184231996536, 0.018949154764413834, 9.352207416668534e-05]\n"
          ]
        }
      ],
      "source": [
        "test_embedding = emb_text(\"This is a test\")\n",
        "embedding_dim = len(test_embedding)\n",
        "print(embedding_dim)\n",
        "print(test_embedding[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBWgnlfSslz1"
      },
      "source": [
        "## Process Data Using Contextual AI Parser\n",
        "\n",
        "Contextual AI Parser can parse various document formats into structured markdown with document hierarchy preservation. The parser handles complex documents with images, tables, and hierarchical structures, providing multiple output formats including:\n",
        "- `markdown-document`: Single concatenated markdown output\n",
        "- `markdown-per-page`: Page-by-page markdown output\n",
        "- `blocks-per-page`: Structured JSON with document hierarchy\n",
        "\n",
        "For a full list of supported input and output formats, please refer to [the official documentation](https://docs.contextual.ai/api-reference/parse/parse-file?utm_campaign=Parse-api-integration&utm_source=milvus&utm_medium=github&utm_content=notebook).\n",
        "\n",
        "In this tutorial, we will parse two distinct document types: a research paper and a table-rich document. We'll use the `blocks-per-page` format to extract structured chunks suitable for downstream RAG tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVo833WDslz1",
        "outputId": "5fd4fd4f-d1f6-4d5d-fd23-2892f8b20c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitting parse job for: Attention Is All You Need\n",
            "Submitting parse job for: OmniDocBench Dataset Documentation\n",
            "Submitted 2 parse jobs. Monitoring status...\n",
            "Job 1/2 (Attention Is All You Need): processing\n",
            "Job 2/2 (OmniDocBench Dataset Documentation): processing\n",
            "Waiting for remaining jobs to complete...\n",
            "Job 1/2 (Attention Is All You Need): processing\n",
            "Job 2/2 (OmniDocBench Dataset Documentation): completed\n",
            "Waiting for remaining jobs to complete...\n",
            "Job 1/2 (Attention Is All You Need): processing\n",
            "Waiting for remaining jobs to complete...\n",
            "Job 1/2 (Attention Is All You Need): completed\n",
            "All parse jobs completed!\n",
            "\n",
            "Extracted 134 text blocks from Attention Is All You Need\n",
            "Extracted 3 text blocks from OmniDocBench Dataset Documentation\n",
            "\n",
            "Total chunks extracted: 137\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Documents to parse with Contextual AI\n",
        "documents = [\n",
        "    {\n",
        "        \"url\": \"https://arxiv.org/pdf/1706.03762\",\n",
        "        \"title\": \"Attention Is All You Need\",\n",
        "        \"type\": \"research_paper\",\n",
        "        \"description\": \"Seminal transformer architecture paper that introduced self-attention mechanisms\"\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/03-standalone-api/04-parse/data/omnidocbench-text.pdf\",\n",
        "        \"title\": \"OmniDocBench Dataset Documentation\",\n",
        "        \"type\": \"table_rich_document\",\n",
        "        \"description\": \"Dataset documentation with large tables demonstrating table extraction capabilities\"\n",
        "    }\n",
        "]\n",
        "\n",
        "job_data = []\n",
        "\n",
        "# Submit parse jobs\n",
        "for doc in documents:\n",
        "    print(f\"Submitting parse job for: {doc['title']}\")\n",
        "\n",
        "    file_content = requests.get(doc[\"url\"]).content\n",
        "    with open(\"temp_file.pdf\", \"wb\") as f:\n",
        "        f.write(file_content)\n",
        "\n",
        "    with open(\"temp_file.pdf\", \"rb\") as fp:\n",
        "        response = contextual_client.parse.create(\n",
        "            raw_file=fp,\n",
        "            parse_mode=\"standard\",\n",
        "            enable_document_hierarchy=True,\n",
        "            enable_split_tables=False,\n",
        "            figure_caption_mode=\"concise\"\n",
        "        )\n",
        "\n",
        "    job_data.append({\"document\": doc, \"job_id\": response.job_id})\n",
        "\n",
        "print(f\"Submitted {len(job_data)} parse jobs. Monitoring status...\")\n",
        "\n",
        "async def wait_for_jobs_async(job_data, max_attempts=20, interval=30.0):\n",
        "    completed_jobs = set()\n",
        "    for attempt in range(max_attempts):\n",
        "        if len(completed_jobs) >= len(job_data):\n",
        "            return completed_jobs\n",
        "\n",
        "        for idx, job_info in enumerate(job_data, start=1):\n",
        "            job_id = job_info[\"job_id\"]\n",
        "            if job_id in completed_jobs:\n",
        "                continue\n",
        "\n",
        "            status = await asyncio.to_thread(contextual_client.parse.job_status, job_id)\n",
        "            doc_title = job_info[\"document\"][\"title\"]\n",
        "            print(f\"Job {idx}/{len(job_data)} ({doc_title}): {status.status}\")\n",
        "\n",
        "            if status.status == \"completed\":\n",
        "                completed_jobs.add(job_id)\n",
        "            elif status.status == \"failed\":\n",
        "                raise RuntimeError(f\"Parse job failed for {doc_title}\")\n",
        "\n",
        "        if len(completed_jobs) < len(job_data):\n",
        "            print(\"Waiting for remaining jobs to complete...\")\n",
        "            await asyncio.sleep(interval)\n",
        "\n",
        "    raise TimeoutError(\"Timed out waiting for parse jobs to complete.\")\n",
        "\n",
        "nest_asyncio.apply()\n",
        "completed_jobs = asyncio.run(wait_for_jobs_async(job_data))\n",
        "print(\"All parse jobs completed!\\n\")\n",
        "\n",
        "# Retrieve results and extract text chunks\n",
        "texts = []\n",
        "for job_info in job_data:\n",
        "    job_id = job_info[\"job_id\"]\n",
        "    doc_title = job_info[\"document\"][\"title\"]\n",
        "\n",
        "    results = contextual_client.parse.job_results(job_id, output_types=[\"blocks-per-page\"])\n",
        "    block_count = 0\n",
        "    for page in results.pages:\n",
        "        for block in page.blocks:\n",
        "            if getattr(block, \"markdown\", None):\n",
        "                texts.append(block.markdown)\n",
        "                block_count += 1\n",
        "\n",
        "    print(f\"Extracted {block_count} text blocks from {doc_title}\")\n",
        "\n",
        "print(f\"\\nTotal chunks extracted: {len(texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0tTAqwslz1"
      },
      "source": [
        "## Load Data into Milvus\n",
        "\n",
        "### Create the collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H16BTYxjslz1"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient\n",
        "\n",
        "milvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n",
        "collection_name = \"my_rag_collection\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbf4UPIslz2"
      },
      "source": [
        "> As for the argument of `MilvusClient`:\n",
        "> - Setting the `uri` as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.\n",
        "> - If you have large scale of data, you can set up a more performant Milvus server on [docker or kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server uri, e.g.`http://localhost:19530`, as your `uri`.\n",
        "> - If you want to use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and Api key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#free-cluster-details) in Zilliz Cloud.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQmtbwmLslz2"
      },
      "source": [
        "Check if the collection already exists and drop it if it does.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lN1z1j70slz2"
      },
      "outputs": [],
      "source": [
        "if milvus_client.has_collection(collection_name):\n",
        "    milvus_client.drop_collection(collection_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gfnaZIqslz2"
      },
      "source": [
        "Create a new collection with specified parameters.\n",
        "\n",
        "If we don't specify any field information, Milvus will automatically create a default `id` field for primary key, and a `vector` field to store the vector data. A reserved JSON field is used to store non-schema-defined fields and their values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r4Sn3LrXslz2"
      },
      "outputs": [],
      "source": [
        "milvus_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    dimension=embedding_dim,\n",
        "    metric_type=\"IP\",  # Inner product distance\n",
        "    # Strong consistency waits for all loads to complete, adding latency with large datasets\n",
        "    # consistency_level=\"Strong\",  # Supported values are (`\"Strong\"`, `\"Session\"`, `\"Bounded\"`, `\"Eventually\"`). See https://milvus.io/docs/consistency.md#Consistency-Level for more details.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFnPoXw1slz2"
      },
      "source": [
        "### Insert data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B17Wcs1vslz2",
        "outputId": "dd9921d3-4cfc-4641-deae-9d202445b08a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks: 100%|██████████| 137/137 [01:00<00:00,  2.27it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'insert_count': 137, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136], 'cost': 0}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "data = []\n",
        "\n",
        "for i, chunk in enumerate(tqdm(texts, desc=\"Processing chunks\")):\n",
        "    embedding = emb_text(chunk)\n",
        "    data.append({\"id\": i, \"vector\": embedding, \"text\": chunk})\n",
        "\n",
        "milvus_client.insert(collection_name=collection_name, data=data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Pt0M5bslz2"
      },
      "source": [
        "## Build RAG\n",
        "\n",
        "### Retrieve data for a query\n",
        "\n",
        "Let's specify a query question about the parsed documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HuJc9MAqslz2"
      },
      "outputs": [],
      "source": [
        "question = \"What is the transformer architecture and how does self-attention work?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWzEqDw5slz2"
      },
      "source": [
        "Search for the question in the collection and retrieve the semantic top-3 matches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "10FKgHtPslz2"
      },
      "outputs": [],
      "source": [
        "search_res = milvus_client.search(\n",
        "    collection_name=collection_name,\n",
        "    data=[emb_text(question)],\n",
        "    limit=3,\n",
        "    search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
        "    output_fields=[\"text\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2U5Fkeslz2"
      },
      "source": [
        "Let's take a look at the search results of the query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPmotPkUslz2",
        "outputId": "c0a03813-9ed8-4df1-9d5c-424910dc6d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    [\n",
            "        \"To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\",\n",
            "        0.7215487360954285\n",
            "    ],\n",
            "    [\n",
            "        \"The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\",\n",
            "        0.7109684944152832\n",
            "    ],\n",
            "    [\n",
            "        \"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\",\n",
            "        0.6668375730514526\n",
            "    ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "retrieved_lines_with_distances = [\n",
        "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
        "]\n",
        "print(json.dumps(retrieved_lines_with_distances, indent=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBr1NtyYslz3"
      },
      "source": [
        "### Use LLM to get a RAG response\n",
        "\n",
        "Convert the retrieved documents into a string format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xes7ZdjCslz3"
      },
      "outputs": [],
      "source": [
        "context = \"\\n\".join(\n",
        "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPqYtHHtslz3"
      },
      "source": [
        "Define system and user prompts for the Language Model. This prompt is assembled with the retrieved documents from Milvus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UBzgLR-pslz3"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
        "\"\"\"\n",
        "USER_PROMPT = f\"\"\"\n",
        "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPoJxtzMslz3"
      },
      "source": [
        "Use OpenAI ChatGPT to generate a response based on the prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFKcNLE9slz3",
        "outputId": "dfcd4e06-33d4-4698-8abb-a5c7718ff3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Transformer architecture: It is a sequence transduction (encoder–decoder) model that relies entirely on attention. Both the encoder and decoder are built from stacked self-attention layers followed by point-wise, fully connected (feed-forward) layers. Unlike earlier encoder–decoder models, it does not use sequence-aligned RNNs or convolutions.\n",
            "\n",
            "- How self-attention works (as described here): The model computes representations of the input and output using attention alone; the recurrent layers are replaced with multi-headed self-attention.\n"
          ]
        }
      ],
      "source": [
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-5\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
        "    ],\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
