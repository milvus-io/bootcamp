{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/integration/build_RAG_with_milvus_and_contextual_ai_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>   <a href=\"https://github.com/milvus-io/bootcamp/blob/master/integration/build_RAG_with_milvus_and_contextual_ai_parser.ipynb\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/View%20on%20GitHub-555555?style=flat&logo=github&logoColor=white\" alt=\"GitHub Repository\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build RAG with Milvus and Contextual AI\n",
        "\n",
        "[Contextual AI Parser](https://docs.contextual.ai/api-reference/parse/parse-file?utm_campaign=Parse-api-integration&utm_source=milvus&utm_medium=github&utm_content=notebook) is a cloud-based document parsing service that excels at extracting structured information from PDFs, DOC/DOCX, and PPT/PPTX files. It provides high-quality markdown extraction with document hierarchy preservation and advanced table extraction, making it ideal for RAG applications.\n",
        "\n",
        "In this tutorial, we'll show you how to build a Retrieval-Augmented Generation (RAG) pipeline using Milvus and Contextual AI Parser. The pipeline integrates Contextual AI Parser for document parsing, Milvus for vector storage, and OpenAI for generating insightful, context-aware responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation\n",
        "\n",
        "### Dependencies and Environment\n",
        "\n",
        "To start, install the required dependencies by running the following command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install --upgrade \"pymilvus[milvus_lite]\" contextual-client openai requests rich\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Up API Keys\n",
        "\n",
        "We will use Contextual AI for document parsing and OpenAI as the LLM in this example. You should prepare the [CONTEXTUAL_API_KEY](https://docs.contextual.ai/user-guides/beginner-guide?utm_campaign=Parse-api-integration&utm_source=milvus&utm_medium=github&utm_content=notebook) and [OPENAI_API_KEY](https://platform.openai.com/docs/quickstart) as environment variables.\n",
        "\n",
        "If you're running this notebook in Google Colab, you can add your API keys as secrets. The code below dynamically handles both Colab secrets and environment variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# API key variable names\n",
        "contextual_api_key_var = \"CONTEXTUAL_API_KEY\"\n",
        "openai_api_key_var = \"OPENAI_API_KEY\"\n",
        "\n",
        "# Fetch API keys\n",
        "try:\n",
        "    # If running in Colab, fetch API keys from Secrets\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "    contextual_api_key = userdata.get(contextual_api_key_var)\n",
        "    openai_api_key = userdata.get(openai_api_key_var)\n",
        "    \n",
        "    if not contextual_api_key:\n",
        "        raise ValueError(f\"Secret '{contextual_api_key_var}' not found in Colab secrets.\")\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(f\"Secret '{openai_api_key_var}' not found in Colab secrets.\")\n",
        "except ImportError:\n",
        "    # If not running in Colab, fetch API keys from environment variables\n",
        "    contextual_api_key = os.getenv(contextual_api_key_var)\n",
        "    openai_api_key = os.getenv(openai_api_key_var)\n",
        "    \n",
        "    if not contextual_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{contextual_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "    if not openai_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{openai_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "\n",
        "os.environ[\"CONTEXTUAL_API_KEY\"] = contextual_api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the LLM and Embedding Model\n",
        "\n",
        "We initialize the OpenAI client for embeddings and Contextual AI client for GLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to generate text embeddings using OpenAI client. We use the [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings) model as an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from contextual import ContextualAI\n",
        "\n",
        "openai_client = OpenAI()\n",
        "contextual_client = ContextualAI(api_key=contextual_api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def emb_text(text):\n",
        "    return (\n",
        "        openai_client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
        "        .data[0]\n",
        "        .embedding\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a test embedding and print its dimension and first few elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_embedding = emb_text(\"This is a test\")\n",
        "embedding_dim = len(test_embedding)\n",
        "print(embedding_dim)\n",
        "print(test_embedding[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Data Using Contextual AI Parser\n",
        "\n",
        "Contextual AI Parser can parse various document formats into structured markdown with document hierarchy preservation. The parser handles complex documents with images, tables, and hierarchical structures, providing multiple output formats including:\n",
        "- `markdown-document`: Single concatenated markdown output\n",
        "- `markdown-per-page`: Page-by-page markdown output\n",
        "- `blocks-per-page`: Structured JSON with document hierarchy\n",
        "\n",
        "For a full list of supported input and output formats, please refer to [the official documentation](https://docs.contextual.ai/api-reference/parse/parse-file?utm_campaign=Parse-api-integration&utm_source=milvus&utm_medium=github&utm_content=notebook).\n",
        "\n",
        "In this tutorial, we will parse two distinct document types: a research paper and a table-rich document. We'll use the `blocks-per-page` format to extract structured chunks suitable for downstream RAG tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from time import sleep\n",
        "\n",
        "# Documents to parse with Contextual AI\n",
        "documents = [\n",
        "    {\n",
        "        \"url\": \"https://arxiv.org/pdf/1706.03762\",\n",
        "        \"title\": \"Attention Is All You Need\",\n",
        "        \"type\": \"research_paper\",\n",
        "        \"description\": \"Seminal transformer architecture paper that introduced self-attention mechanisms\"\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/03-standalone-api/04-parse/data/omnidocbench-text.pdf\",\n",
        "        \"title\": \"OmniDocBench Dataset Documentation\", \n",
        "        \"type\": \"table_rich_document\",\n",
        "        \"description\": \"Dataset documentation with large tables demonstrating table extraction capabilities\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Parse documents and extract text chunks\n",
        "texts = []\n",
        "for doc in documents:\n",
        "    print(f\"Parsing: {doc['title']}\")\n",
        "    \n",
        "    # Download file from URL\n",
        "    file_content = requests.get(doc[\"url\"]).content\n",
        "    \n",
        "    # Submit parse job\n",
        "    with open(\"temp_file.pdf\", \"wb\") as f:\n",
        "        f.write(file_content)\n",
        "    \n",
        "    with open(\"temp_file.pdf\", \"rb\") as fp:\n",
        "        response = contextual_client.parse.create(\n",
        "            raw_file=fp,\n",
        "            parse_mode=\"standard\",\n",
        "            enable_document_hierarchy=True,\n",
        "            enable_split_tables=False,\n",
        "            figure_caption_mode=\"concise\"\n",
        "        )\n",
        "    \n",
        "    job_id = response.job_id\n",
        "    \n",
        "    # Wait for job to complete\n",
        "    while True:\n",
        "        status = contextual_client.parse.job_status(job_id)\n",
        "        if status.status == \"completed\":\n",
        "            break\n",
        "        elif status.status == \"failed\":\n",
        "            raise Exception(\"Parse job failed\")\n",
        "        sleep(30)  # Wait 30 seconds before checking again\n",
        "    \n",
        "    # Get results\n",
        "    results = contextual_client.parse.job_results(job_id, output_types=[\"blocks-per-page\"])\n",
        "    \n",
        "    # Extract text from blocks\n",
        "    for page in results.pages:\n",
        "        for block in page.blocks:\n",
        "            if block.markdown:\n",
        "                texts.append(block.markdown)\n",
        "    \n",
        "    print(f\"  Extracted {len([b for page in results.pages for b in page.blocks if b.markdown])} text blocks\")\n",
        "\n",
        "print(f\"\\nTotal chunks extracted: {len(texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data into Milvus\n",
        "\n",
        "### Create the collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient\n",
        "\n",
        "milvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n",
        "collection_name = \"my_rag_collection\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> As for the argument of `MilvusClient`:\n",
        "> - Setting the `uri` as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.\n",
        "> - If you have large scale of data, you can set up a more performant Milvus server on [docker or kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server uri, e.g.`http://localhost:19530`, as your `uri`.\n",
        "> - If you want to use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and Api key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#free-cluster-details) in Zilliz Cloud.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if the collection already exists and drop it if it does.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if milvus_client.has_collection(collection_name):\n",
        "    milvus_client.drop_collection(collection_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a new collection with specified parameters.\n",
        "\n",
        "If we don't specify any field information, Milvus will automatically create a default `id` field for primary key, and a `vector` field to store the vector data. A reserved JSON field is used to store non-schema-defined fields and their values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "milvus_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    dimension=embedding_dim,\n",
        "    metric_type=\"IP\",  # Inner product distance\n",
        "    # Strong consistency waits for all loads to complete, adding latency with large datasets\n",
        "    # consistency_level=\"Strong\",  # Supported values are (`\"Strong\"`, `\"Session\"`, `\"Bounded\"`, `\"Eventually\"`). See https://milvus.io/docs/consistency.md#Consistency-Level for more details.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insert data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "data = []\n",
        "\n",
        "for i, chunk in enumerate(tqdm(texts, desc=\"Processing chunks\")):\n",
        "    embedding = emb_text(chunk)\n",
        "    data.append({\"id\": i, \"vector\": embedding, \"text\": chunk})\n",
        "\n",
        "milvus_client.insert(collection_name=collection_name, data=data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build RAG\n",
        "\n",
        "### Retrieve data for a query\n",
        "\n",
        "Let's specify a query question about the parsed documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"What is the transformer architecture and how does self-attention work?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Search for the question in the collection and retrieve the semantic top-3 matches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_res = milvus_client.search(\n",
        "    collection_name=collection_name,\n",
        "    data=[emb_text(question)],\n",
        "    limit=3,\n",
        "    search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
        "    output_fields=[\"text\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the search results of the query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "retrieved_lines_with_distances = [\n",
        "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
        "]\n",
        "print(json.dumps(retrieved_lines_with_distances, indent=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use LLM to get a RAG response\n",
        "\n",
        "Convert the retrieved documents into a string format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = \"\\n\".join(\n",
        "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define system and user prompts for the Language Model. This prompt is assembled with the retrieved documents from Milvus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
        "\"\"\"\n",
        "USER_PROMPT = f\"\"\"\n",
        "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use OpenAI ChatGPT to generate a response based on the prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
        "    ],\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
