{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y-_run6vfI3"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/integration/build_RAG_with_milvus_and_contextual_ai_glm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>   <a href=\"https://github.com/milvus-io/bootcamp/blob/master/integration/build_RAG_with_milvus_and_contextual_ai_glm.ipynb\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/View%20on%20GitHub-555555?style=flat&logo=github&logoColor=white\" alt=\"GitHub Repository\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t6N2-43vfI6"
      },
      "source": [
        "# Build RAG with Milvus and Contextual AI GLM\n",
        "\n",
        "**Versions used:**\n",
        "- Milvus version `1.3.4`\n",
        "- Contextual AI client `0.9.0`\n",
        "\n",
        "[Contextual AI's Grounded Language Model (GLM)](https://contextual.ai/blog/introducing-grounded-language-model?utm_campaign=GLM-integration&utm_source=milvus&utm_medium=github&utm_content=notebook) is the most grounded language model in the world, making it the best choice for RAG and agentic use cases where minimizing hallucinations is critical. Unlike traditional LLMs that rely heavily on parametric knowledge, GLM prioritizes the knowledge you explicitly provide, ensuring responses are grounded in your specific data.\n",
        "\n",
        "In this tutorial, we'll show you how to build a Retrieval-Augmented Generation (RAG) pipeline using Milvus and Contextual AI's GLM. The pipeline integrates Milvus for vector storage, OpenAI for embeddings, and GLM for enterprise-grade, hallucination-free response generation.\n",
        "\n",
        "## Preparation\n",
        "### Dependencies and Environment\n",
        "\n",
        "To start, install the required dependencies by running the following command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyYH-4djvfI7"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade \"pymilvus[milvus_lite]\" contextual-client openai requests tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYVre4NYvfI7"
      },
      "source": [
        "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK0WtyDOvfI8",
        "outputId": "ba947e34-2d82-493b-bad6-9503cf3750dc"
      },
      "source": [
        "### Setting Up API Keys\n",
        "\n",
        "We will use Contextual AI for GLM access and OpenAI for text embeddings in this example. You should prepare the [CONTEXTUAL_API_KEY](https://docs.contextual.ai/user-guides/beginner-guide?utm_campaign=GLM-integration&utm_source=milvus&utm_medium=github&utm_content=notebook) and [OPENAI_API_KEY](https://platform.openai.com/docs/quickstart) as environment variables.\n",
        "\n",
        "If you're running this notebook in Google Colab, you can add your API keys as secrets. The code below dynamically handles both Colab secrets and environment variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e4ZxDsurr-vV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# API key variable names\n",
        "contextual_api_key_var = \"CONTEXTUAL_API_KEY\"\n",
        "openai_api_key_var = \"OPENAI_API_KEY\"\n",
        "\n",
        "# Fetch API keys\n",
        "try:\n",
        "    # If running in Colab, fetch API keys from Secrets\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "    contextual_api_key = userdata.get(contextual_api_key_var)\n",
        "    openai_api_key = userdata.get(openai_api_key_var)\n",
        "\n",
        "    if not contextual_api_key:\n",
        "        raise ValueError(f\"Secret '{contextual_api_key_var}' not found in Colab secrets.\")\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(f\"Secret '{openai_api_key_var}' not found in Colab secrets.\")\n",
        "except ImportError:\n",
        "    # If not running in Colab, fetch API keys from environment variables\n",
        "    contextual_api_key = os.getenv(contextual_api_key_var)\n",
        "    openai_api_key = os.getenv(openai_api_key_var)\n",
        "\n",
        "    if not contextual_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{contextual_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "    if not openai_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{openai_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "\n",
        "os.environ[\"CONTEXTUAL_API_KEY\"] = contextual_api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne5YIecfvfI8"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "We'll use a sample enterprise knowledge base to demonstrate GLM's grounding capabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PqCrOnJgr-vW"
      },
      "outputs": [],
      "source": [
        "# Sample enterprise knowledge base\n",
        "enterprise_docs = [\n",
        "    \"Our company's Q3 revenue reached $2.4 billion, representing a 15% year-over-year growth. The growth was primarily driven by our cloud services division, which saw a 28% increase in subscription revenue.\",\n",
        "    \"The new AI-powered customer service platform reduced average response time by 40% and improved customer satisfaction scores by 23%. The platform processes over 10,000 customer inquiries daily.\",\n",
        "    \"Our sustainability initiatives have resulted in a 35% reduction in carbon emissions compared to 2022. We've invested $50 million in renewable energy projects and achieved carbon neutrality in 12 facilities.\",\n",
        "    \"The recent product launch of our enterprise security suite generated $180 million in revenue within the first quarter. The suite includes advanced threat detection, automated incident response, and compliance management features.\",\n",
        "    \"Employee satisfaction surveys show a 18% improvement in workplace satisfaction, with 89% of employees reporting high job satisfaction. Our remote work policies and professional development programs were cited as key factors.\",\n",
        "    \"Our research and development investment increased by 22% this year, totaling $340 million. This investment has led to 15 new patent applications and the development of three breakthrough technologies in AI and quantum computing.\",\n",
        "    \"The company's market share in the enterprise software sector grew from 12% to 16% this year, driven by strategic partnerships with major cloud providers and enhanced product capabilities.\",\n",
        "    \"Customer retention rates improved to 94% this quarter, up from 87% last year. The improvement is attributed to our enhanced customer success program and 24/7 technical support availability.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xexSh51Hr-vW"
      },
      "source": [
        "### Prepare the LLM and Embedding Model\n",
        "\n",
        "We initialize the OpenAI client for embeddings and Contextual AI client for GLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yp5ykk3dr-vW"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from contextual import ContextualAI\n",
        "\n",
        "openai_client = OpenAI()\n",
        "contextual_client = ContextualAI(api_key=contextual_api_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzxMYi6Or-vW"
      },
      "source": [
        "Define a function to generate text embeddings using OpenAI client. We use the [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings) model as an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wpPm7JuBr-vW"
      },
      "outputs": [],
      "source": [
        "def emb_text(text):\n",
        "    return (\n",
        "        openai_client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
        "        .data[0]\n",
        "        .embedding\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHDc_OgQr-vW"
      },
      "source": [
        "Generate a test embedding and print its dimension and first few elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqvaclRNr-vW",
        "outputId": "2652b0a9-b6a0-481c-9090-869990843d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1536\n",
            "[0.009889289736747742, -0.005578675772994757, 0.00683477520942688, -0.03805781528353691, -0.01824733428657055, -0.04121600463986397, -0.007636285852640867, 0.03225184231996536, 0.018949154764413834, 9.352207416668534e-05]\n"
          ]
        }
      ],
      "source": [
        "test_embedding = emb_text(\"This is a test\")\n",
        "embedding_dim = len(test_embedding)\n",
        "print(embedding_dim)\n",
        "print(test_embedding[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpkANB5Vr-vW"
      },
      "source": [
        "## Load Data into Milvus\n",
        "\n",
        "### Create the Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P8FPQcBAr-vX"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient\n",
        "\n",
        "milvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n",
        "collection_name = \"my_rag_collection\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYGmsURlr-vX"
      },
      "source": [
        "> As for the argument of `MilvusClient`:\n",
        "> - Setting the `uri` as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.\n",
        "> - If you have large scale of data, you can set up a more performant Milvus server on [docker or kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server uri, e.g.`http://localhost:19530`, as your `uri`.\n",
        "> - If you want to use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and Api key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#free-cluster-details) in Zilliz Cloud.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYWa6GyMr-vX"
      },
      "source": [
        "Check if the collection already exists and drop it if it does.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UOJb2GVNr-vX"
      },
      "outputs": [],
      "source": [
        "if milvus_client.has_collection(collection_name):\n",
        "    milvus_client.drop_collection(collection_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ttEwd5gr-vX"
      },
      "source": [
        "Create a new collection with specified parameters.\n",
        "\n",
        "If we don't specify any field information, Milvus will automatically create a default `id` field for primary key, and a `vector` field to store the vector data. A reserved JSON field is used to store non-schema-defined fields and their values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4ziB45ITr-vX"
      },
      "outputs": [],
      "source": [
        "milvus_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    dimension=embedding_dim,\n",
        "    metric_type=\"IP\",  # Inner product distance\n",
        "    # Strong consistency waits for all loads to complete, adding latency with large datasets\n",
        "    # consistency_level=\"Strong\",  # Supported values are (`\"Strong\"`, `\"Session\"`, `\"Bounded\"`, `\"Eventually\"`). See https://milvus.io/docs/consistency.md#Consistency-Level for more details.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN5urqV4r-vX"
      },
      "source": [
        "### Insert data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IPEwTv0r-vX",
        "outputId": "5d829e07-3005-40d9-be14-a4fd5c59d3e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks: 100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'insert_count': 8, 'ids': [0, 1, 2, 3, 4, 5, 6, 7], 'cost': 0}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "data = []\n",
        "\n",
        "for i, chunk in enumerate(tqdm(enterprise_docs, desc=\"Processing chunks\")):\n",
        "    embedding = emb_text(chunk)\n",
        "    data.append({\"id\": i, \"vector\": embedding, \"text\": chunk})\n",
        "\n",
        "milvus_client.insert(collection_name=collection_name, data=data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjmK-L58r-vX"
      },
      "source": [
        "## Build RAG\n",
        "\n",
        "Contextual AI's GLM (Grounded Language Model) prioritizes provided knowledge over parametric knowledge, ensuring responses are grounded in your specific data. Let's create a RAG pipeline class to demonstrate GLM's capabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o0pe0GT7r-vX"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "class GLMRAGPipeline:\n",
        "    def __init__(self, collection_name: str = \"my_rag_collection\"):\n",
        "        self.milvus_client = milvus_client\n",
        "        self.openai_client = openai_client\n",
        "        self.contextual_client = contextual_client\n",
        "        self.collection_name = collection_name\n",
        "\n",
        "    def emb_text(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embeddings using OpenAI\"\"\"\n",
        "        return (\n",
        "            self.openai_client.embeddings.create(\n",
        "                input=text, model=\"text-embedding-3-small\"\n",
        "            )\n",
        "            .data[0]\n",
        "            .embedding\n",
        "        )\n",
        "\n",
        "    def retrieve_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieve relevant context from Milvus\"\"\"\n",
        "        search_res = self.milvus_client.search(\n",
        "            collection_name=self.collection_name,\n",
        "            data=[self.emb_text(query)],\n",
        "            limit=top_k,\n",
        "            search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
        "            output_fields=[\"text\"],\n",
        "        )\n",
        "\n",
        "        return [res[\"entity\"][\"text\"] for res in search_res[0]]\n",
        "\n",
        "    def generate_response(self, query: str, conversation_history: List[Dict] = None, top_k: int = 3) -> str:\n",
        "        \"\"\"Generate grounded response using GLM\"\"\"\n",
        "        # Retrieve relevant context\n",
        "        knowledge = self.retrieve_context(query, top_k)\n",
        "\n",
        "        # Prepare conversation messages (create a copy to avoid modifying the original)\n",
        "        messages = (conversation_history or []).copy()\n",
        "        messages.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "        # Generate response using GLM\n",
        "        response = self.contextual_client.generate.create(\n",
        "            model=\"v2\",\n",
        "            messages=messages,\n",
        "            knowledge=knowledge,\n",
        "            avoid_commentary=False,\n",
        "            max_new_tokens=1024,\n",
        "            temperature=0.1,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "        return response.response\n",
        "\n",
        "# Initialize RAG pipeline\n",
        "rag = GLMRAGPipeline()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3yu_L1ovfI9"
      },
      "source": [
        "### Retrieve data for a query\n",
        "\n",
        "Let's specify a query question about the enterprise knowledge base.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "02S-lBnAvfI-"
      },
      "outputs": [],
      "source": [
        "question = \"What was our Q3 revenue and what drove the growth?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URgSVm1CvfI-"
      },
      "source": [
        "Search for the question in the collection and retrieve the semantic top-3 matches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ccLIfpxvfI-",
        "outputId": "0afaa0de-603b-46da-97e6-e2ba553304cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved context:\n",
            "1. Our company's Q3 revenue reached $2.4 billion, representing a 15% year-over-year growth. The growth ...\n",
            "2. Our research and development investment increased by 22% this year, totaling $340 million. This inve...\n",
            "3. The recent product launch of our enterprise security suite generated $180 million in revenue within ...\n"
          ]
        }
      ],
      "source": [
        "context = rag.retrieve_context(question, top_k=3)\n",
        "\n",
        "# Display retrieved context\n",
        "print(\"Retrieved context:\")\n",
        "for i, doc in enumerate(context, 1):\n",
        "    print(f\"{i}. {doc[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IwB8UFtr-vY"
      },
      "source": [
        "### Use LLM to get a RAG response\n",
        "\n",
        "Generate a response using GLM with the retrieved context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdjAhj8or-vY",
        "outputId": "5ffb24d3-56b7-4c24-8a1d-43491631ec20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What was our Q3 revenue and what drove the growth?\n",
            "\n",
            "Response: <fact>Our company achieved Q3 revenue of $2.4 billion, showing a 15% year-over-year growth.[0]()</fact>\n",
            "\n",
            "<fact>The primary growth driver was our cloud services division, which experienced a 28% increase in subscription revenue.[0]()</fact>\n"
          ]
        }
      ],
      "source": [
        "response = rag.generate_response(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"\\nResponse: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKKZG5Q8r-vY"
      },
      "source": [
        "## Advanced Features\n",
        "\n",
        "GLM supports advanced features like multi-turn conversations with context preservation. Let's demonstrate this capability:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z2_Ktz9vfI-"
      },
      "source": [
        "### Multi-Turn Conversation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au9XsmRlvfI-",
        "outputId": "f0b6c823-3a9f-4810-9962-be292bcb05ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Tell me about our company's recent performance\n",
            "Assistant: <fact>Our company's Q3 revenue reached $2.4 billion, showing a 15% year-over-year growth, with the cloud services division specifically experiencing a 28% increase in subscription revenue.[0]()</fact>\n",
            "\n",
            "<fact>Key performance indicators show positive trends in customer relationships, with customer retention rates improving to 94% this quarter, up from 87% last year. This improvement is specifically attributed to our enhanced customer success program and 24/7 technical support availability.[0]()</fact>\n",
            "\n",
            "<fact>In terms of market position, the company has expanded its market share in the enterprise software sector from 12% to 16% this year. This growth is attributed to strategic partnerships with major cloud providers and enhanced product capabilities.[0]()</fact>\n",
            "\n",
            "User: What about our sustainability efforts?\n",
            "Assistant: <fact>Our company has made significant progress in sustainability, achieving a 35% reduction in carbon emissions compared to 2022. Additionally, we've invested $50 million in renewable energy projects and successfully achieved carbon neutrality in 12 facilities.[0]()</fact>\n",
            "\n",
            "User: How are our employees responding to these changes?\n",
            "Assistant: <fact>Employee satisfaction metrics show positive trends, with an 18% improvement in workplace satisfaction, and 89% of employees reporting high job satisfaction levels. The key contributing factors identified in this improvement are our remote work policies and professional development programs.[0]()</fact>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def multi_turn_conversation(rag_pipeline, initial_query: str, follow_up_queries: List[str]):\n",
        "    \"\"\"Demonstrate multi-turn conversation with GLM\"\"\"\n",
        "    conversation_history = []\n",
        "\n",
        "    # First turn\n",
        "    response = rag_pipeline.generate_response(initial_query, conversation_history)\n",
        "    conversation_history.extend([\n",
        "        {\"role\": \"user\", \"content\": initial_query},\n",
        "        {\"role\": \"assistant\", \"content\": response}\n",
        "    ])\n",
        "\n",
        "    print(f\"User: {initial_query}\")\n",
        "    print(f\"Assistant: {response}\\n\")\n",
        "\n",
        "    # Follow-up turns\n",
        "    for query in follow_up_queries:\n",
        "        response = rag_pipeline.generate_response(query, conversation_history)\n",
        "        conversation_history.extend([\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "            {\"role\": \"assistant\", \"content\": response}\n",
        "        ])\n",
        "        print(f\"User: {query}\")\n",
        "        print(f\"Assistant: {response}\\n\")\n",
        "\n",
        "    return conversation_history\n",
        "\n",
        "# Example multi-turn conversation\n",
        "initial_query = \"Tell me about our company's recent performance\"\n",
        "follow_ups = [\n",
        "    \"What about our sustainability efforts?\",\n",
        "    \"How are our employees responding to these changes?\"\n",
        "]\n",
        "\n",
        "conversation = multi_turn_conversation(rag, initial_query, follow_ups)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIJdIn02vfI-"
      },
      "source": [
        "### Dynamic Knowledge Injection\n",
        "\n",
        "You can add dynamic knowledge to retrieved context for enhanced responses:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozRM14rkvfI-",
        "outputId": "56ff98d1-4862-40a0-b092-8e379abd6c9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What are our current sustainability metrics?\n",
            "Response: <fact>Our key sustainability metrics include:\n",
            "- 35% reduction in carbon emissions compared to 2022[0]() \n",
            "- $50 million investment in renewable energy projects[0]() \n",
            "- 100% renewable energy usage achieved in European operations[0]()</fact>\n",
            "\n",
            "<fact>Additional environmental initiatives include a partnership with three major environmental organizations for planting 50,000 trees this year.[0]()</fact>\n",
            "\n",
            "<fact>Supporting operational metrics show:\n",
            "- 94% customer retention rate (up from 87% last year)[0]() \n",
            "- 89% of employees report high job satisfaction, with an 18% improvement in workplace satisfaction overall[0]()</fact>\n"
          ]
        }
      ],
      "source": [
        "def add_dynamic_knowledge(rag_pipeline, query: str, additional_knowledge: List[str], top_k: int = 3):\n",
        "    \"\"\"Add dynamic knowledge to retrieved context\"\"\"\n",
        "    retrieved_context = rag_pipeline.retrieve_context(query, top_k)\n",
        "    combined_knowledge = retrieved_context + additional_knowledge\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": query}]\n",
        "\n",
        "    response = rag_pipeline.contextual_client.generate.create(\n",
        "        model=\"v2\",\n",
        "        messages=messages,\n",
        "        knowledge=combined_knowledge,\n",
        "        avoid_commentary=False,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    return response.response\n",
        "\n",
        "# Example with additional knowledge\n",
        "query = \"What are our current sustainability metrics?\"\n",
        "additional_knowledge = [\n",
        "    \"Our latest sustainability report shows we've achieved 100% renewable energy usage in our European operations.\",\n",
        "    \"We've partnered with three major environmental organizations to plant 50,000 trees this year.\"\n",
        "]\n",
        "\n",
        "response = add_dynamic_knowledge(rag, query, additional_knowledge)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Response: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV3imwH8vfI-"
      },
      "source": [
        "Great! We have successfully built a RAG pipeline with Milvus and Contextual AI GLM. GLM provides enterprise-grade, hallucination-free responses by prioritizing your provided knowledge over parametric knowledge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmcP2qfVvfI-"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This guide demonstrated how to build a production-ready RAG system using Milvus and Contextual AI's GLM. The combination provides:\n",
        "\n",
        "### Key Advantages of GLM + Milvus:\n",
        "\n",
        "- **Hallucination-Free Responses**: GLM prioritizes provided knowledge over parametric knowledge\n",
        "- **Enterprise-Grade Reliability**: Designed for production RAG applications with minimal tuning\n",
        "- **Multi-Turn Conversations**: Full conversation context preservation across interactions\n",
        "- **Scalable Vector Search**: Milvus enables efficient similarity search across large document collections\n",
        "- **Dynamic Knowledge Management**: Easy integration of diverse knowledge sources\n",
        "\n",
        "### Technical Implementation:\n",
        "\n",
        "- **Complete RAG Pipeline**: Milvus vector store + OpenAI embeddings + GLM generation\n",
        "- **Multi-Turn Support**: Conversation history preservation for context-aware responses\n",
        "- **Advanced Features**: Dynamic knowledge injection and response quality analysis\n",
        "- **Production Ready**: Robust architecture suitable for enterprise deployment\n",
        "\n",
        "### Key Advantages:\n",
        "\n",
        "- **Hallucination-Free Responses**: GLM prioritizes provided knowledge over parametric knowledge\n",
        "- **Enterprise-Grade Reliability**: Production-ready with minimal tuning required\n",
        "- **Multi-Turn Conversations**: Full conversation context preservation across interactions\n",
        "- **Scalable Vector Search**: Milvus enables efficient similarity search across large document collections\n",
        "- **Dynamic Knowledge Management**: Easy integration of diverse knowledge sources\n",
        "- **Performance Monitoring**: Built-in metrics for optimization and scaling\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Experiment with different knowledge base structures for your specific domain\n",
        "- Implement custom response validation and quality metrics\n",
        "- Scale to larger document collections with Milvus clustering\n",
        "- Integrate with your existing enterprise systems and workflows\n",
        "\n",
        "This integration represents a significant advancement in RAG technology, providing the reliability and control needed for enterprise applications while maintaining the flexibility and scalability of modern vector databases.\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to get started?** This notebook provides a complete, production-ready example of integrating Contextual AI's GLM with Milvus for enterprise-grade RAG applications. The combination of GLM's grounding capabilities and Milvus's powerful vector search features creates a robust foundation for reliable, scalable AI applications.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
