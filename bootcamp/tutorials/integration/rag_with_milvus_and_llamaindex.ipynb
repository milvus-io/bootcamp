{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1496f9de",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/bootcamp/tutorials/integration/rag_with_milvus_and_llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b692c73",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Milvus and LlamaIndex\n",
    "\n",
    "This guide demonstrates how to build a Retrieval-Augmented Generation (RAG) system using LlamaIndex and Milvus.\n",
    "\n",
    "The RAG system combines a retrieval system with a generative model to generate new text based on a given prompt. The system first retrieves relevant documents from a corpus using a vector similarity search engine like Milvus, and then uses a generative model to generate new text based on the retrieved documents.\n",
    "\n",
    "[LlamaIndex](https://www.llamaindex.ai/) is a simple, flexible data framework for connecting custom data sources to large language models (LLMs). [Milvus](https://milvus.io/) is the world's most advanced open-source vector database, built to power embedding similarity search and AI applications.\n",
    "\n",
    "In this notebook we are going to show a quick demo of using the MilvusVectorStore. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f81e2c81",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "### Install dependencies\n",
    "Code snippets on this page require pymilvus and llamaindex dependencies. You can install them using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa64984",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pymilvus>=2.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c18ca",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80700a",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc8c56",
   "metadata": {},
   "source": [
    "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b97a89",
   "metadata": {},
   "source": [
    "### Setup OpenAI\n",
    "\n",
    "Lets first begin by adding the openai api key. This will allow us to access chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c9f4d21-145a-401e-95ff-ccb259e8ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-***********\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3d4e638",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "You can download sample data with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p 'data/paul_graham/'\n",
    "! wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59ff935d",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Generate our data\n",
    "As a first example, lets generate a document from the file found in the `data/paul_graham/` folder. In this folder there is a single essay from Paul Graham titled `What I Worked On`. To generate the documents we will use the SimpleDirectoryReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 11c3a6fe-799e-4e40-8122-2339936c2722\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd270925",
   "metadata": {},
   "source": [
    "### Create an index across the data\n",
    "\n",
    "Now that we have a document, we can can create an index and insert the document. For the index we will use a GPTMilvusIndex. GPTMilvusIndex takes in a few arguments:\n",
    "\n",
    "- `uri (str, optional)`: The URI to connect to, comes in the form of \"https://address:port\" if using Milvus or Zilliz Cloud service, or \"path/to/local/milvus.db\" if using a lite local Milvus. Defaults to \"./milvus_llamaindex.db\".\n",
    "- `token (str, optional)`: The token for log in. Empty if not using rbac, if using rbac it will most likely be \"username:password\". Defaults to \"\".\n",
    "- `collection_name (str, optional)`: The name of the collection where data will be stored. Defaults to \"llamalection\".\n",
    "- `dim (int, optional)`: The dimension of the embeddings. If it is not provided, collection creation will be done on first insert. Defaults to None.\n",
    "- `embedding_field (str, optional)`: The name of the embedding field for the collection, defaults to DEFAULT_EMBEDDING_KEY.\n",
    "- `doc_id_field (str, optional)`: The name of the doc_id field for the collection, defaults to DEFAULT_DOC_ID_KEY.\n",
    "- `similarity_metric (str, optional)`: The similarity metric to use, currently supports IP and L2. Defaults to \"IP\".\n",
    "- `consistency_level (str, optional)`: Which consistency level to use for a newly created collection. Defaults to \"Strong\".\n",
    "- `overwrite (bool, optional)`: Whether to overwrite existing collection with same name. Defaults to False.\n",
    "- `text_key (str, optional)`: What key text is stored in in the passed collection. Used when bringing your own collection. Defaults to None.\n",
    "- `index_config (dict, optional)`: The configuration used for building the Milvus index. Defaults to None.\n",
    "- `search_config (dict, optional)`: The configuration used for searching the Milvus index. Note that this must be compatible with the index type specified by index_config. Defaults to None.\n",
    "\n",
    "> Please note that **Milvus Lite** requires `pymilvus>=2.4.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1558b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index over the documents\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "\n",
    "vector_store = MilvusVectorStore(uri=\"./milvus_demo.db\", dim=1536, overwrite=True)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> For the parameters of `MilvusVectorStore`:\n",
    "> - Setting the `uri` as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.\n",
    "> - If you have large scale of data, you can set up a more performant Milvus server on [docker or kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server uri, e.g.`http://localhost:19530`, as your `uri`.\n",
    "> - If you want to use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and Api key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#free-cluster-details) in Zilliz Cloud."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {},
   "source": [
    "### Query the data\n",
    "Now that we have our document stored in the index, we can ask questions against the index. The index will use the data stored in itself as the knowledge base for chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35369eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author learned about programming on early computers like the IBM 1401 using Fortran, the\n",
      "limitations of early computing technology, the transition to microcomputers, and the excitement of\n",
      "having a personal computer like the TRS-80. Additionally, the author explored different academic\n",
      "paths, initially planning to study philosophy but eventually switching to AI due to a lack of\n",
      "interest in philosophy courses. Later on, the author pursued art education, attending RISD and the\n",
      "Accademia di Belli Arti in Florence, where they encountered a different approach to teaching art.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99212d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with the stress and challenges related to managing Hacker News was a difficult moment for\n",
      "the author.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What was a hard moment for the author?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64cc925b",
   "metadata": {},
   "source": [
    "This next test shows that overwriting removes the previous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d641e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: The author is the individual who created the content or work in question.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "\n",
    "vector_store = MilvusVectorStore(uri=\"./milvus_demo.db\", dim=1536, overwrite=True)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    [Document(text=\"The number that is being searched for is ten.\")],\n",
    "    storage_context,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"Who is the author?\")\n",
    "print(\"Res:\", res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8123529",
   "metadata": {},
   "source": [
    "The next test shows adding additional data to an already existing  index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c429a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: The number is ten.\n"
     ]
    }
   ],
   "source": [
    "del index, vector_store, storage_context, query_engine\n",
    "\n",
    "vector_store = MilvusVectorStore(uri=\"./milvus_demo.db\", overwrite=False)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"What is the number?\")\n",
    "print(\"Res:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5287c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Paul Graham\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"Who is the author?\")\n",
    "print(\"Res:\", res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}