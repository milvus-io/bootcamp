{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Milvus, LangChain & Anthropic Claude\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/behroozazarkhalili/Milvus-RAG/blob/master/RAG_Milvus_LangChain_Anthropic.ipynb)\n",
    "\n",
    "**Note**: If the repository is private, you'll need to make it public for the Colab badge to work, or manually upload the notebook to Colab.\n",
    "\n",
    "**Architecture**: Document Processing â†’ Embedding Generation â†’ Vector Storage â†’ Retrieval â†’ Generation\n",
    "\n",
    "- **Milvus**: Vector database for similarity search\n",
    "- **LangChain**: LLM application framework  \n",
    "- **Anthropic Claude**: Response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "!pip install -q pymilvus langchain langchain-community anthropic sentence-transformers python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites & Environment Setup\n",
    "\n",
    "### ğŸ”§ **System Requirements**\n",
    "- **Python**: 3.8+ with pip package manager\n",
    "- **Docker**: For Milvus vector database server\n",
    "- **Memory**: 4GB+ RAM recommended for embedding processing\n",
    "- **Storage**: 2GB+ available space for vector indices\n",
    "\n",
    "### ğŸš€ **Quick Start Setup**\n",
    "\n",
    "#### 1. **Start Milvus Vector Database**\n",
    "\n",
    "**Step-by-Step Milvus Installation:**\n",
    "\n",
    "```bash\n",
    "# Download the official Milvus standalone script\n",
    "curl -sfL https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/standalone_embed.sh -o milvus.sh\n",
    "\n",
    "# Make the script executable  \n",
    "chmod +x milvus.sh\n",
    "\n",
    "# Start Milvus server with all required services\n",
    "bash milvus.sh start\n",
    "```\n",
    "\n",
    "**What Each Command Does:**\n",
    "\n",
    "**ğŸ”½ Download Script:**\n",
    "- `curl -sfL`: Download silently with redirect following and failure on HTTP errors\n",
    "- Downloads the official Milvus standalone deployment script\n",
    "- Saves as `milvus.sh` in your current directory\n",
    "\n",
    "**ğŸ”§ Make Executable:**\n",
    "- `chmod +x`: Grants execution permissions to the script\n",
    "- Required for bash script execution\n",
    "\n",
    "**ğŸš€ Start Services:**\n",
    "- `bash milvus.sh start`: Launches complete Milvus stack via Docker\n",
    "  - **Milvus Server**: Vector database engine (port 19530)\n",
    "  - **Etcd**: Metadata storage and service discovery\n",
    "  - **MinIO**: Object storage for vector data persistence\n",
    "\n",
    "**âœ… Verification:**\n",
    "```bash\n",
    "# Check if all Milvus containers are running\n",
    "docker ps | grep milvus\n",
    "\n",
    "# Expected output: 3 running containers\n",
    "# - milvus-standalone\n",
    "# - milvus-etcd  \n",
    "# - milvus-minio\n",
    "```\n",
    "\n",
    "*Milvus will be accessible at `localhost:19530` for your RAG application*\n",
    "\n",
    "#### 2. **Configure API Access**\n",
    "Create `.env` file in project root:\n",
    "```bash\n",
    "# Required: Anthropic Claude API\n",
    "ANTHROPIC_API_KEY=your-anthropic-api-key-here\n",
    "\n",
    "# Optional: Additional configurations\n",
    "MILVUS_HOST=localhost\n",
    "MILVUS_PORT=19530\n",
    "```\n",
    "\n",
    "#### 3. **Prepare Documents**\n",
    "```bash\n",
    "# Create directories for your documents\n",
    "mkdir -p pdf_files sample_docs\n",
    "\n",
    "# Add your PDF files to pdf_files/\n",
    "# Supported formats: PDF, TXT, MD\n",
    "```\n",
    "\n",
    "### ğŸ¯ **Ready to Run?**\n",
    "âœ… Milvus server running  \n",
    "âœ… Environment variables set  \n",
    "âœ… Documents in place  \n",
    "âœ… Dependencies installed  \n",
    "\n",
    "**â†’ Proceed to next cell to import libraries and start building your RAG system!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries & Setup Logging\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
    "\n",
    "# Milvus imports - using modern MilvusClient only\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "# Embedding and LLM imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import anthropic\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Configuration\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration class for RAG application.\"\"\"\n",
    "    \n",
    "    # Milvus configuration\n",
    "    milvus_host: str = \"localhost\"\n",
    "    milvus_port: str = \"19530\"\n",
    "    collection_name: str = \"rag_documents\"\n",
    "    \n",
    "    # Embedding configuration\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "    embedding_dim: int = 384\n",
    "    \n",
    "    # Text processing configuration\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "    \n",
    "    # Retrieval configuration\n",
    "    top_k: int = 5\n",
    "    \n",
    "    # Anthropic configuration\n",
    "    ANTHROPIC_API_KEY: Optional[str] = \"your_api_key\"\n",
    "    model_name: str = \"claude-sonnet-4-20250514\"\n",
    "    max_tokens: int = 1000\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Post-initialization to set API key from environment if not provided.\"\"\"\n",
    "        if self.ANTHROPIC_API_KEY is None:\n",
    "            self.ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "        if not self.ANTHROPIC_API_KEY:\n",
    "            raise ValueError(\"ANTHROPIC_API_KEY must be set in environment or config\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = RAGConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Processor\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document loading and text chunking.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize document processor.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Maximum size of text chunks\n",
    "            chunk_overlap: Overlap between consecutive chunks\n",
    "        \"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "    def load_documents_from_directory(self, directory_path: str, file_types: List[str] = None) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load documents from a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory_path: Path to directory containing documents\n",
    "            file_types: List of file extensions to load (e.g., ['.txt', '.pdf'])\n",
    "            \n",
    "        Returns:\n",
    "            List of loaded documents\n",
    "        \"\"\"\n",
    "        if file_types is None:\n",
    "            file_types = [\".txt\", \".pdf\", \".md\"]\n",
    "            \n",
    "        documents = []\n",
    "        \n",
    "        for file_type in file_types:\n",
    "            if file_type == \".pdf\":\n",
    "                loader = DirectoryLoader(\n",
    "                    directory_path,\n",
    "                    glob=f\"**/*{file_type}\",\n",
    "                    loader_cls=PyPDFLoader\n",
    "                )\n",
    "            else:\n",
    "                loader = DirectoryLoader(\n",
    "                    directory_path,\n",
    "                    glob=f\"**/*{file_type}\",\n",
    "                    loader_cls=TextLoader\n",
    "                )\n",
    "            \n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "            \n",
    "        logger.info(f\"Loaded {len(documents)} documents from {directory_path}\")\n",
    "        return documents\n",
    "    \n",
    "    def load_single_document(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load a single document.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document file\n",
    "            \n",
    "        Returns:\n",
    "            List containing the loaded document\n",
    "        \"\"\"\n",
    "        file_extension = Path(file_path).suffix.lower()\n",
    "        \n",
    "        if file_extension == \".pdf\":\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        else:\n",
    "            loader = TextLoader(file_path)\n",
    "            \n",
    "        documents = loader.load()\n",
    "        logger.info(f\"Loaded document from {file_path}\")\n",
    "        return documents\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split documents into smaller chunks.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of documents to chunk\n",
    "            \n",
    "        Returns:\n",
    "            List of document chunks\n",
    "        \"\"\"\n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "        logger.info(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Generator\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Handles text embedding generation using sentence transformers.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize embedding generator.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the sentence transformer model\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        logger.info(f\"Initialized embedding model: {model_name} (dim: {self.embedding_dim})\")\n",
    "    \n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embedding for a single text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to embed\n",
    "            \n",
    "        Returns:\n",
    "            Embedding vector as numpy array\n",
    "        \"\"\"\n",
    "        embedding = self.model.encode(text, convert_to_numpy=True)\n",
    "        return embedding\n",
    "    \n",
    "    def embed_documents(self, documents: List[Document]) -> Tuple[List[str], List[np.ndarray], List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of documents.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of documents to embed\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (texts, embeddings, metadata)\n",
    "        \"\"\"\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        metadata = [doc.metadata for doc in documents]\n",
    "        \n",
    "        logger.info(f\"Generating embeddings for {len(texts)} documents...\")\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "        \n",
    "        return texts, embeddings, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusVectorStore:\n",
    "    \"\"\"Handles Milvus vector database operations using modern MilvusClient.\"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"localhost\", port: str = \"19530\", collection_name: str = \"rag_documents\"):\n",
    "        \"\"\"\n",
    "        Initialize Milvus vector store.\n",
    "        \n",
    "        Args:\n",
    "            host: Milvus server host\n",
    "            port: Milvus server port\n",
    "            collection_name: Name of the collection to use\n",
    "        \"\"\"\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Connect to Milvus using the modern MilvusClient\n",
    "        self._connect()\n",
    "    \n",
    "    def _connect(self) -> None:\n",
    "        \"\"\"\n",
    "        Establish connection to Milvus server using MilvusClient.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use the modern MilvusClient with uri endpoint\n",
    "            uri = f\"http://{self.host}:{self.port}\"\n",
    "            self.client = MilvusClient(uri=uri)\n",
    "            logger.info(f\"Connected to Milvus at {uri}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to Milvus: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_collection(self, embedding_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Create a new collection using MilvusClient's simplified approach.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of the embedding vectors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Drop existing collection if it exists\n",
    "            if self.client.has_collection(self.collection_name):\n",
    "                self.client.drop_collection(self.collection_name)\n",
    "                logger.info(f\"Dropped existing collection: {self.collection_name}\")\n",
    "            \n",
    "            # MilvusClient uses a simplified schema creation approach\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                dimension=embedding_dim,\n",
    "                metric_type=\"COSINE\",\n",
    "                index_type=\"IVF_FLAT\",\n",
    "                index_params={\"nlist\": 1024}\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Created collection: {self.collection_name} with dimension {embedding_dim}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating collection: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_collection(self) -> None:\n",
    "        \"\"\"\n",
    "        Load existing collection.\n",
    "        \"\"\"\n",
    "        if self.client.has_collection(self.collection_name):\n",
    "            logger.info(f\"Collection {self.collection_name} exists and is ready\")\n",
    "        else:\n",
    "            raise ValueError(f\"Collection {self.collection_name} does not exist\")\n",
    "    \n",
    "    def add_documents(self, texts: List[str], embeddings: List[np.ndarray], metadata: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"\n",
    "        Add documents to the collection using MilvusClient.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of document texts\n",
    "            embeddings: List of embedding vectors\n",
    "            metadata: List of metadata dictionaries\n",
    "        \"\"\"\n",
    "        # Prepare data for MilvusClient insertion (include id field)\n",
    "        data = []\n",
    "        for i in range(len(texts)):\n",
    "            data.append({\n",
    "                \"id\": i,  # Add required id field\n",
    "                \"text\": texts[i],\n",
    "                \"vector\": embeddings[i].tolist(),\n",
    "                \"metadata\": json.dumps(metadata[i])\n",
    "            })\n",
    "        \n",
    "        # Insert data using MilvusClient\n",
    "        result = self.client.insert(\n",
    "            collection_name=self.collection_name,\n",
    "            data=data\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Added {len(texts)} documents to collection\")\n",
    "        return result\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for similar documents using MilvusClient.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: Query embedding vector\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of search results with text, metadata, and similarity scores\n",
    "        \"\"\"\n",
    "        # Ensure collection exists\n",
    "        if not self.client.has_collection(self.collection_name):\n",
    "            raise ValueError(f\"Collection {self.collection_name} does not exist\")\n",
    "        \n",
    "        # Use MilvusClient search method with proper vector field specification\n",
    "        results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            data=[query_embedding.tolist()],\n",
    "            anns_field=\"vector\",  # Specify the vector field name\n",
    "            search_params={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "            limit=top_k,\n",
    "            output_fields=[\"text\", \"metadata\"]\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for hit in results[0]:\n",
    "            formatted_results.append({\n",
    "                \"text\": hit[\"text\"],\n",
    "                \"metadata\": json.loads(hit[\"metadata\"]),\n",
    "                \"score\": 1.0 - hit[\"distance\"],  # Convert distance to similarity score for COSINE\n",
    "                \"id\": hit[\"id\"]\n",
    "            })\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Close the MilvusClient connection.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'client'):\n",
    "            self.client.close()\n",
    "            logger.info(\"Closed MilvusClient connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaudeGenerator:\n",
    "    \"\"\"Handles text generation using Anthropic Claude.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model_name: str = \"claude-sonnet-4-20250514\", max_tokens: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize Claude generator.\n",
    "        \n",
    "        Args:\n",
    "            api_key: Anthropic API key\n",
    "            model_name: Claude model to use\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \"\"\"\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "        self.max_tokens = max_tokens\n",
    "        logger.info(f\"Initialized Claude generator with model: {model_name}\")\n",
    "    \n",
    "    def generate_response(self, query: str, context_documents: List[Dict[str, Any]], \n",
    "                         system_prompt: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate response based on query and retrieved context.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            context_documents: Retrieved documents from vector search\n",
    "            system_prompt: Optional system prompt to guide the model\n",
    "            \n",
    "        Returns:\n",
    "            Generated response text\n",
    "        \"\"\"\n",
    "        # Prepare context from retrieved documents\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document {i+1}:\\n{doc['text']}\"\n",
    "            for i, doc in enumerate(context_documents)\n",
    "        ])\n",
    "        \n",
    "        # Default system prompt\n",
    "        if system_prompt is None:\n",
    "            system_prompt = (\n",
    "                \"You are a helpful AI assistant that answers questions based on the provided context. \"\n",
    "                \"Use only the information from the context to answer questions. \"\n",
    "                \"If the context doesn't contain enough information to answer the question, \"\n",
    "                \"say so explicitly and suggest what additional information might be needed.\"\n",
    "            )\n",
    "        \n",
    "        # Construct the prompt\n",
    "        user_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive answer based on the context above.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Generate response using Claude\n",
    "            response = self.client.messages.create(\n",
    "                model=self.model_name,\n",
    "                max_tokens=self.max_tokens,\n",
    "                system=system_prompt,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            return response.content[0].text\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {e}\")\n",
    "            raise e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"Main RAG pipeline that orchestrates all components.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        \"\"\"\n",
    "        Initialize RAG pipeline with configuration.\n",
    "        \n",
    "        Args:\n",
    "            config: RAG configuration object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize components\n",
    "        self.doc_processor = DocumentProcessor(\n",
    "            chunk_size=config.chunk_size,\n",
    "            chunk_overlap=config.chunk_overlap\n",
    "        )\n",
    "        \n",
    "        self.embedding_generator = EmbeddingGenerator(config.embedding_model)\n",
    "        \n",
    "        self.vector_store = MilvusVectorStore(\n",
    "            host=config.milvus_host,\n",
    "            port=config.milvus_port,\n",
    "            collection_name=config.collection_name\n",
    "        )\n",
    "        \n",
    "        self.llm = ClaudeGenerator(\n",
    "            api_key=config.ANTHROPIC_API_KEY,\n",
    "            model_name=config.model_name,\n",
    "            max_tokens=config.max_tokens\n",
    "        )\n",
    "        \n",
    "        logger.info(\"RAG pipeline initialized successfully\")\n",
    "    \n",
    "    def index_documents(self, document_source: str, is_directory: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Index documents into the vector store.\n",
    "        \n",
    "        Args:\n",
    "            document_source: Path to documents (file or directory)\n",
    "            is_directory: Whether the source is a directory or single file\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting document indexing from: {document_source}\")\n",
    "        \n",
    "        # Load documents\n",
    "        if is_directory:\n",
    "            documents = self.doc_processor.load_documents_from_directory(document_source)\n",
    "        else:\n",
    "            documents = self.doc_processor.load_single_document(document_source)\n",
    "        \n",
    "        if not documents:\n",
    "            logger.warning(\"No documents found to index\")\n",
    "            return\n",
    "        \n",
    "        # Chunk documents\n",
    "        chunks = self.doc_processor.chunk_documents(documents)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        texts, embeddings, metadata = self.embedding_generator.embed_documents(chunks)\n",
    "        \n",
    "        # Create collection with correct embedding dimension\n",
    "        self.vector_store.create_collection(self.embedding_generator.embedding_dim)\n",
    "        \n",
    "        # Add documents to vector store\n",
    "        self.vector_store.add_documents(texts, embeddings, metadata)\n",
    "        \n",
    "        logger.info(f\"Successfully indexed {len(chunks)} document chunks\")\n",
    "    \n",
    "    def query(self, question: str, top_k: Optional[int] = None, \n",
    "              system_prompt: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            top_k: Number of documents to retrieve (uses config default if None)\n",
    "            system_prompt: Optional system prompt for the LLM\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing answer, retrieved documents, and metadata\n",
    "        \"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.config.top_k\n",
    "        \n",
    "        logger.info(f\"Processing query: {question[:100]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Ensure collection exists\n",
    "            if not self.vector_store.client.has_collection(self.vector_store.collection_name):\n",
    "                raise ValueError(f\"Collection {self.vector_store.collection_name} does not exist. Please index documents first.\")\n",
    "            \n",
    "            # Generate query embedding\n",
    "            query_embedding = self.embedding_generator.embed_text(question)\n",
    "            \n",
    "            # Retrieve relevant documents\n",
    "            retrieved_docs = self.vector_store.search(query_embedding, top_k=top_k)\n",
    "            \n",
    "            if not retrieved_docs:\n",
    "                return {\n",
    "                    \"answer\": \"No relevant documents found for your query.\",\n",
    "                    \"retrieved_documents\": [],\n",
    "                    \"num_retrieved\": 0\n",
    "                }\n",
    "            \n",
    "            # Generate answer using Claude\n",
    "            answer = self.llm.generate_response(\n",
    "                query=question,\n",
    "                context_documents=retrieved_docs,\n",
    "                system_prompt=system_prompt\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Query processed successfully\")\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"retrieved_documents\": retrieved_docs,\n",
    "                \"num_retrieved\": len(retrieved_docs)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {e}\")\n",
    "            return {\n",
    "                \"answer\": f\"Error processing query: {str(e)}\",\n",
    "                \"retrieved_documents\": [],\n",
    "                \"num_retrieved\": 0\n",
    "            }\n",
    "    \n",
    "    def get_collection_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get statistics about the current collection using MilvusClient.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with collection statistics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if collection exists\n",
    "            if not self.vector_store.client.has_collection(self.vector_store.collection_name):\n",
    "                return {\n",
    "                    \"error\": f\"Collection {self.vector_store.collection_name} does not exist. Please index documents first.\"\n",
    "                }\n",
    "            \n",
    "            # Get collection statistics using MilvusClient\n",
    "            collection_info = self.vector_store.client.describe_collection(self.vector_store.collection_name)\n",
    "            \n",
    "            # Get entity count - this might not be available in MilvusClient, so we'll provide what we can\n",
    "            stats = {\n",
    "                \"collection_name\": self.config.collection_name,\n",
    "                \"embedding_dim\": self.embedding_generator.embedding_dim,\n",
    "                \"embedding_model\": self.config.embedding_model,\n",
    "                \"collection_exists\": True,\n",
    "                \"schema\": collection_info if collection_info else \"Schema information not available\"\n",
    "            }\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting collection stats: {e}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Load Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sample Documents from Files\n",
    "sample_documents_dir = \"sample_docs\"\n",
    "\n",
    "# Verify sample documents exist\n",
    "import glob\n",
    "sample_files = glob.glob(f\"{sample_documents_dir}/*.txt\")\n",
    "\n",
    "if sample_files:\n",
    "    print(f\"Found {len(sample_files)} sample documents:\")\n",
    "    for file in sample_files:\n",
    "        print(f\"  ğŸ“„ {file}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ No sample documents found in {sample_documents_dir}/\")\n",
    "    print(\"ğŸ“ Expected files: ai_overview.txt, vector_databases.txt, rag_systems.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG Pipeline\n",
    "rag = RAGPipeline(config)\n",
    "\n",
    "print(\"RAG Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Sample Documents & Get Stats\n",
    "rag.index_documents(sample_documents_dir, is_directory=True)\n",
    "\n",
    "# Get collection statistics\n",
    "stats = rag.get_collection_stats()\n",
    "print(\"\\nCollection Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Sample Queries\n",
    "queries = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does Milvus work as a vector database?\",\n",
    "    \"Explain the RAG pipeline steps\",\n",
    "    \"What are the differences between machine learning and deep learning?\"\n",
    "]\n",
    "\n",
    "print(\"Testing RAG Pipeline with sample queries...\\n\")\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = rag.query(query, top_k=3)\n",
    "    \n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"\\nRetrieved {result['num_retrieved']} documents:\")\n",
    "    \n",
    "    for j, doc in enumerate(result['retrieved_documents'], 1):\n",
    "        print(f\"  {j}. Score: {doc['score']:.4f}\")\n",
    "        print(f\"     Text: {doc['text'][:100]}...\")\n",
    "        print(f\"     Source: {doc['metadata'].get('source', 'Unknown')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with Custom System Prompt\n",
    "custom_system_prompt = \"\"\"\n",
    "You are an expert AI researcher and educator. When answering questions:\n",
    "1. Provide detailed, technical explanations\n",
    "2. Include relevant examples when possible\n",
    "3. Mention any limitations or caveats\n",
    "4. Cite the specific documents used in your response\n",
    "5. If asked about comparisons, provide a structured analysis\n",
    "\"\"\"\n",
    "\n",
    "query = \"Compare machine learning and deep learning approaches\"\n",
    "result = rag.query(query, top_k=2, system_prompt=custom_system_prompt)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer with custom prompt: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Session & Utility Functions\n",
    "def interactive_rag_session(rag_pipeline: RAGPipeline) -> None:\n",
    "    \"\"\"Run an interactive RAG session.\"\"\"\n",
    "    print(\"Interactive RAG Session Started!\")\n",
    "    print(\"Type 'quit' to exit, 'stats' for collection statistics\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nEnter your question: \").strip()\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.lower() == 'stats':\n",
    "            stats = rag_pipeline.get_collection_stats()\n",
    "            print(\"\\nCollection Statistics:\")\n",
    "            for key, value in stats.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            continue\n",
    "        \n",
    "        if not query:\n",
    "            print(\"Please enter a valid question.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nProcessing...\")\n",
    "        result = rag_pipeline.query(query)\n",
    "        \n",
    "        print(f\"\\nAnswer: {result['answer']}\")\n",
    "        print(f\"\\nBased on {result['num_retrieved']} retrieved documents.\")\n",
    "\n",
    "# Uncomment to run interactive session\n",
    "# interactive_rag_session(rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource Cleanup Functions\n",
    "def cleanup_resources() -> None:\n",
    "    \"\"\"Clean up resources and connections.\"\"\"\n",
    "    try:\n",
    "        # Close MilvusClient connections if they exist\n",
    "        if 'rag' in globals() and hasattr(rag.vector_store, 'client'):\n",
    "            rag.vector_store.close()\n",
    "            \n",
    "        if 'pdf_rag' in globals() and hasattr(pdf_rag.vector_store, 'client'):\n",
    "            pdf_rag.vector_store.close()\n",
    "            \n",
    "        print(\"Closed MilvusClient connections\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "# Perform cleanup\n",
    "cleanup_resources()\n",
    "\n",
    "print(\"Cleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF RAG Workflow\n",
    "\n",
    "**Overview**: Complete workflow for processing PDF documents using the RAG pipeline.\n",
    "\n",
    "**Requirements**: \n",
    "- PDF files placed in `pdf_files/` directory\n",
    "- Milvus server running (`bash milvus.sh start`)\n",
    "- Environment variables configured\n",
    "\n",
    "**Workflow Steps**:\n",
    "1. **Setup PDF Pipeline** - Initialize PDF-specific RAG configuration\n",
    "2. **Index PDF Documents** - Process and embed PDF content into vector store  \n",
    "3. **Test PDF Queries** - Validate system with sample questions\n",
    "4. **Interactive Session** - Live question-answering interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup PDF RAG Pipeline\n",
    "print(\"ğŸš€ Setting up PDF RAG Pipeline...\")\n",
    "\n",
    "# Create PDF-specific configuration and pipeline\n",
    "pdf_config = RAGConfig(collection_name=\"pdf_documents\")\n",
    "pdf_rag = RAGPipeline(pdf_config)\n",
    "\n",
    "# Check for PDF files\n",
    "import glob\n",
    "pdf_files = glob.glob(\"pdf_files/*.pdf\")\n",
    "print(f\"ğŸ“ Found {len(pdf_files)} PDF files:\")\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"  ğŸ“„ {pdf_file}\")\n",
    "\n",
    "if pdf_files:\n",
    "    # Index the PDF documents\n",
    "    print(\"\\nğŸ“š Indexing PDF documents...\")\n",
    "    pdf_rag.index_documents(\"pdf_files/\", is_directory=True)\n",
    "    \n",
    "    # Get statistics\n",
    "    pdf_stats = pdf_rag.get_collection_stats()\n",
    "    print(\"\\nğŸ“Š PDF Collection Statistics:\")\n",
    "    for key, value in pdf_stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nâœ… PDF RAG ready! Indexed {pdf_stats.get('num_entities', 0)} chunks.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No PDF files found in pdf_files/ directory.\")\n",
    "    print(\"ğŸ“ Please add PDF files to test PDF RAG functionality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Test PDF RAG Queries\n",
    "if 'pdf_rag' in locals() and pdf_files:\n",
    "    pdf_queries = [\n",
    "        \"What is Milvus?\",\n",
    "        \"What are the key features of RAG systems?\", \n",
    "        \"How do vector databases work?\",\n",
    "        \"What are the benefits of using Milvus for AI applications?\"\n",
    "    ]\n",
    "\n",
    "    print(\"ğŸ§ª Testing PDF RAG with sample queries...\\n\")\n",
    "\n",
    "    for i, query in enumerate(pdf_queries, 1):\n",
    "        print(f\"ğŸ” PDF Query {i}: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = pdf_rag.query(query, top_k=3)\n",
    "        \n",
    "        # Display answer\n",
    "        answer = result['answer']\n",
    "        print(f\"ğŸ’¡ Answer: {answer}\")\n",
    "        print(f\"\\nğŸ“š Retrieved {result['num_retrieved']} documents:\")\n",
    "        \n",
    "        for j, doc in enumerate(result['retrieved_documents'], 1):\n",
    "            print(f\"  {j}. ğŸ“Š Score: {doc['score']:.4f}\")\n",
    "            print(f\"     ğŸ“„ Source: {doc['metadata'].get('source', 'Unknown')}\")\n",
    "            print(f\"     ğŸ“ Preview: {doc['text'][:100]}...\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "    print(\"ğŸ¯ PDF RAG testing completed!\")\n",
    "else:\n",
    "    print(\"âš ï¸ PDF RAG pipeline not available.\")\n",
    "    print(\"ğŸ“ Run the setup cell above to initialize PDF indexing first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Interactive PDF RAG Session\n",
    "def interactive_pdf_rag_session(pdf_rag_pipeline: RAGPipeline) -> None:\n",
    "    \"\"\"Run an interactive RAG session with PDF documents.\"\"\"\n",
    "    print(\"ğŸ” PDF RAG Interactive Session Started!\")\n",
    "    print(\"Commands: 'quit'/'exit'/'q' to exit, 'stats' for collection statistics\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nğŸ“„ Ask about your PDFs: \").strip()\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"ğŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.lower() == 'stats':\n",
    "            stats = pdf_rag_pipeline.get_collection_stats()\n",
    "            print(\"\\nğŸ“Š PDF Collection Statistics:\")\n",
    "            for key, value in stats.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            continue\n",
    "        \n",
    "        if not query:\n",
    "            print(\"âš ï¸ Please enter a valid question.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nğŸ”„ Processing...\")\n",
    "        result = pdf_rag_pipeline.query(query)\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ Answer: {result['answer']}\")\n",
    "        print(f\"ğŸ“š Based on {result['num_retrieved']} document chunks.\")\n",
    "        \n",
    "        # Show source information\n",
    "        sources = set([doc['metadata'].get('source', 'Unknown') for doc in result['retrieved_documents']])\n",
    "        print(f\"ğŸ—‚ï¸ Sources: {', '.join(sources)}\")\n",
    "\n",
    "# Ready to run interactive session\n",
    "print(\"ğŸ¯ Interactive PDF RAG function ready!\")\n",
    "print(\"ğŸ’¡ To start: interactive_pdf_rag_session(pdf_rag)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions & Features\n",
    "\n",
    "**Current Implementation:**\n",
    "- âœ… Modular RAG pipeline design\n",
    "- âœ… Support for both sample docs and PDF files  \n",
    "- âœ… Interactive query sessions\n",
    "- âœ… Comprehensive error handling & logging\n",
    "- âœ… Configurable via RAGConfig class\n",
    "\n",
    "**Possible Extensions:**\n",
    "- ğŸ”„ Response caching system\n",
    "- ğŸ“Š Advanced retrieval & generation metrics\n",
    "- ğŸ–¼ï¸ Multi-modal support (images, tables)\n",
    "- ğŸ” Hybrid dense/sparse search\n",
    "- ğŸŒ Web interface for RAG system\n",
    "- ğŸ“ˆ Performance monitoring & analytics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behrooz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
