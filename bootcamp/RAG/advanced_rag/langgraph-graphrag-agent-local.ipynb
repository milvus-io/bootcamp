{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/langgraph-rag-agent-local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain_community arxiv tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-ollama langchain-huggingface beautifulsoup4 langchain-experimental neo4j json-repair langchain-openai langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph GraphRAG agent with Llama 3.1 and GPT4o\n",
    "\n",
    "\n",
    "Let's build an Advanced RAG with a GraphRAG agent that will run a combination of Llama 3.1 and GPT4o, for Llama 3.1 we will use Ollama. The idea is that we use GPT4o for advanced tasks, like generating the Neo4j query and Llama3.1 for the rest. \n",
    "\n",
    "## Ideas\n",
    "\n",
    "We'll combine ideas from three RAG papers into a RAG agent:\n",
    "\n",
    "- **Routing:**  Adaptive RAG ([paper](https://arxiv.org/abs/2403.14403)). Route questions to different retrieval approaches\n",
    "- **Fallback:** Corrective RAG ([paper](https://arxiv.org/pdf/2401.15884.pdf)). Fallback to web search if docs are not relevant to query\n",
    "- **Self-correction:** Self-RAG ([paper](https://arxiv.org/abs/2310.11511)). Fix answers w/ hallucinations or donâ€™t address question\n",
    "\n",
    "![langgraph_adaptive_rag.png](imgs/RAG_Agent_langGraph.png)\n",
    "\n",
    "Note that this will incorperate [a few general ideas for agents](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/):\n",
    "\n",
    "- **Reflection**: The self-correction mechanism is a form of reflection, where the LangGraph agent reflects on its retrieval and generations\n",
    "- **Planning**: The control flow laid out in the graph is a form of planning \n",
    "- **Tool use**: Specific nodes in the control flow (e.g., web search) will use tools\n",
    "\n",
    "## Local models\n",
    "\n",
    "### LLM\n",
    "\n",
    "Use [Ollama](https://ollama.ai/) and [llama3](https://ollama.ai/library/llama3):\n",
    "\n",
    "```\n",
    "ollama pull llama3\n",
    "```\n",
    "\n",
    "### Env Variables\n",
    "Variables needed in an .env file or loaded as variables at start:\n",
    "\n",
    "Required:\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "TAVILY_API_KEY=tvly-...\n",
    "NEO4J_URI=...\n",
    "NEO4J_USERNAME=...\n",
    "NEO4J_PASSWORD=...\n",
    "```\n",
    "\n",
    "### Search\n",
    "\n",
    "Uses [Tavily](https://tavily.com/#api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "\n",
    "local_llm = 'llama3.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "search_query = \"agent OR 'large language model' OR 'prompt engineering'\"\n",
    "max_results = 20\n",
    "\n",
    "# Fetch papers from arXiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=search_query,\n",
    "    max_results=max_results,\n",
    "    sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 20\n",
      "Number of chunks: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kv/3dw9p_ts4b114chqt9m027pc0000gn/T/ipykernel_27609/932846617.py:26: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding=HuggingFaceEmbeddings(),\n",
      "/Users/stephen/Library/Caches/pypoetry/virtualenvs/milvus-bootcamp-rag-MiiP0ihC-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Milvus Lite Vectorstore\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append({\"title\": result.title, \"summary\": result.summary, \"url\": result.entry_id})\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.create_documents([doc['summary'] for doc in docs], metadatas=docs)    \n",
    "\n",
    "print(f'Number of papers: {len(docs)}')\n",
    "print(f'Number of chunks: {len(doc_splits)}')\n",
    "\n",
    "\n",
    "# Add to Milvus\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag_milvus\",\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    connection_args={\"uri\": \"./milvus_ingest.db\"},\n",
    "\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Prompt design and engineering has rapidly become essential for maximizing the\\npotential of large language models. In this paper, we introduce core concepts,\\nadvanced techniques like Chain-of-Thought and Reflection, and the principles\\nbehind building LLM-based agents. Finally, we provide a survey of tools for\\nprompt engineers.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Prompt design and engineering has rapidly become essential for maximizing the\\npotential of large language models. In this paper, we introduce core concepts,\\nadvanced techniques like Chain-of-Thought and Reflection, and the principles\\nbehind building LLM-based agents. Finally, we provide a survey of tools for\\nprompt engineers.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Prompt design and engineering has rapidly become essential for maximizing the\\npotential of large language models. In this paper, we introduce core concepts,\\nadvanced techniques like Chain-of-Thought and Reflection, and the principles\\nbehind building LLM-based agents. Finally, we provide a survey of tools for\\nprompt engineers.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [5.85s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_J6mfFuGvbWNugFoPMl2dcxrx\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\\\"}]},{\\\"id\\\":\\\"core concepts\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"advanced techniques like Chain-of-Thought and Reflection\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"principles behind building LLM-based agents\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"survey of tools for prompt engineers\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"core concepts\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"advanced techniques like Chain-of-Thought and Reflection\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"principles behind building LLM-based agents\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"survey of tools for prompt engineers\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 314,\n",
      "                \"prompt_tokens\": 806,\n",
      "                \"total_tokens\": 1120,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-72f6875e-d805-4d77-a8be-b9a60c3293a1-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"core concepts\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"advanced techniques like Chain-of-Thought and Reflection\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"principles behind building LLM-based agents\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"survey of tools for prompt engineers\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"core concepts\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"advanced techniques like Chain-of-Thought and Reflection\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"principles behind building LLM-based agents\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt design and engineering has rapidly become essential for maximizing the potential of large language models\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"survey of tools for prompt engineers\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_J6mfFuGvbWNugFoPMl2dcxrx\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 806,\n",
      "              \"output_tokens\": 314,\n",
      "              \"total_tokens\": 1120\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 314,\n",
      "      \"prompt_tokens\": 806,\n",
      "      \"total_tokens\": 1120,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [5.85s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [5.86s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Two ways has been discussed to unlock the reasoning capability of a large\\nlanguage model. The first one is prompt engineering and the second one is to\\ncombine the multiple inferences of large language models, or the multi-agent\\ndiscussion. Theoretically, this paper justifies the multi-agent discussion\\nmechanisms from the symmetry of agents. Empirically, this paper reports the\\nempirical results of the interplay of prompts and discussion mechanisms,\\nrevealing the empirical state-of-the-art performance of complex multi-agent\\nmechanisms can be approached by carefully developed prompt engineering. This\\npaper also proposes a scalable discussion mechanism based on conquer and merge,\\nproviding a simple multi-agent discussion solution with simple prompts but\\nstate-of-the-art performance.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Two ways has been discussed to unlock the reasoning capability of a large\\nlanguage model. The first one is prompt engineering and the second one is to\\ncombine the multiple inferences of large language models, or the multi-agent\\ndiscussion. Theoretically, this paper justifies the multi-agent discussion\\nmechanisms from the symmetry of agents. Empirically, this paper reports the\\nempirical results of the interplay of prompts and discussion mechanisms,\\nrevealing the empirical state-of-the-art performance of complex multi-agent\\nmechanisms can be approached by carefully developed prompt engineering. This\\npaper also proposes a scalable discussion mechanism based on conquer and merge,\\nproviding a simple multi-agent discussion solution with simple prompts but\\nstate-of-the-art performance.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Two ways has been discussed to unlock the reasoning capability of a large\\nlanguage model. The first one is prompt engineering and the second one is to\\ncombine the multiple inferences of large language models, or the multi-agent\\ndiscussion. Theoretically, this paper justifies the multi-agent discussion\\nmechanisms from the symmetry of agents. Empirically, this paper reports the\\nempirical results of the interplay of prompts and discussion mechanisms,\\nrevealing the empirical state-of-the-art performance of complex multi-agent\\nmechanisms can be approached by carefully developed prompt engineering. This\\npaper also proposes a scalable discussion mechanism based on conquer and merge,\\nproviding a simple multi-agent discussion solution with simple prompts but\\nstate-of-the-art performance.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [3.31s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_uCP3OKLuMpqKK6dHHogn3TZN\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Two ways to unlock the reasoning capability of a large language model\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"Two ways to unlock the reasoning capability of a large language model\\\"}]},{\\\"id\\\":\\\"prompt engineering\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"multi-agent discussion\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"scalable discussion mechanism based on conquer and merge\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Two ways to unlock the reasoning capability of a large language model\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompt engineering\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Two ways to unlock the reasoning capability of a large language model\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"multi-agent discussion\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Two ways to unlock the reasoning capability of a large language model\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"scalable discussion mechanism based on conquer and merge\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 221,\n",
      "                \"prompt_tokens\": 893,\n",
      "                \"total_tokens\": 1114,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6bb7b99b-d0ac-4469-9e02-9ea2c3db1090-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Two ways to unlock the reasoning capability of a large language model\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"Two ways to unlock the reasoning capability of a large language model\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"prompt engineering\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"multi-agent discussion\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"scalable discussion mechanism based on conquer and merge\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Two ways to unlock the reasoning capability of a large language model\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompt engineering\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Two ways to unlock the reasoning capability of a large language model\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"multi-agent discussion\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Two ways to unlock the reasoning capability of a large language model\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"scalable discussion mechanism based on conquer and merge\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_uCP3OKLuMpqKK6dHHogn3TZN\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 893,\n",
      "              \"output_tokens\": 221,\n",
      "              \"total_tokens\": 1114\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 221,\n",
      "      \"prompt_tokens\": 893,\n",
      "      \"total_tokens\": 1114,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [3.32s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [3.32s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"The final frontier for simulation is the accurate representation of complex,\\nreal-world social systems. While agent-based modeling (ABM) seeks to study the\\nbehavior and interactions of agents within a larger system, it is unable to\\nfaithfully capture the full complexity of human-driven behavior. Large language\\nmodels (LLMs), like ChatGPT, have emerged as a potential solution to this\\nbottleneck by enabling researchers to explore human-driven interactions in\\npreviously unimaginable ways. Our research investigates simulations of human\\ninteractions using LLMs. Through prompt engineering, inspired by Park et al.\\n(2023), we present two simulations of believable proxies of human behavior: a\\ntwo-agent negotiation and a six-agent murder mystery game.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"The final frontier for simulation is the accurate representation of complex,\\nreal-world social systems. While agent-based modeling (ABM) seeks to study the\\nbehavior and interactions of agents within a larger system, it is unable to\\nfaithfully capture the full complexity of human-driven behavior. Large language\\nmodels (LLMs), like ChatGPT, have emerged as a potential solution to this\\nbottleneck by enabling researchers to explore human-driven interactions in\\npreviously unimaginable ways. Our research investigates simulations of human\\ninteractions using LLMs. Through prompt engineering, inspired by Park et al.\\n(2023), we present two simulations of believable proxies of human behavior: a\\ntwo-agent negotiation and a six-agent murder mystery game.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: The final frontier for simulation is the accurate representation of complex,\\nreal-world social systems. While agent-based modeling (ABM) seeks to study the\\nbehavior and interactions of agents within a larger system, it is unable to\\nfaithfully capture the full complexity of human-driven behavior. Large language\\nmodels (LLMs), like ChatGPT, have emerged as a potential solution to this\\nbottleneck by enabling researchers to explore human-driven interactions in\\npreviously unimaginable ways. Our research investigates simulations of human\\ninteractions using LLMs. Through prompt engineering, inspired by Park et al.\\n(2023), we present two simulations of believable proxies of human behavior: a\\ntwo-agent negotiation and a six-agent murder mystery game.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [4.55s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_3SeFcG330mwV1XCfSUjU16lj\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"The final frontier for simulation\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"The final frontier for simulation\\\"}]},{\\\"id\\\":\\\"agent-based modeling\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Large language models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"ChatGPT\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Our research\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"Our research\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), presents two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.\\\"}]},{\\\"id\\\":\\\"Park et al. (2023)\\\",\\\"type\\\":\\\"Paper\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"The final frontier for simulation\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"agent-based modeling\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"The final frontier for simulation\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Large language models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Large language models\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"ChatGPT\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Our research\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Large language models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Our research\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Park et al. (2023)\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"RELATED_TO\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 332,\n",
      "                \"prompt_tokens\": 892,\n",
      "                \"total_tokens\": 1224,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6924ee37-21d2-4892-962d-5845b368063f-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"The final frontier for simulation\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"The final frontier for simulation\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"agent-based modeling\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Large language models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"ChatGPT\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Our research\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"Our research\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), presents two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Park et al. (2023)\",\n",
      "                      \"type\": \"Paper\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"The final frontier for simulation\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"agent-based modeling\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"The final frontier for simulation\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Large language models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Large language models\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"ChatGPT\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Our research\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Large language models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Our research\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Park et al. (2023)\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_3SeFcG330mwV1XCfSUjU16lj\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 892,\n",
      "              \"output_tokens\": 332,\n",
      "              \"total_tokens\": 1224\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 332,\n",
      "      \"prompt_tokens\": 892,\n",
      "      \"total_tokens\": 1224,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [4.55s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [4.56s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"The AI community has been exploring a pathway to artificial general\\nintelligence (AGI) by developing \\\"language agents\\\", which are complex large\\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\\nusage methods. While language agents have demonstrated impressive capabilities\\nfor many real-world tasks, a fundamental limitation of current language agents\\nresearch is that they are model-centric, or engineering-centric. That's to say,\\nthe progress on prompts, tools, and pipelines of language agents requires\\nsubstantial manual engineering efforts from human experts rather than\\nautomatically learning from data. We believe the transition from model-centric,\\nor engineering-centric, to data-centric, i.e., the ability of language agents\\nto autonomously learn and evolve in environments, is the key for them to\\npossibly achieve AGI.\\n  In this work, we introduce agent symbolic learning, a systematic framework\\nthat enables language agents to optimize themselves on their own in a\\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\\nsymbolic networks where learnable weights are defined by prompts, tools, and\\nthe way they are stacked together. Agent symbolic learning is designed to\\noptimize the symbolic network within language agents by mimicking two\\nfundamental algorithms in connectionist learning: back-propagation and gradient\\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\\nwith natural language simulacrums of weights, loss, and gradients. We conduct\\nproof-of-concept experiments on both standard benchmarks and complex real-world\\ntasks and show that agent symbolic learning enables language agents to update\\nthemselves after being created and deployed in the wild, resulting in\\n\\\"self-evolving agents\\\".\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"The AI community has been exploring a pathway to artificial general\\nintelligence (AGI) by developing \\\"language agents\\\", which are complex large\\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\\nusage methods. While language agents have demonstrated impressive capabilities\\nfor many real-world tasks, a fundamental limitation of current language agents\\nresearch is that they are model-centric, or engineering-centric. That's to say,\\nthe progress on prompts, tools, and pipelines of language agents requires\\nsubstantial manual engineering efforts from human experts rather than\\nautomatically learning from data. We believe the transition from model-centric,\\nor engineering-centric, to data-centric, i.e., the ability of language agents\\nto autonomously learn and evolve in environments, is the key for them to\\npossibly achieve AGI.\\n  In this work, we introduce agent symbolic learning, a systematic framework\\nthat enables language agents to optimize themselves on their own in a\\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\\nsymbolic networks where learnable weights are defined by prompts, tools, and\\nthe way they are stacked together. Agent symbolic learning is designed to\\noptimize the symbolic network within language agents by mimicking two\\nfundamental algorithms in connectionist learning: back-propagation and gradient\\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\\nwith natural language simulacrums of weights, loss, and gradients. We conduct\\nproof-of-concept experiments on both standard benchmarks and complex real-world\\ntasks and show that agent symbolic learning enables language agents to update\\nthemselves after being created and deployed in the wild, resulting in\\n\\\"self-evolving agents\\\".\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: The AI community has been exploring a pathway to artificial general\\nintelligence (AGI) by developing \\\"language agents\\\", which are complex large\\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\\nusage methods. While language agents have demonstrated impressive capabilities\\nfor many real-world tasks, a fundamental limitation of current language agents\\nresearch is that they are model-centric, or engineering-centric. That's to say,\\nthe progress on prompts, tools, and pipelines of language agents requires\\nsubstantial manual engineering efforts from human experts rather than\\nautomatically learning from data. We believe the transition from model-centric,\\nor engineering-centric, to data-centric, i.e., the ability of language agents\\nto autonomously learn and evolve in environments, is the key for them to\\npossibly achieve AGI.\\n  In this work, we introduce agent symbolic learning, a systematic framework\\nthat enables language agents to optimize themselves on their own in a\\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\\nsymbolic networks where learnable weights are defined by prompts, tools, and\\nthe way they are stacked together. Agent symbolic learning is designed to\\noptimize the symbolic network within language agents by mimicking two\\nfundamental algorithms in connectionist learning: back-propagation and gradient\\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\\nwith natural language simulacrums of weights, loss, and gradients. We conduct\\nproof-of-concept experiments on both standard benchmarks and complex real-world\\ntasks and show that agent symbolic learning enables language agents to update\\nthemselves after being created and deployed in the wild, resulting in\\n\\\"self-evolving agents\\\".\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [4.72s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_Fuxbom4ofXCub4pR6rkHp6O3\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"AI community\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"artificial general intelligence\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"language agents\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"large language models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"agent symbolic learning\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Paper on agent symbolic learning\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"Agent Symbolic Learning for Self-Evolving Language Agents\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"A systematic framework that enables language agents to optimize themselves in a data-centric way using symbolic optimizers, mimicking back-propagation and gradient descent with natural language simulacrums of weights, loss, and gradients.\\\"}]},{\\\"id\\\":\\\"self-evolving agents\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"AI community\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"artificial general intelligence\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"language agents\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"large language models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"language agents\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"artificial general intelligence\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"agent symbolic learning\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"language agents\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Paper on agent symbolic learning\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"agent symbolic learning\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"self-evolving agents\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"agent symbolic learning\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 352,\n",
      "                \"prompt_tokens\": 1087,\n",
      "                \"total_tokens\": 1439,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-362cc433-89b8-4719-ad7e-86422c5d155c-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"AI community\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"artificial general intelligence\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"language agents\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"large language models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"agent symbolic learning\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Paper on agent symbolic learning\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"Agent Symbolic Learning for Self-Evolving Language Agents\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"A systematic framework that enables language agents to optimize themselves in a data-centric way using symbolic optimizers, mimicking back-propagation and gradient descent with natural language simulacrums of weights, loss, and gradients.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"self-evolving agents\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"AI community\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"artificial general intelligence\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"language agents\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"large language models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"language agents\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"artificial general intelligence\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"agent symbolic learning\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"language agents\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Paper on agent symbolic learning\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"agent symbolic learning\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"self-evolving agents\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"agent symbolic learning\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_Fuxbom4ofXCub4pR6rkHp6O3\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1087,\n",
      "              \"output_tokens\": 352,\n",
      "              \"total_tokens\": 1439\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 352,\n",
      "      \"prompt_tokens\": 1087,\n",
      "      \"total_tokens\": 1439,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [4.72s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [4.73s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"In this past year, large language models (LLMs) have had remarkable success\\nin domains outside the traditional natural language processing, and people are\\nstarting to explore the usage of LLMs in more general and close to application\\ndomains like code generation, travel planning, and robot controls. Connecting\\nthese LLMs with great capacity and external tools, people are building the\\nso-called LLM agents, which are supposed to help people do all kinds of work in\\neveryday life. In all these domains, the prompt to the LLMs has been shown to\\nmake a big difference in what the LLM would generate and thus affect the\\nperformance of the LLM agents. Therefore, automatic prompt engineering has\\nbecome an important question for many researchers and users of LLMs. In this\\npaper, we propose a novel method, \\\\textsc{RePrompt}, which does \\\"gradient\\ndescent\\\" to optimize the step-by-step instructions in the prompt of the LLM\\nagents based on the chat history obtained from interactions with LLM agents. By\\noptimizing the prompt, the LLM will learn how to plan in specific domains. We\\nhave used experiments in PDDL generation and travel planning to show that our\\nmethod could generally improve the performance for different reasoning tasks\\nwhen using the updated prompt as the initial prompt.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"In this past year, large language models (LLMs) have had remarkable success\\nin domains outside the traditional natural language processing, and people are\\nstarting to explore the usage of LLMs in more general and close to application\\ndomains like code generation, travel planning, and robot controls. Connecting\\nthese LLMs with great capacity and external tools, people are building the\\nso-called LLM agents, which are supposed to help people do all kinds of work in\\neveryday life. In all these domains, the prompt to the LLMs has been shown to\\nmake a big difference in what the LLM would generate and thus affect the\\nperformance of the LLM agents. Therefore, automatic prompt engineering has\\nbecome an important question for many researchers and users of LLMs. In this\\npaper, we propose a novel method, \\\\textsc{RePrompt}, which does \\\"gradient\\ndescent\\\" to optimize the step-by-step instructions in the prompt of the LLM\\nagents based on the chat history obtained from interactions with LLM agents. By\\noptimizing the prompt, the LLM will learn how to plan in specific domains. We\\nhave used experiments in PDDL generation and travel planning to show that our\\nmethod could generally improve the performance for different reasoning tasks\\nwhen using the updated prompt as the initial prompt.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: In this past year, large language models (LLMs) have had remarkable success\\nin domains outside the traditional natural language processing, and people are\\nstarting to explore the usage of LLMs in more general and close to application\\ndomains like code generation, travel planning, and robot controls. Connecting\\nthese LLMs with great capacity and external tools, people are building the\\nso-called LLM agents, which are supposed to help people do all kinds of work in\\neveryday life. In all these domains, the prompt to the LLMs has been shown to\\nmake a big difference in what the LLM would generate and thus affect the\\nperformance of the LLM agents. Therefore, automatic prompt engineering has\\nbecome an important question for many researchers and users of LLMs. In this\\npaper, we propose a novel method, \\\\textsc{RePrompt}, which does \\\"gradient\\ndescent\\\" to optimize the step-by-step instructions in the prompt of the LLM\\nagents based on the chat history obtained from interactions with LLM agents. By\\noptimizing the prompt, the LLM will learn how to plan in specific domains. We\\nhave used experiments in PDDL generation and travel planning to show that our\\nmethod could generally improve the performance for different reasoning tasks\\nwhen using the updated prompt as the initial prompt.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [6.18s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_96uSluXsQ0HgA4sxS4Mvh3wY\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"RePrompt\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"RePrompt: Gradient Descent for Optimizing Prompts in LLM Agents\\\"}]},{\\\"id\\\":\\\"large language models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"code generation\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"travel planning\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"robot controls\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"LLM agents\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"automatic prompt engineering\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"PDDL generation\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"reasoning tasks\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"RePrompt\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"large language models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"RePrompt\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"code generation\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"RePrompt\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"travel planning\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"RePrompt\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"robot controls\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"RePrompt\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"LLM agents\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"RePrompt\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"automatic prompt engineering\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"RePrompt\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"PDDL generation\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"RePrompt\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"reasoning tasks\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 383,\n",
      "                \"prompt_tokens\": 1019,\n",
      "                \"total_tokens\": 1402,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a116526b-3a3f-4d4f-9c63-f4674e5a3414-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"RePrompt\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"RePrompt: Gradient Descent for Optimizing Prompts in LLM Agents\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"large language models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"code generation\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"travel planning\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"robot controls\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"LLM agents\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"automatic prompt engineering\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"PDDL generation\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"reasoning tasks\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"RePrompt\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"large language models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"RePrompt\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"code generation\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"RePrompt\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"travel planning\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"RePrompt\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"robot controls\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"RePrompt\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"LLM agents\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"RePrompt\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"automatic prompt engineering\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"RePrompt\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"PDDL generation\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"RePrompt\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"reasoning tasks\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_96uSluXsQ0HgA4sxS4Mvh3wY\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1019,\n",
      "              \"output_tokens\": 383,\n",
      "              \"total_tokens\": 1402\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 383,\n",
      "      \"prompt_tokens\": 1019,\n",
      "      \"total_tokens\": 1402,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [6.18s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [6.18s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Traditional base station siting (BSS) methods rely heavily on drive testing\\nand user feedback, which are laborious and require extensive expertise in\\ncommunication, networking, and optimization. As large language models (LLMs)\\nand their associated technologies advance, particularly in the realms of prompt\\nengineering and agent engineering, network optimization will witness a\\nrevolutionary approach. This approach entails the strategic use of well-crafted\\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\\nand the deployment of autonomous agents as a communication bridge to seamlessly\\nconnect the machine language based LLMs with human users using natural\\nlanguage. This integration represents the future paradigm of artificial\\nintelligence (AI) as a service and AI for more ease. As a preliminary\\nexploration, this research first develops a novel LLM-empowered BSS\\noptimization framework, and heuristically proposes four different potential\\nimplementations: the strategies based on Prompt-optimized LLM (PoL),\\nhuman-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and\\nCooperative multiple LLM-based autonomous BSS agents (CLaBa). Through\\nevaluation on real-world data, the experiments demonstrate that prompt-assisted\\nLLMs and LLM-based agents can generate more efficient, cost-effective, and\\nreliable network deployments, noticeably enhancing the efficiency of BSS\\noptimization and reducing trivial manual participation.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Traditional base station siting (BSS) methods rely heavily on drive testing\\nand user feedback, which are laborious and require extensive expertise in\\ncommunication, networking, and optimization. As large language models (LLMs)\\nand their associated technologies advance, particularly in the realms of prompt\\nengineering and agent engineering, network optimization will witness a\\nrevolutionary approach. This approach entails the strategic use of well-crafted\\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\\nand the deployment of autonomous agents as a communication bridge to seamlessly\\nconnect the machine language based LLMs with human users using natural\\nlanguage. This integration represents the future paradigm of artificial\\nintelligence (AI) as a service and AI for more ease. As a preliminary\\nexploration, this research first develops a novel LLM-empowered BSS\\noptimization framework, and heuristically proposes four different potential\\nimplementations: the strategies based on Prompt-optimized LLM (PoL),\\nhuman-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and\\nCooperative multiple LLM-based autonomous BSS agents (CLaBa). Through\\nevaluation on real-world data, the experiments demonstrate that prompt-assisted\\nLLMs and LLM-based agents can generate more efficient, cost-effective, and\\nreliable network deployments, noticeably enhancing the efficiency of BSS\\noptimization and reducing trivial manual participation.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Traditional base station siting (BSS) methods rely heavily on drive testing\\nand user feedback, which are laborious and require extensive expertise in\\ncommunication, networking, and optimization. As large language models (LLMs)\\nand their associated technologies advance, particularly in the realms of prompt\\nengineering and agent engineering, network optimization will witness a\\nrevolutionary approach. This approach entails the strategic use of well-crafted\\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\\nand the deployment of autonomous agents as a communication bridge to seamlessly\\nconnect the machine language based LLMs with human users using natural\\nlanguage. This integration represents the future paradigm of artificial\\nintelligence (AI) as a service and AI for more ease. As a preliminary\\nexploration, this research first develops a novel LLM-empowered BSS\\noptimization framework, and heuristically proposes four different potential\\nimplementations: the strategies based on Prompt-optimized LLM (PoL),\\nhuman-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and\\nCooperative multiple LLM-based autonomous BSS agents (CLaBa). Through\\nevaluation on real-world data, the experiments demonstrate that prompt-assisted\\nLLMs and LLM-based agents can generate more efficient, cost-effective, and\\nreliable network deployments, noticeably enhancing the efficiency of BSS\\noptimization and reducing trivial manual participation.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [13.35s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_tBwGaXX3XWpNWJy9UXwRDEY7\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Traditional base station siting methods\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"drive testing\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"user feedback\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"large language models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"prompt engineering\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"agent engineering\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"network optimization\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"well-crafted prompts\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"autonomous agents\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"machine language based LLMs\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"human users\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"natural language\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"artificial intelligence as a service\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"AI for more ease\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"LLM-empowered BSS optimization framework\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Prompt-optimized LLM\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"human-in-the-Loop LLM\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"LLM-empowered autonomous BSS agent\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Cooperative multiple LLM-based autonomous BSS agents\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"prompt-assisted LLMs\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"LLM-based agents\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"more efficient network deployments\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"cost-effective network deployments\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"reliable network deployments\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"BSS optimization\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"trivial manual participation\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Traditional base station siting methods\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"drive testing\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Traditional base station siting methods\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"user feedback\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"large language models\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"prompt engineering\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"large language models\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"agent engineering\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"large language models\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"network optimization\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"well-crafted prompts\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"large language models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"autonomous agents\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"large language models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"machine language based LLMs\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"human users\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"machine language based LLMs\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"natural language\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"artificial intelligence as a service\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"AI for more ease\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"LLM-empowered BSS optimization framework\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"Prompt-optimized LLM\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"LLM-empowered BSS optimization framework\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"human-in-the-Loop LLM\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"LLM-empowered BSS optimization framework\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-empowered autonomous BSS agent\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"LLM-empowered BSS optimization framework\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"Cooperative multiple LLM-based autonomous BSS agents\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"prompt-assisted LLMs\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"more efficient network deployments\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"prompt-assisted LLMs\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"cost-effective network deployments\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"prompt-assisted LLMs\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"reliable network deployments\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"LLM-based agents\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"more efficient network deployments\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"LLM-based agents\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"cost-effective network deployments\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"LLM-based agents\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"reliable network deployments\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"more efficient network deployments\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"BSS optimization\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"cost-effective network deployments\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"BSS optimization\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"reliable network deployments\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"BSS optimization\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"BSS optimization\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"trivial manual participation\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 1167,\n",
      "                \"prompt_tokens\": 1038,\n",
      "                \"total_tokens\": 2205,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f36bbc09-79a9-4d7b-81c8-d107bd1d28ac-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Traditional base station siting methods\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"drive testing\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"user feedback\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"large language models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"prompt engineering\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"agent engineering\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"network optimization\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"well-crafted prompts\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"autonomous agents\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"machine language based LLMs\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"human users\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"natural language\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"artificial intelligence as a service\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"AI for more ease\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"LLM-empowered BSS optimization framework\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Prompt-optimized LLM\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"human-in-the-Loop LLM\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"LLM-empowered autonomous BSS agent\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Cooperative multiple LLM-based autonomous BSS agents\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"prompt-assisted LLMs\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"LLM-based agents\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"more efficient network deployments\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"cost-effective network deployments\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"reliable network deployments\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"BSS optimization\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"trivial manual participation\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Traditional base station siting methods\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"drive testing\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Traditional base station siting methods\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"user feedback\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"large language models\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"prompt engineering\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"large language models\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"agent engineering\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"large language models\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"network optimization\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"well-crafted prompts\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"large language models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"autonomous agents\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"large language models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"machine language based LLMs\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"human users\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"machine language based LLMs\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"natural language\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"artificial intelligence as a service\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"AI for more ease\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"LLM-empowered BSS optimization framework\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"Prompt-optimized LLM\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"LLM-empowered BSS optimization framework\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"human-in-the-Loop LLM\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"LLM-empowered BSS optimization framework\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-empowered autonomous BSS agent\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"LLM-empowered BSS optimization framework\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"Cooperative multiple LLM-based autonomous BSS agents\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"prompt-assisted LLMs\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"more efficient network deployments\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"prompt-assisted LLMs\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"cost-effective network deployments\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"prompt-assisted LLMs\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"reliable network deployments\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"LLM-based agents\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"more efficient network deployments\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"LLM-based agents\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"cost-effective network deployments\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"LLM-based agents\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"reliable network deployments\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"more efficient network deployments\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"BSS optimization\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"cost-effective network deployments\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"BSS optimization\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"reliable network deployments\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"BSS optimization\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"BSS optimization\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"trivial manual participation\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_tBwGaXX3XWpNWJy9UXwRDEY7\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1038,\n",
      "              \"output_tokens\": 1167,\n",
      "              \"total_tokens\": 2205\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 1167,\n",
      "      \"prompt_tokens\": 1038,\n",
      "      \"total_tokens\": 2205,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [13.35s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [13.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Instruction-following agents must ground language into their observation and\\naction spaces. Learning to ground language is challenging, typically requiring\\ndomain-specific engineering or large quantities of human interaction data. To\\naddress this challenge, we propose using pretrained vision-language models\\n(VLMs) to supervise embodied agents. We combine ideas from model distillation\\nand hindsight experience replay (HER), using a VLM to retroactively generate\\nlanguage describing the agent's behavior. Simple prompting allows us to control\\nthe supervision signal, teaching an agent to interact with novel objects based\\non their names (e.g., planes) or their features (e.g., colors) in a 3D rendered\\nenvironment. Fewshot prompting lets us teach abstract category membership,\\nincluding pre-existing categories (food vs toys) and ad-hoc ones (arbitrary\\npreferences over objects). Our work outlines a new and effective way to use\\ninternet-scale VLMs, repurposing the generic language grounding acquired by\\nsuch models to teach task-relevant groundings to embodied agents.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Instruction-following agents must ground language into their observation and\\naction spaces. Learning to ground language is challenging, typically requiring\\ndomain-specific engineering or large quantities of human interaction data. To\\naddress this challenge, we propose using pretrained vision-language models\\n(VLMs) to supervise embodied agents. We combine ideas from model distillation\\nand hindsight experience replay (HER), using a VLM to retroactively generate\\nlanguage describing the agent's behavior. Simple prompting allows us to control\\nthe supervision signal, teaching an agent to interact with novel objects based\\non their names (e.g., planes) or their features (e.g., colors) in a 3D rendered\\nenvironment. Fewshot prompting lets us teach abstract category membership,\\nincluding pre-existing categories (food vs toys) and ad-hoc ones (arbitrary\\npreferences over objects). Our work outlines a new and effective way to use\\ninternet-scale VLMs, repurposing the generic language grounding acquired by\\nsuch models to teach task-relevant groundings to embodied agents.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Instruction-following agents must ground language into their observation and\\naction spaces. Learning to ground language is challenging, typically requiring\\ndomain-specific engineering or large quantities of human interaction data. To\\naddress this challenge, we propose using pretrained vision-language models\\n(VLMs) to supervise embodied agents. We combine ideas from model distillation\\nand hindsight experience replay (HER), using a VLM to retroactively generate\\nlanguage describing the agent's behavior. Simple prompting allows us to control\\nthe supervision signal, teaching an agent to interact with novel objects based\\non their names (e.g., planes) or their features (e.g., colors) in a 3D rendered\\nenvironment. Fewshot prompting lets us teach abstract category membership,\\nincluding pre-existing categories (food vs toys) and ad-hoc ones (arbitrary\\npreferences over objects). Our work outlines a new and effective way to use\\ninternet-scale VLMs, repurposing the generic language grounding acquired by\\nsuch models to teach task-relevant groundings to embodied agents.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [11.25s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_Uc8RYu41ABUHJNPFkYZg1XHu\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Instruction-following agents must ground language into their observation and action spaces\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"Instruction-following agents must ground language into their observation and action spaces\\\"}]},{\\\"id\\\":\\\"Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data\\\"}]},{\\\"id\\\":\\\"We propose using pretrained vision-language models (VLMs) to supervise embodied agents\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"We propose using pretrained vision-language models (VLMs) to supervise embodied agents\\\"}]},{\\\"id\\\":\\\"We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior\\\"}]},{\\\"id\\\":\\\"Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment\\\"}]},{\\\"id\\\":\\\"Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects)\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects)\\\"}]},{\\\"id\\\":\\\"Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents\\\"}]}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Instruction-following agents must ground language into their observation and action spaces\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Instruction-following agents must ground language into their observation and action spaces\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"We propose using pretrained vision-language models (VLMs) to supervise embodied agents\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Instruction-following agents must ground language into their observation and action spaces\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Instruction-following agents must ground language into their observation and action spaces\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Instruction-following agents must ground language into their observation and action spaces\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects)\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Instruction-following agents must ground language into their observation and action spaces\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"RELATED_TO\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 932,\n",
      "                \"prompt_tokens\": 955,\n",
      "                \"total_tokens\": 1887,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-05f95a28-834c-424a-a160-62e437f55391-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Instruction-following agents must ground language into their observation and action spaces\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"Instruction-following agents must ground language into their observation and action spaces\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"We propose using pretrained vision-language models (VLMs) to supervise embodied agents\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"We propose using pretrained vision-language models (VLMs) to supervise embodied agents\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects)\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects)\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Instruction-following agents must ground language into their observation and action spaces\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Instruction-following agents must ground language into their observation and action spaces\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"We propose using pretrained vision-language models (VLMs) to supervise embodied agents\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Instruction-following agents must ground language into their observation and action spaces\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Instruction-following agents must ground language into their observation and action spaces\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Instruction-following agents must ground language into their observation and action spaces\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects)\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Instruction-following agents must ground language into their observation and action spaces\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_Uc8RYu41ABUHJNPFkYZg1XHu\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 955,\n",
      "              \"output_tokens\": 932,\n",
      "              \"total_tokens\": 1887\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 932,\n",
      "      \"prompt_tokens\": 955,\n",
      "      \"total_tokens\": 1887,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [11.25s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [11.26s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Pre-trained and frozen large language models (LLMs) can effectively map\\nsimple scene rearrangement instructions to programs over a robot's visuomotor\\nfunctions through appropriate few-shot example prompting. To parse open-domain\\nnatural language and adapt to a user's idiosyncratic procedures, not known\\nduring prompt engineering time, fixed prompts fall short. In this paper, we\\nintroduce HELPER, an embodied agent equipped with an external memory of\\nlanguage-program pairs that parses free-form human-robot dialogue into action\\nprograms through retrieval-augmented LLM prompting: relevant memories are\\nretrieved based on the current dialogue, instruction, correction, or VLM\\ndescription, and used as in-context prompt examples for LLM querying. The\\nmemory is expanded during deployment to include pairs of user's language and\\naction plans, to assist future inferences and personalize them to the user's\\nlanguage and routines. HELPER sets a new state-of-the-art in the TEACh\\nbenchmark in both Execution from Dialog History (EDH) and Trajectory from\\nDialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for\\nTfD. Our models, code, and video results can be found in our project's website:\\nhttps://helper-agent-llm.github.io.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Pre-trained and frozen large language models (LLMs) can effectively map\\nsimple scene rearrangement instructions to programs over a robot's visuomotor\\nfunctions through appropriate few-shot example prompting. To parse open-domain\\nnatural language and adapt to a user's idiosyncratic procedures, not known\\nduring prompt engineering time, fixed prompts fall short. In this paper, we\\nintroduce HELPER, an embodied agent equipped with an external memory of\\nlanguage-program pairs that parses free-form human-robot dialogue into action\\nprograms through retrieval-augmented LLM prompting: relevant memories are\\nretrieved based on the current dialogue, instruction, correction, or VLM\\ndescription, and used as in-context prompt examples for LLM querying. The\\nmemory is expanded during deployment to include pairs of user's language and\\naction plans, to assist future inferences and personalize them to the user's\\nlanguage and routines. HELPER sets a new state-of-the-art in the TEACh\\nbenchmark in both Execution from Dialog History (EDH) and Trajectory from\\nDialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for\\nTfD. Our models, code, and video results can be found in our project's website:\\nhttps://helper-agent-llm.github.io.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Pre-trained and frozen large language models (LLMs) can effectively map\\nsimple scene rearrangement instructions to programs over a robot's visuomotor\\nfunctions through appropriate few-shot example prompting. To parse open-domain\\nnatural language and adapt to a user's idiosyncratic procedures, not known\\nduring prompt engineering time, fixed prompts fall short. In this paper, we\\nintroduce HELPER, an embodied agent equipped with an external memory of\\nlanguage-program pairs that parses free-form human-robot dialogue into action\\nprograms through retrieval-augmented LLM prompting: relevant memories are\\nretrieved based on the current dialogue, instruction, correction, or VLM\\ndescription, and used as in-context prompt examples for LLM querying. The\\nmemory is expanded during deployment to include pairs of user's language and\\naction plans, to assist future inferences and personalize them to the user's\\nlanguage and routines. HELPER sets a new state-of-the-art in the TEACh\\nbenchmark in both Execution from Dialog History (EDH) and Trajectory from\\nDialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for\\nTfD. Our models, code, and video results can be found in our project's website:\\nhttps://helper-agent-llm.github.io.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [5.98s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_k3QqbyDh6JvckPrpHeaXpHH0\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"HELPER\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"HELPER: An Embodied Agent with External Memory for Parsing Human-Robot Dialogue into Action Programs\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"HELPER is an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting. It sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD).\\\"},{\\\"key\\\":\\\"url\\\",\\\"value\\\":\\\"https://helper-agent-llm.github.io\\\"}]},{\\\"id\\\":\\\"TEACh\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Execution from Dialog History (EDH)\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Trajectory from Dialogue (TfD)\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Pre-trained and frozen large language models (LLMs)\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"robot's visuomotor functions\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"retrieval-augmented LLM prompting\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"HELPER\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"TEACh\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"HELPER\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Execution from Dialog History (EDH)\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"HELPER\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Trajectory from Dialogue (TfD)\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"HELPER\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Pre-trained and frozen large language models (LLMs)\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"HELPER\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"robot's visuomotor functions\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"HELPER\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"retrieval-augmented LLM prompting\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 448,\n",
      "                \"prompt_tokens\": 1011,\n",
      "                \"total_tokens\": 1459,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-79d0ee30-04e5-45d0-91b2-aed645d872c3-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"HELPER\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"HELPER: An Embodied Agent with External Memory for Parsing Human-Robot Dialogue into Action Programs\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"HELPER is an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting. It sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD).\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"url\",\n",
      "                          \"value\": \"https://helper-agent-llm.github.io\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"TEACh\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Execution from Dialog History (EDH)\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Trajectory from Dialogue (TfD)\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Pre-trained and frozen large language models (LLMs)\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"robot's visuomotor functions\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"retrieval-augmented LLM prompting\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"HELPER\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"TEACh\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"HELPER\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Execution from Dialog History (EDH)\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"HELPER\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Trajectory from Dialogue (TfD)\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"HELPER\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Pre-trained and frozen large language models (LLMs)\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"HELPER\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"robot's visuomotor functions\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"HELPER\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"retrieval-augmented LLM prompting\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_k3QqbyDh6JvckPrpHeaXpHH0\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1011,\n",
      "              \"output_tokens\": 448,\n",
      "              \"total_tokens\": 1459\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 448,\n",
      "      \"prompt_tokens\": 1011,\n",
      "      \"total_tokens\": 1459,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [5.98s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [5.99s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Highly effective, task-specific prompts are often heavily engineered by\\nexperts to integrate detailed instructions and domain insights based on a deep\\nunderstanding of both instincts of large language models (LLMs) and the\\nintricacies of the target task. However, automating the generation of such\\nexpert-level prompts remains elusive. Existing prompt optimization methods tend\\nto overlook the depth of domain knowledge and struggle to efficiently explore\\nthe vast space of expert-level prompts. Addressing this, we present\\nPromptAgent, an optimization method that autonomously crafts prompts equivalent\\nin quality to those handcrafted by experts. At its core, PromptAgent views\\nprompt optimization as a strategic planning problem and employs a principled\\nplanning algorithm, rooted in Monte Carlo tree search, to strategically\\nnavigate the expert-level prompt space. Inspired by human-like trial-and-error\\nexploration, PromptAgent induces precise expert-level insights and in-depth\\ninstructions by reflecting on model errors and generating constructive error\\nfeedback. Such a novel framework allows the agent to iteratively examine\\nintermediate prompts (states), refine them based on error feedbacks (actions),\\nsimulate future rewards, and search for high-reward paths leading to expert\\nprompts. We apply PromptAgent to 12 tasks spanning three practical domains:\\nBIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing\\nit significantly outperforms strong Chain-of-Thought and recent prompt\\noptimization baselines. Extensive analyses emphasize its capability to craft\\nexpert-level, detailed, and domain-insightful prompts with great efficiency and\\ngeneralizability.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Highly effective, task-specific prompts are often heavily engineered by\\nexperts to integrate detailed instructions and domain insights based on a deep\\nunderstanding of both instincts of large language models (LLMs) and the\\nintricacies of the target task. However, automating the generation of such\\nexpert-level prompts remains elusive. Existing prompt optimization methods tend\\nto overlook the depth of domain knowledge and struggle to efficiently explore\\nthe vast space of expert-level prompts. Addressing this, we present\\nPromptAgent, an optimization method that autonomously crafts prompts equivalent\\nin quality to those handcrafted by experts. At its core, PromptAgent views\\nprompt optimization as a strategic planning problem and employs a principled\\nplanning algorithm, rooted in Monte Carlo tree search, to strategically\\nnavigate the expert-level prompt space. Inspired by human-like trial-and-error\\nexploration, PromptAgent induces precise expert-level insights and in-depth\\ninstructions by reflecting on model errors and generating constructive error\\nfeedback. Such a novel framework allows the agent to iteratively examine\\nintermediate prompts (states), refine them based on error feedbacks (actions),\\nsimulate future rewards, and search for high-reward paths leading to expert\\nprompts. We apply PromptAgent to 12 tasks spanning three practical domains:\\nBIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing\\nit significantly outperforms strong Chain-of-Thought and recent prompt\\noptimization baselines. Extensive analyses emphasize its capability to craft\\nexpert-level, detailed, and domain-insightful prompts with great efficiency and\\ngeneralizability.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Highly effective, task-specific prompts are often heavily engineered by\\nexperts to integrate detailed instructions and domain insights based on a deep\\nunderstanding of both instincts of large language models (LLMs) and the\\nintricacies of the target task. However, automating the generation of such\\nexpert-level prompts remains elusive. Existing prompt optimization methods tend\\nto overlook the depth of domain knowledge and struggle to efficiently explore\\nthe vast space of expert-level prompts. Addressing this, we present\\nPromptAgent, an optimization method that autonomously crafts prompts equivalent\\nin quality to those handcrafted by experts. At its core, PromptAgent views\\nprompt optimization as a strategic planning problem and employs a principled\\nplanning algorithm, rooted in Monte Carlo tree search, to strategically\\nnavigate the expert-level prompt space. Inspired by human-like trial-and-error\\nexploration, PromptAgent induces precise expert-level insights and in-depth\\ninstructions by reflecting on model errors and generating constructive error\\nfeedback. Such a novel framework allows the agent to iteratively examine\\nintermediate prompts (states), refine them based on error feedbacks (actions),\\nsimulate future rewards, and search for high-reward paths leading to expert\\nprompts. We apply PromptAgent to 12 tasks spanning three practical domains:\\nBIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing\\nit significantly outperforms strong Chain-of-Thought and recent prompt\\noptimization baselines. Extensive analyses emphasize its capability to craft\\nexpert-level, detailed, and domain-insightful prompts with great efficiency and\\ngeneralizability.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [3.37s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_LhpfzA9gWmwtew70xVJAM33c\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"PromptAgent\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"PromptAgent: Autonomous Expert-Level Prompt Optimization\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"PromptAgent is an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. It views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to navigate the expert-level prompt space. PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. It significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines across 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-specific, and general NLP tasks.\\\"}]}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"PromptAgent\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"BIG-Bench Hard (BBH)\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptAgent\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"domain-specific tasks\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptAgent\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"general NLP tasks\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 263,\n",
      "                \"prompt_tokens\": 1066,\n",
      "                \"total_tokens\": 1329,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8ed0b56b-9cda-4162-b6bc-82a3eb4c7d2b-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"PromptAgent\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"PromptAgent: Autonomous Expert-Level Prompt Optimization\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"PromptAgent is an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. It views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to navigate the expert-level prompt space. PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. It significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines across 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-specific, and general NLP tasks.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptAgent\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"BIG-Bench Hard (BBH)\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptAgent\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"domain-specific tasks\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptAgent\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"general NLP tasks\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_LhpfzA9gWmwtew70xVJAM33c\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1066,\n",
      "              \"output_tokens\": 263,\n",
      "              \"total_tokens\": 1329\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 263,\n",
      "      \"prompt_tokens\": 1066,\n",
      "      \"total_tokens\": 1329,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [3.37s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [3.38s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Large language models (LLMs) have revolutionized AI across diverse domains,\\nshowcasing remarkable capabilities. Central to their success is the concept of\\nprompting, which guides model output generation. However, manual prompt\\nengineering is labor-intensive and domain-specific, necessitating automated\\nsolutions. This paper introduces PromptWizard, a novel framework leveraging\\nLLMs to iteratively synthesize and refine prompts tailored to specific tasks.\\nUnlike existing approaches, PromptWizard optimizes both prompt instructions and\\nin-context examples, maximizing model performance. The framework iteratively\\nrefines prompts by mutating instructions and incorporating negative examples to\\ndeepen understanding and ensure diversity. It further enhances both\\ninstructions and examples with the aid of a critic, synthesizing new\\ninstructions and examples enriched with detailed reasoning steps for optimal\\nperformance. PromptWizard offers several key features and capabilities,\\nincluding computational efficiency compared to state-of-the-art approaches,\\nadaptability to scenarios with varying amounts of training data, and\\neffectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8\\ndatasets demonstrates PromptWizard's superiority over existing prompt\\nstrategies, showcasing its efficacy and scalability in prompt optimization.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Large language models (LLMs) have revolutionized AI across diverse domains,\\nshowcasing remarkable capabilities. Central to their success is the concept of\\nprompting, which guides model output generation. However, manual prompt\\nengineering is labor-intensive and domain-specific, necessitating automated\\nsolutions. This paper introduces PromptWizard, a novel framework leveraging\\nLLMs to iteratively synthesize and refine prompts tailored to specific tasks.\\nUnlike existing approaches, PromptWizard optimizes both prompt instructions and\\nin-context examples, maximizing model performance. The framework iteratively\\nrefines prompts by mutating instructions and incorporating negative examples to\\ndeepen understanding and ensure diversity. It further enhances both\\ninstructions and examples with the aid of a critic, synthesizing new\\ninstructions and examples enriched with detailed reasoning steps for optimal\\nperformance. PromptWizard offers several key features and capabilities,\\nincluding computational efficiency compared to state-of-the-art approaches,\\nadaptability to scenarios with varying amounts of training data, and\\neffectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8\\ndatasets demonstrates PromptWizard's superiority over existing prompt\\nstrategies, showcasing its efficacy and scalability in prompt optimization.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Large language models (LLMs) have revolutionized AI across diverse domains,\\nshowcasing remarkable capabilities. Central to their success is the concept of\\nprompting, which guides model output generation. However, manual prompt\\nengineering is labor-intensive and domain-specific, necessitating automated\\nsolutions. This paper introduces PromptWizard, a novel framework leveraging\\nLLMs to iteratively synthesize and refine prompts tailored to specific tasks.\\nUnlike existing approaches, PromptWizard optimizes both prompt instructions and\\nin-context examples, maximizing model performance. The framework iteratively\\nrefines prompts by mutating instructions and incorporating negative examples to\\ndeepen understanding and ensure diversity. It further enhances both\\ninstructions and examples with the aid of a critic, synthesizing new\\ninstructions and examples enriched with detailed reasoning steps for optimal\\nperformance. PromptWizard offers several key features and capabilities,\\nincluding computational efficiency compared to state-of-the-art approaches,\\nadaptability to scenarios with varying amounts of training data, and\\neffectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8\\ndatasets demonstrates PromptWizard's superiority over existing prompt\\nstrategies, showcasing its efficacy and scalability in prompt optimization.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [7.50s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_QHfy6ftv8ZXUfZrgTLEmHSOI\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"PromptWizard\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"PromptWizard: A Novel Framework for Prompt Optimization\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"PromptWizard leverages LLMs to iteratively synthesize and refine prompts tailored to specific tasks, optimizing both prompt instructions and in-context examples to maximize model performance. It incorporates negative examples and a critic to enhance instructions and examples with detailed reasoning steps, offering computational efficiency, adaptability, and effectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8 datasets demonstrates its superiority over existing prompt strategies.\\\"}]}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Large language models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompting\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompt engineering\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"automated solutions\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompt instructions\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"in-context examples\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"model performance\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"negative examples\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"detailed reasoning steps\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"computational efficiency\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"training data\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"smaller LLMs\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptWizard\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompt optimization\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 553,\n",
      "                \"prompt_tokens\": 981,\n",
      "                \"total_tokens\": 1534,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-bce822cc-6c5f-4a59-9a91-5b1486450fae-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"PromptWizard\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"PromptWizard: A Novel Framework for Prompt Optimization\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"PromptWizard leverages LLMs to iteratively synthesize and refine prompts tailored to specific tasks, optimizing both prompt instructions and in-context examples to maximize model performance. It incorporates negative examples and a critic to enhance instructions and examples with detailed reasoning steps, offering computational efficiency, adaptability, and effectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8 datasets demonstrates its superiority over existing prompt strategies.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Large language models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompting\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompt engineering\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"automated solutions\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompt instructions\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"in-context examples\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"model performance\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"negative examples\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"detailed reasoning steps\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"computational efficiency\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"training data\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"smaller LLMs\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptWizard\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompt optimization\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_QHfy6ftv8ZXUfZrgTLEmHSOI\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 981,\n",
      "              \"output_tokens\": 553,\n",
      "              \"total_tokens\": 1534\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 553,\n",
      "      \"prompt_tokens\": 981,\n",
      "      \"total_tokens\": 1534,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [7.51s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [7.51s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Recent advancements in Large Language Models (LLMs) and Prompt Engineering\\nhave made chatbot customization more accessible, significantly reducing\\nbarriers to tasks that previously required programming skills. However, prompt\\nevaluation, especially at the dataset scale, remains complex due to the need to\\nassess prompts across thousands of test instances within a dataset. Our study,\\nbased on a comprehensive literature review and pilot study, summarized five\\ncritical challenges in prompt evaluation. In response, we introduce a\\nfeature-oriented workflow for systematic prompt evaluation. In the context of\\ntext summarization, our workflow advocates evaluation with summary\\ncharacteristics (feature metrics) such as complexity, formality, or\\nnaturalness, instead of using traditional quality metrics like ROUGE. This\\ndesign choice enables a more user-friendly evaluation of prompts, as it guides\\nusers in sorting through the ambiguity inherent in natural language. To support\\nthis workflow, we introduce Awesum, a visual analytics system that facilitates\\nidentifying optimal prompt refinements for text summarization through\\ninteractive visualizations, featuring a novel Prompt Comparator design that\\nemploys a BubbleSet-inspired design enhanced by dimensionality reduction\\ntechniques. We evaluate the effectiveness and general applicability of the\\nsystem with practitioners from various domains and found that (1) our design\\nhelps overcome the learning curve for non-technical people to conduct a\\nsystematic evaluation of summarization prompts, and (2) our feature-oriented\\nworkflow has the potential to generalize to other NLG and image-generation\\ntasks. For future works, we advocate moving towards feature-oriented evaluation\\nof LLM prompts and discuss unsolved challenges in terms of human-agent\\ninteraction.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Recent advancements in Large Language Models (LLMs) and Prompt Engineering\\nhave made chatbot customization more accessible, significantly reducing\\nbarriers to tasks that previously required programming skills. However, prompt\\nevaluation, especially at the dataset scale, remains complex due to the need to\\nassess prompts across thousands of test instances within a dataset. Our study,\\nbased on a comprehensive literature review and pilot study, summarized five\\ncritical challenges in prompt evaluation. In response, we introduce a\\nfeature-oriented workflow for systematic prompt evaluation. In the context of\\ntext summarization, our workflow advocates evaluation with summary\\ncharacteristics (feature metrics) such as complexity, formality, or\\nnaturalness, instead of using traditional quality metrics like ROUGE. This\\ndesign choice enables a more user-friendly evaluation of prompts, as it guides\\nusers in sorting through the ambiguity inherent in natural language. To support\\nthis workflow, we introduce Awesum, a visual analytics system that facilitates\\nidentifying optimal prompt refinements for text summarization through\\ninteractive visualizations, featuring a novel Prompt Comparator design that\\nemploys a BubbleSet-inspired design enhanced by dimensionality reduction\\ntechniques. We evaluate the effectiveness and general applicability of the\\nsystem with practitioners from various domains and found that (1) our design\\nhelps overcome the learning curve for non-technical people to conduct a\\nsystematic evaluation of summarization prompts, and (2) our feature-oriented\\nworkflow has the potential to generalize to other NLG and image-generation\\ntasks. For future works, we advocate moving towards feature-oriented evaluation\\nof LLM prompts and discuss unsolved challenges in terms of human-agent\\ninteraction.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Recent advancements in Large Language Models (LLMs) and Prompt Engineering\\nhave made chatbot customization more accessible, significantly reducing\\nbarriers to tasks that previously required programming skills. However, prompt\\nevaluation, especially at the dataset scale, remains complex due to the need to\\nassess prompts across thousands of test instances within a dataset. Our study,\\nbased on a comprehensive literature review and pilot study, summarized five\\ncritical challenges in prompt evaluation. In response, we introduce a\\nfeature-oriented workflow for systematic prompt evaluation. In the context of\\ntext summarization, our workflow advocates evaluation with summary\\ncharacteristics (feature metrics) such as complexity, formality, or\\nnaturalness, instead of using traditional quality metrics like ROUGE. This\\ndesign choice enables a more user-friendly evaluation of prompts, as it guides\\nusers in sorting through the ambiguity inherent in natural language. To support\\nthis workflow, we introduce Awesum, a visual analytics system that facilitates\\nidentifying optimal prompt refinements for text summarization through\\ninteractive visualizations, featuring a novel Prompt Comparator design that\\nemploys a BubbleSet-inspired design enhanced by dimensionality reduction\\ntechniques. We evaluate the effectiveness and general applicability of the\\nsystem with practitioners from various domains and found that (1) our design\\nhelps overcome the learning curve for non-technical people to conduct a\\nsystematic evaluation of summarization prompts, and (2) our feature-oriented\\nworkflow has the potential to generalize to other NLG and image-generation\\ntasks. For future works, we advocate moving towards feature-oriented evaluation\\nof LLM prompts and discuss unsolved challenges in terms of human-agent\\ninteraction.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [9.00s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_h4HseUp2x43qaZjqI4C7XEPt\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Recent advancements in Large Language Models (LLMs) and Prompt Engineering\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"chatbot customization\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"prompt evaluation\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"dataset scale\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Our study\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation and introduced a feature-oriented workflow for systematic prompt evaluation.\\\"}]},{\\\"id\\\":\\\"text summarization\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"summary characteristics\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Awesum\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"visual analytics system\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Prompt Comparator\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"BubbleSet-inspired design\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"dimensionality reduction techniques\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"practitioners from various domains\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"non-technical people\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"feature-oriented workflow\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"NLG\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"image-generation tasks\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"human-agent interaction\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Recent advancements in Large Language Models (LLMs) and Prompt Engineering\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"chatbot customization\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"prompt evaluation\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"dataset scale\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Our study\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompt evaluation\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Our study\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"feature-oriented workflow\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"feature-oriented workflow\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"text summarization\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"text summarization\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"summary characteristics\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Awesum\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"visual analytics system\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"visual analytics system\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"Prompt Comparator\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Prompt Comparator\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"BubbleSet-inspired design\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"BubbleSet-inspired design\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"dimensionality reduction techniques\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Our study\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"practitioners from various domains\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Our study\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"non-technical people\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"feature-oriented workflow\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"NLG\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"feature-oriented workflow\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"image-generation tasks\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Our study\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"human-agent interaction\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 747,\n",
      "                \"prompt_tokens\": 1081,\n",
      "                \"total_tokens\": 1828,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-1591f9ac-d047-468b-b593-f95a7c0facbf-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Recent advancements in Large Language Models (LLMs) and Prompt Engineering\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"chatbot customization\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"prompt evaluation\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"dataset scale\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Our study\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation and introduced a feature-oriented workflow for systematic prompt evaluation.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"text summarization\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"summary characteristics\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Awesum\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"visual analytics system\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Prompt Comparator\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"BubbleSet-inspired design\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"dimensionality reduction techniques\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"practitioners from various domains\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"non-technical people\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"feature-oriented workflow\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"NLG\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"image-generation tasks\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"human-agent interaction\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Recent advancements in Large Language Models (LLMs) and Prompt Engineering\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"chatbot customization\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"prompt evaluation\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"dataset scale\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Our study\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompt evaluation\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Our study\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"feature-oriented workflow\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"feature-oriented workflow\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"text summarization\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"text summarization\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"summary characteristics\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Awesum\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"visual analytics system\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"visual analytics system\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"Prompt Comparator\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt Comparator\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"BubbleSet-inspired design\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"BubbleSet-inspired design\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"dimensionality reduction techniques\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Our study\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"practitioners from various domains\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Our study\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"non-technical people\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"feature-oriented workflow\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"NLG\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"feature-oriented workflow\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"image-generation tasks\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Our study\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"human-agent interaction\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_h4HseUp2x43qaZjqI4C7XEPt\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1081,\n",
      "              \"output_tokens\": 747,\n",
      "              \"total_tokens\": 1828\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 747,\n",
      "      \"prompt_tokens\": 1081,\n",
      "      \"total_tokens\": 1828,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [9.01s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [9.01s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"This technical report presents the Drama Engine, a novel framework for\\nagentic interaction with large language models designed for narrative purposes.\\nThe framework adapts multi-agent system principles to create dynamic,\\ncontext-aware companions that can develop over time and interact with users and\\neach other. Key features include multi-agent workflows with delegation, dynamic\\nprompt assembly, and model-agnostic design. The Drama Engine introduces unique\\nelements such as companion development, mood systems, and automatic context\\nsummarising. It is implemented in TypeScript. The framework's applications\\ninclude multi-agent chats and virtual co-workers for creative writing. The\\npaper discusses the system's architecture, prompt assembly process, delegation\\nmechanisms, and moderation techniques, as well as potential ethical\\nconsiderations and future extensions.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"This technical report presents the Drama Engine, a novel framework for\\nagentic interaction with large language models designed for narrative purposes.\\nThe framework adapts multi-agent system principles to create dynamic,\\ncontext-aware companions that can develop over time and interact with users and\\neach other. Key features include multi-agent workflows with delegation, dynamic\\nprompt assembly, and model-agnostic design. The Drama Engine introduces unique\\nelements such as companion development, mood systems, and automatic context\\nsummarising. It is implemented in TypeScript. The framework's applications\\ninclude multi-agent chats and virtual co-workers for creative writing. The\\npaper discusses the system's architecture, prompt assembly process, delegation\\nmechanisms, and moderation techniques, as well as potential ethical\\nconsiderations and future extensions.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: This technical report presents the Drama Engine, a novel framework for\\nagentic interaction with large language models designed for narrative purposes.\\nThe framework adapts multi-agent system principles to create dynamic,\\ncontext-aware companions that can develop over time and interact with users and\\neach other. Key features include multi-agent workflows with delegation, dynamic\\nprompt assembly, and model-agnostic design. The Drama Engine introduces unique\\nelements such as companion development, mood systems, and automatic context\\nsummarising. It is implemented in TypeScript. The framework's applications\\ninclude multi-agent chats and virtual co-workers for creative writing. The\\npaper discusses the system's architecture, prompt assembly process, delegation\\nmechanisms, and moderation techniques, as well as potential ethical\\nconsiderations and future extensions.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [7.99s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_WwOcwsvImgGnh4EtsIGfdXRZ\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Drama Engine\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"Drama Engine\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"This technical report presents the Drama Engine, a novel framework for agentic interaction with large language models designed for narrative purposes. The framework adapts multi-agent system principles to create dynamic, context-aware companions that can develop over time and interact with users and each other. Key features include multi-agent workflows with delegation, dynamic prompt assembly, and model-agnostic design. The Drama Engine introduces unique elements such as companion development, mood systems, and automatic context summarising. It is implemented in TypeScript. The framework's applications include multi-agent chats and virtual co-workers for creative writing. The paper discusses the system's architecture, prompt assembly process, delegation mechanisms, and moderation techniques, as well as potential ethical considerations and future extensions.\\\"}]}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"multi-agent workflows\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"dynamic prompt assembly\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"model-agnostic design\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"companion development\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"mood systems\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"automatic context summarising\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"multi-agent chats\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"virtual co-workers for creative writing\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"system's architecture\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompt assembly process\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"delegation mechanisms\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"moderation techniques\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"ethical considerations\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Drama Engine\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"future extensions\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 648,\n",
      "                \"prompt_tokens\": 901,\n",
      "                \"total_tokens\": 1549,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6556a60b-d2c6-4979-ad47-66295ab9935c-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Drama Engine\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"Drama Engine\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"This technical report presents the Drama Engine, a novel framework for agentic interaction with large language models designed for narrative purposes. The framework adapts multi-agent system principles to create dynamic, context-aware companions that can develop over time and interact with users and each other. Key features include multi-agent workflows with delegation, dynamic prompt assembly, and model-agnostic design. The Drama Engine introduces unique elements such as companion development, mood systems, and automatic context summarising. It is implemented in TypeScript. The framework's applications include multi-agent chats and virtual co-workers for creative writing. The paper discusses the system's architecture, prompt assembly process, delegation mechanisms, and moderation techniques, as well as potential ethical considerations and future extensions.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"multi-agent workflows\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"dynamic prompt assembly\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"model-agnostic design\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"companion development\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"mood systems\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"automatic context summarising\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"multi-agent chats\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"virtual co-workers for creative writing\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"system's architecture\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompt assembly process\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"delegation mechanisms\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"moderation techniques\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"ethical considerations\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Drama Engine\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"future extensions\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_WwOcwsvImgGnh4EtsIGfdXRZ\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 901,\n",
      "              \"output_tokens\": 648,\n",
      "              \"total_tokens\": 1549\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 648,\n",
      "      \"prompt_tokens\": 901,\n",
      "      \"total_tokens\": 1549,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [7.99s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [8.00s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Prompt engineering is a technique that involves augmenting a large\\npre-trained model with task-specific hints, known as prompts, to adapt the\\nmodel to new tasks. Prompts can be created manually as natural language\\ninstructions or generated automatically as either natural language instructions\\nor vector representations. Prompt engineering enables the ability to perform\\npredictions based solely on prompts without updating model parameters, and the\\neasier application of large pre-trained models in real-world tasks. In past\\nyears, Prompt engineering has been well-studied in natural language processing.\\nRecently, it has also been intensively studied in vision-language modeling.\\nHowever, there is currently a lack of a systematic overview of prompt\\nengineering on pre-trained vision-language models. This paper aims to provide a\\ncomprehensive survey of cutting-edge research in prompt engineering on three\\ntypes of vision-language models: multimodal-to-text generation models (e.g.\\nFlamingo), image-text matching models (e.g. CLIP), and text-to-image generation\\nmodels (e.g. Stable Diffusion). For each type of model, a brief model summary,\\nprompting methods, prompting-based applications, and the corresponding\\nresponsibility and integrity issues are summarized and discussed. Furthermore,\\nthe commonalities and differences between prompting on vision-language models,\\nlanguage models, and vision models are also discussed. The challenges, future\\ndirections, and research opportunities are summarized to foster future research\\non this topic.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Prompt engineering is a technique that involves augmenting a large\\npre-trained model with task-specific hints, known as prompts, to adapt the\\nmodel to new tasks. Prompts can be created manually as natural language\\ninstructions or generated automatically as either natural language instructions\\nor vector representations. Prompt engineering enables the ability to perform\\npredictions based solely on prompts without updating model parameters, and the\\neasier application of large pre-trained models in real-world tasks. In past\\nyears, Prompt engineering has been well-studied in natural language processing.\\nRecently, it has also been intensively studied in vision-language modeling.\\nHowever, there is currently a lack of a systematic overview of prompt\\nengineering on pre-trained vision-language models. This paper aims to provide a\\ncomprehensive survey of cutting-edge research in prompt engineering on three\\ntypes of vision-language models: multimodal-to-text generation models (e.g.\\nFlamingo), image-text matching models (e.g. CLIP), and text-to-image generation\\nmodels (e.g. Stable Diffusion). For each type of model, a brief model summary,\\nprompting methods, prompting-based applications, and the corresponding\\nresponsibility and integrity issues are summarized and discussed. Furthermore,\\nthe commonalities and differences between prompting on vision-language models,\\nlanguage models, and vision models are also discussed. The challenges, future\\ndirections, and research opportunities are summarized to foster future research\\non this topic.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Prompt engineering is a technique that involves augmenting a large\\npre-trained model with task-specific hints, known as prompts, to adapt the\\nmodel to new tasks. Prompts can be created manually as natural language\\ninstructions or generated automatically as either natural language instructions\\nor vector representations. Prompt engineering enables the ability to perform\\npredictions based solely on prompts without updating model parameters, and the\\neasier application of large pre-trained models in real-world tasks. In past\\nyears, Prompt engineering has been well-studied in natural language processing.\\nRecently, it has also been intensively studied in vision-language modeling.\\nHowever, there is currently a lack of a systematic overview of prompt\\nengineering on pre-trained vision-language models. This paper aims to provide a\\ncomprehensive survey of cutting-edge research in prompt engineering on three\\ntypes of vision-language models: multimodal-to-text generation models (e.g.\\nFlamingo), image-text matching models (e.g. CLIP), and text-to-image generation\\nmodels (e.g. Stable Diffusion). For each type of model, a brief model summary,\\nprompting methods, prompting-based applications, and the corresponding\\nresponsibility and integrity issues are summarized and discussed. Furthermore,\\nthe commonalities and differences between prompting on vision-language models,\\nlanguage models, and vision models are also discussed. The challenges, future\\ndirections, and research opportunities are summarized to foster future research\\non this topic.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [8.67s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_mGDJgSM2w5KnCUfPZdxsOPpQ\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Prompt engineering\\\",\\\"type\\\":\\\"Topic\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"A technique that involves augmenting a large pre-trained model with task-specific hints, known as prompts, to adapt the model to new tasks.\\\"}]},{\\\"id\\\":\\\"natural language processing\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"vision-language modeling\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"multimodal-to-text generation models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Flamingo\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"image-text matching models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"CLIP\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"text-to-image generation models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Stable Diffusion\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"This paper\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"A comprehensive survey of cutting-edge research in prompt engineering on vision-language models\\\"}]},{\\\"id\\\":\\\"Prompting methods\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Prompting-based applications\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Responsibility and integrity issues\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Commonalities and differences between prompting on vision-language models, language models, and vision models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Challenges, future directions, and research opportunities\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Prompt engineering\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"natural language processing\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Prompt engineering\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"vision-language modeling\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Prompt engineering\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"multimodal-to-text generation models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Flamingo\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"image-text matching models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"CLIP\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"text-to-image generation models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Stable Diffusion\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Prompting methods\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Prompting-based applications\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Responsibility and integrity issues\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Commonalities and differences between prompting on vision-language models, language models, and vision models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"This paper\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Challenges, future directions, and research opportunities\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 741,\n",
      "                \"prompt_tokens\": 1037,\n",
      "                \"total_tokens\": 1778,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-d04a5a55-3e1b-41dc-bc1a-78e4f63934d7-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Prompt engineering\",\n",
      "                      \"type\": \"Topic\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"A technique that involves augmenting a large pre-trained model with task-specific hints, known as prompts, to adapt the model to new tasks.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"natural language processing\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"vision-language modeling\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"multimodal-to-text generation models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Flamingo\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"image-text matching models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"CLIP\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"text-to-image generation models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Stable Diffusion\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"This paper\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"A comprehensive survey of cutting-edge research in prompt engineering on vision-language models\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Prompting methods\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Prompting-based applications\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Responsibility and integrity issues\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Commonalities and differences between prompting on vision-language models, language models, and vision models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Challenges, future directions, and research opportunities\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt engineering\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"natural language processing\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt engineering\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"vision-language modeling\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Prompt engineering\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"multimodal-to-text generation models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Flamingo\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"image-text matching models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"CLIP\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"text-to-image generation models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Stable Diffusion\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Prompting methods\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Prompting-based applications\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Responsibility and integrity issues\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Commonalities and differences between prompting on vision-language models, language models, and vision models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"This paper\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Challenges, future directions, and research opportunities\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_mGDJgSM2w5KnCUfPZdxsOPpQ\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1037,\n",
      "              \"output_tokens\": 741,\n",
      "              \"total_tokens\": 1778\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 741,\n",
      "      \"prompt_tokens\": 1037,\n",
      "      \"total_tokens\": 1778,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [8.67s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [8.67s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Prompt optimization aims to find the best prompt to a large language model\\n(LLM) for a given task. LLMs have been successfully used to help find and\\nimprove prompt candidates for single-step tasks. However, realistic tasks for\\nagents are multi-step and introduce new challenges: (1) Prompt content is\\nlikely to be more extensive and complex, making it more difficult for LLMs to\\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\\nand (3) different people may have varied preferences about task execution.\\nWhile humans struggle to optimize prompts, they are good at providing feedback\\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\\noptimization framework PROMST that incorporates human-designed feedback rules\\nto automatically offer direct suggestions for improvement. We also use an extra\\nlearned heuristic model that predicts prompt performance to efficiently sample\\nfrom prompt candidates. This approach significantly outperforms both\\nhuman-engineered prompts and several other prompt optimization methods across\\n11 representative multi-step tasks (an average 10.6\\\\%-29.3\\\\% improvement to\\ncurrent best methods on five LLMs respectively). We believe our work can serve\\nas a benchmark for automatic prompt optimization for LLM-driven multi-step\\ntasks. Datasets and Codes are available at\\nhttps://github.com/yongchao98/PROMST. Project Page is available at\\nhttps://yongchao98.github.io/MIT-REALM-PROMST/.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Prompt optimization aims to find the best prompt to a large language model\\n(LLM) for a given task. LLMs have been successfully used to help find and\\nimprove prompt candidates for single-step tasks. However, realistic tasks for\\nagents are multi-step and introduce new challenges: (1) Prompt content is\\nlikely to be more extensive and complex, making it more difficult for LLMs to\\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\\nand (3) different people may have varied preferences about task execution.\\nWhile humans struggle to optimize prompts, they are good at providing feedback\\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\\noptimization framework PROMST that incorporates human-designed feedback rules\\nto automatically offer direct suggestions for improvement. We also use an extra\\nlearned heuristic model that predicts prompt performance to efficiently sample\\nfrom prompt candidates. This approach significantly outperforms both\\nhuman-engineered prompts and several other prompt optimization methods across\\n11 representative multi-step tasks (an average 10.6\\\\%-29.3\\\\% improvement to\\ncurrent best methods on five LLMs respectively). We believe our work can serve\\nas a benchmark for automatic prompt optimization for LLM-driven multi-step\\ntasks. Datasets and Codes are available at\\nhttps://github.com/yongchao98/PROMST. Project Page is available at\\nhttps://yongchao98.github.io/MIT-REALM-PROMST/.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Prompt optimization aims to find the best prompt to a large language model\\n(LLM) for a given task. LLMs have been successfully used to help find and\\nimprove prompt candidates for single-step tasks. However, realistic tasks for\\nagents are multi-step and introduce new challenges: (1) Prompt content is\\nlikely to be more extensive and complex, making it more difficult for LLMs to\\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\\nand (3) different people may have varied preferences about task execution.\\nWhile humans struggle to optimize prompts, they are good at providing feedback\\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\\noptimization framework PROMST that incorporates human-designed feedback rules\\nto automatically offer direct suggestions for improvement. We also use an extra\\nlearned heuristic model that predicts prompt performance to efficiently sample\\nfrom prompt candidates. This approach significantly outperforms both\\nhuman-engineered prompts and several other prompt optimization methods across\\n11 representative multi-step tasks (an average 10.6\\\\%-29.3\\\\% improvement to\\ncurrent best methods on five LLMs respectively). We believe our work can serve\\nas a benchmark for automatic prompt optimization for LLM-driven multi-step\\ntasks. Datasets and Codes are available at\\nhttps://github.com/yongchao98/PROMST. Project Page is available at\\nhttps://yongchao98.github.io/MIT-REALM-PROMST/.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [3.82s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_tBwGaXX3XWpNWJy9UXwRDEY7\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Prompt optimization\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"large language model\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"PROMST\\\",\\\"type\\\":\\\"Topic\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"A new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement.\\\"},{\\\"key\\\":\\\"url\\\",\\\"value\\\":\\\"https://github.com/yongchao98/PROMST\\\"}]},{\\\"id\\\":\\\"Paper on PROMST\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"PROMST: A New LLM-driven Discrete Prompt Optimization Framework\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"This paper introduces PROMST, a framework that uses human-designed feedback rules and a learned heuristic model to optimize prompts for multi-step tasks, significantly outperforming other methods.\\\"},{\\\"key\\\":\\\"url\\\",\\\"value\\\":\\\"https://yongchao98.github.io/MIT-REALM-PROMST/\\\"}]}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Prompt optimization\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"large language model\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"PROMST\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"large language model\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"RELATED_TO\\\"},{\\\"source_node_id\\\":\\\"Paper on PROMST\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"PROMST\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 289,\n",
      "                \"prompt_tokens\": 1052,\n",
      "                \"total_tokens\": 1341,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c5072629-295f-45fc-b70d-5e9834382f84-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Prompt optimization\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"large language model\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"PROMST\",\n",
      "                      \"type\": \"Topic\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"A new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement.\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"url\",\n",
      "                          \"value\": \"https://github.com/yongchao98/PROMST\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Paper on PROMST\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"PROMST: A New LLM-driven Discrete Prompt Optimization Framework\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"This paper introduces PROMST, a framework that uses human-designed feedback rules and a learned heuristic model to optimize prompts for multi-step tasks, significantly outperforming other methods.\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"url\",\n",
      "                          \"value\": \"https://yongchao98.github.io/MIT-REALM-PROMST/\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt optimization\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"large language model\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PROMST\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"large language model\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Paper on PROMST\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"PROMST\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_tBwGaXX3XWpNWJy9UXwRDEY7\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1052,\n",
      "              \"output_tokens\": 289,\n",
      "              \"total_tokens\": 1341\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 289,\n",
      "      \"prompt_tokens\": 1052,\n",
      "      \"total_tokens\": 1341,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [3.82s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [3.82s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"The increasing reliance on large language models (LLMs) such as ChatGPT in\\nvarious fields emphasizes the importance of ``prompt engineering,'' a\\ntechnology to improve the quality of model outputs. With companies investing\\nsignificantly in expert prompt engineers and educational resources rising to\\nmeet market demand, designing high-quality prompts has become an intriguing\\nchallenge. In this paper, we propose a novel attack against LLMs, named prompt\\nstealing attacks. Our proposed prompt stealing attack aims to steal these\\nwell-designed prompts based on the generated answers. The prompt stealing\\nattack contains two primary modules: the parameter extractor and the prompt\\nreconstruction. The goal of the parameter extractor is to figure out the\\nproperties of the original prompts. We first observe that most prompts fall\\ninto one of three categories: direct prompt, role-based prompt, and in-context\\nprompt. Our parameter extractor first tries to distinguish the type of prompts\\nbased on the generated answers. Then, it can further predict which role or how\\nmany contexts are used based on the types of prompts. Following the parameter\\nextractor, the prompt reconstructor can be used to reconstruct the original\\nprompts based on the generated answers and the extracted features. The final\\ngoal of the prompt reconstructor is to generate the reversed prompts, which are\\nsimilar to the original prompts. Our experimental results show the remarkable\\nperformance of our proposed attacks. Our proposed attacks add a new dimension\\nto the study of prompt engineering and call for more attention to the security\\nissues on LLMs.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"The increasing reliance on large language models (LLMs) such as ChatGPT in\\nvarious fields emphasizes the importance of ``prompt engineering,'' a\\ntechnology to improve the quality of model outputs. With companies investing\\nsignificantly in expert prompt engineers and educational resources rising to\\nmeet market demand, designing high-quality prompts has become an intriguing\\nchallenge. In this paper, we propose a novel attack against LLMs, named prompt\\nstealing attacks. Our proposed prompt stealing attack aims to steal these\\nwell-designed prompts based on the generated answers. The prompt stealing\\nattack contains two primary modules: the parameter extractor and the prompt\\nreconstruction. The goal of the parameter extractor is to figure out the\\nproperties of the original prompts. We first observe that most prompts fall\\ninto one of three categories: direct prompt, role-based prompt, and in-context\\nprompt. Our parameter extractor first tries to distinguish the type of prompts\\nbased on the generated answers. Then, it can further predict which role or how\\nmany contexts are used based on the types of prompts. Following the parameter\\nextractor, the prompt reconstructor can be used to reconstruct the original\\nprompts based on the generated answers and the extracted features. The final\\ngoal of the prompt reconstructor is to generate the reversed prompts, which are\\nsimilar to the original prompts. Our experimental results show the remarkable\\nperformance of our proposed attacks. Our proposed attacks add a new dimension\\nto the study of prompt engineering and call for more attention to the security\\nissues on LLMs.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: The increasing reliance on large language models (LLMs) such as ChatGPT in\\nvarious fields emphasizes the importance of ``prompt engineering,'' a\\ntechnology to improve the quality of model outputs. With companies investing\\nsignificantly in expert prompt engineers and educational resources rising to\\nmeet market demand, designing high-quality prompts has become an intriguing\\nchallenge. In this paper, we propose a novel attack against LLMs, named prompt\\nstealing attacks. Our proposed prompt stealing attack aims to steal these\\nwell-designed prompts based on the generated answers. The prompt stealing\\nattack contains two primary modules: the parameter extractor and the prompt\\nreconstruction. The goal of the parameter extractor is to figure out the\\nproperties of the original prompts. We first observe that most prompts fall\\ninto one of three categories: direct prompt, role-based prompt, and in-context\\nprompt. Our parameter extractor first tries to distinguish the type of prompts\\nbased on the generated answers. Then, it can further predict which role or how\\nmany contexts are used based on the types of prompts. Following the parameter\\nextractor, the prompt reconstructor can be used to reconstruct the original\\nprompts based on the generated answers and the extracted features. The final\\ngoal of the prompt reconstructor is to generate the reversed prompts, which are\\nsimilar to the original prompts. Our experimental results show the remarkable\\nperformance of our proposed attacks. Our proposed attacks add a new dimension\\nto the study of prompt engineering and call for more attention to the security\\nissues on LLMs.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [4.03s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_VY3E2tKWj0FJgUCnnovMbRFj\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Prompt Stealing Attacks\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"Prompt Stealing Attacks\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"The paper proposes a novel attack against LLMs, named prompt stealing attacks, which aims to steal well-designed prompts based on generated answers. The attack contains two primary modules: the parameter extractor and the prompt reconstruction.\\\"}]},{\\\"id\\\":\\\"Prompt Engineering\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Large Language Models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Parameter Extractor\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Prompt Reconstruction\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Prompt Stealing Attacks\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Prompt Engineering\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Prompt Stealing Attacks\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Large Language Models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Prompt Stealing Attacks\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Parameter Extractor\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Prompt Stealing Attacks\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Prompt Reconstruction\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 267,\n",
      "                \"prompt_tokens\": 1059,\n",
      "                \"total_tokens\": 1326,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f1a662e7-f355-4772-ab99-bec4b49c82ca-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Prompt Stealing Attacks\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"Prompt Stealing Attacks\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"The paper proposes a novel attack against LLMs, named prompt stealing attacks, which aims to steal well-designed prompts based on generated answers. The attack contains two primary modules: the parameter extractor and the prompt reconstruction.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Prompt Engineering\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Large Language Models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Parameter Extractor\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Prompt Reconstruction\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt Stealing Attacks\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Prompt Engineering\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt Stealing Attacks\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Large Language Models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt Stealing Attacks\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Parameter Extractor\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Prompt Stealing Attacks\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Prompt Reconstruction\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_VY3E2tKWj0FJgUCnnovMbRFj\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1059,\n",
      "              \"output_tokens\": 267,\n",
      "              \"total_tokens\": 1326\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 267,\n",
      "      \"prompt_tokens\": 1059,\n",
      "      \"total_tokens\": 1326,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [4.03s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [4.04s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Recent trends are emerging in the use of Large Language Models (LLMs) as\\nautonomous agents that take actions based on the content of the user text\\nprompts. We intend to apply these concepts to the field of Guidance,\\nNavigation, and Control in space, enabling LLMs to have a significant role in\\nthe decision-making process for autonomous satellite operations. As a first\\nstep towards this goal, we have developed a pure LLM-based solution for the\\nKerbal Space Program Differential Games (KSPDG) challenge, a public software\\ndesign competition where participants create autonomous agents for maneuvering\\nsatellites involved in non-cooperative space operations, running on the KSP\\ngame engine. Our approach leverages prompt engineering, few-shot prompting, and\\nfine-tuning techniques to create an effective LLM-based agent that ranked 2nd\\nin the competition. To the best of our knowledge, this work pioneers the\\nintegration of LLM agents into space research. Code is available at\\nhttps://github.com/ARCLab-MIT/kspdg.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Recent trends are emerging in the use of Large Language Models (LLMs) as\\nautonomous agents that take actions based on the content of the user text\\nprompts. We intend to apply these concepts to the field of Guidance,\\nNavigation, and Control in space, enabling LLMs to have a significant role in\\nthe decision-making process for autonomous satellite operations. As a first\\nstep towards this goal, we have developed a pure LLM-based solution for the\\nKerbal Space Program Differential Games (KSPDG) challenge, a public software\\ndesign competition where participants create autonomous agents for maneuvering\\nsatellites involved in non-cooperative space operations, running on the KSP\\ngame engine. Our approach leverages prompt engineering, few-shot prompting, and\\nfine-tuning techniques to create an effective LLM-based agent that ranked 2nd\\nin the competition. To the best of our knowledge, this work pioneers the\\nintegration of LLM agents into space research. Code is available at\\nhttps://github.com/ARCLab-MIT/kspdg.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Recent trends are emerging in the use of Large Language Models (LLMs) as\\nautonomous agents that take actions based on the content of the user text\\nprompts. We intend to apply these concepts to the field of Guidance,\\nNavigation, and Control in space, enabling LLMs to have a significant role in\\nthe decision-making process for autonomous satellite operations. As a first\\nstep towards this goal, we have developed a pure LLM-based solution for the\\nKerbal Space Program Differential Games (KSPDG) challenge, a public software\\ndesign competition where participants create autonomous agents for maneuvering\\nsatellites involved in non-cooperative space operations, running on the KSP\\ngame engine. Our approach leverages prompt engineering, few-shot prompting, and\\nfine-tuning techniques to create an effective LLM-based agent that ranked 2nd\\nin the competition. To the best of our knowledge, this work pioneers the\\nintegration of LLM agents into space research. Code is available at\\nhttps://github.com/ARCLab-MIT/kspdg.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [6.50s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_Oy9vsmwPiCtd0QGqxAySbhnn\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Recent trends in LLMs as autonomous agents\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"Recent trends in LLMs as autonomous agents\\\"}]},{\\\"id\\\":\\\"Guidance, Navigation, and Control in space\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Kerbal Space Program Differential Games (KSPDG) challenge\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"autonomous satellite operations\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"prompt engineering\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"few-shot prompting\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"fine-tuning techniques\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"https://github.com/ARCLab-MIT/kspdg\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"url\\\",\\\"value\\\":\\\"https://github.com/ARCLab-MIT/kspdg\\\"}]}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Recent trends in LLMs as autonomous agents\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Guidance, Navigation, and Control in space\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Recent trends in LLMs as autonomous agents\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Kerbal Space Program Differential Games (KSPDG) challenge\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Recent trends in LLMs as autonomous agents\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"autonomous satellite operations\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Recent trends in LLMs as autonomous agents\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompt engineering\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Recent trends in LLMs as autonomous agents\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"few-shot prompting\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Recent trends in LLMs as autonomous agents\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"fine-tuning techniques\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Recent trends in LLMs as autonomous agents\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"https://github.com/ARCLab-MIT/kspdg\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"RELATED_TO\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 471,\n",
      "                \"prompt_tokens\": 963,\n",
      "                \"total_tokens\": 1434,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a122d9dd-0d16-4d84-bdf0-45fc01358748-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Recent trends in LLMs as autonomous agents\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"Recent trends in LLMs as autonomous agents\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Guidance, Navigation, and Control in space\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Kerbal Space Program Differential Games (KSPDG) challenge\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"autonomous satellite operations\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"prompt engineering\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"few-shot prompting\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"fine-tuning techniques\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"https://github.com/ARCLab-MIT/kspdg\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"url\",\n",
      "                          \"value\": \"https://github.com/ARCLab-MIT/kspdg\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Recent trends in LLMs as autonomous agents\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Guidance, Navigation, and Control in space\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Recent trends in LLMs as autonomous agents\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Kerbal Space Program Differential Games (KSPDG) challenge\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Recent trends in LLMs as autonomous agents\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"autonomous satellite operations\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Recent trends in LLMs as autonomous agents\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompt engineering\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Recent trends in LLMs as autonomous agents\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"few-shot prompting\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Recent trends in LLMs as autonomous agents\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"fine-tuning techniques\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Recent trends in LLMs as autonomous agents\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"https://github.com/ARCLab-MIT/kspdg\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_Oy9vsmwPiCtd0QGqxAySbhnn\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 963,\n",
      "              \"output_tokens\": 471,\n",
      "              \"total_tokens\": 1434\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 471,\n",
      "      \"prompt_tokens\": 963,\n",
      "      \"total_tokens\": 1434,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [6.50s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [6.51s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"The rise of capabilities expressed by large language models has been quickly\\nfollowed by the integration of the same complex systems into application level\\nlogic. Algorithms, programs, systems, and companies are built around structured\\nprompting to black box models where the majority of the design and\\nimplementation lies in capturing and quantifying the `agent mode'. The standard\\nway to shape a closed language model is to prime it for a specific task with a\\ntailored prompt, often initially handwritten by a human. The textual prompts\\nco-evolve with the codebase, taking shape over the course of project life as\\nartifacts which must be reviewed and maintained, just as the traditional code\\nfiles might be. Unlike traditional code, we find that prompts do not receive\\neffective static testing and linting to prevent runtime issues. In this work,\\nwe present a novel dataset called PromptSet, with more than 61,000 unique\\ndeveloper prompts used in open source Python programs. We perform analysis on\\nthis dataset and introduce the notion of a static linter for prompts. Released\\nwith this publication is a HuggingFace dataset and a Github repository to\\nrecreate collection and processing efforts, both under the name\\n\\\\texttt{pisterlabs/promptset}.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"The rise of capabilities expressed by large language models has been quickly\\nfollowed by the integration of the same complex systems into application level\\nlogic. Algorithms, programs, systems, and companies are built around structured\\nprompting to black box models where the majority of the design and\\nimplementation lies in capturing and quantifying the `agent mode'. The standard\\nway to shape a closed language model is to prime it for a specific task with a\\ntailored prompt, often initially handwritten by a human. The textual prompts\\nco-evolve with the codebase, taking shape over the course of project life as\\nartifacts which must be reviewed and maintained, just as the traditional code\\nfiles might be. Unlike traditional code, we find that prompts do not receive\\neffective static testing and linting to prevent runtime issues. In this work,\\nwe present a novel dataset called PromptSet, with more than 61,000 unique\\ndeveloper prompts used in open source Python programs. We perform analysis on\\nthis dataset and introduce the notion of a static linter for prompts. Released\\nwith this publication is a HuggingFace dataset and a Github repository to\\nrecreate collection and processing efforts, both under the name\\n\\\\texttt{pisterlabs/promptset}.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: The rise of capabilities expressed by large language models has been quickly\\nfollowed by the integration of the same complex systems into application level\\nlogic. Algorithms, programs, systems, and companies are built around structured\\nprompting to black box models where the majority of the design and\\nimplementation lies in capturing and quantifying the `agent mode'. The standard\\nway to shape a closed language model is to prime it for a specific task with a\\ntailored prompt, often initially handwritten by a human. The textual prompts\\nco-evolve with the codebase, taking shape over the course of project life as\\nartifacts which must be reviewed and maintained, just as the traditional code\\nfiles might be. Unlike traditional code, we find that prompts do not receive\\neffective static testing and linting to prevent runtime issues. In this work,\\nwe present a novel dataset called PromptSet, with more than 61,000 unique\\ndeveloper prompts used in open source Python programs. We perform analysis on\\nthis dataset and introduce the notion of a static linter for prompts. Released\\nwith this publication is a HuggingFace dataset and a Github repository to\\nrecreate collection and processing efforts, both under the name\\n\\\\texttt{pisterlabs/promptset}.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [5.49s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_fBRXen2giT7XOCbx4PDBvXEz\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"The rise of capabilities expressed by large language models\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"The rise of capabilities expressed by large language models\\\"}]},{\\\"id\\\":\\\"PromptSet\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"PromptSet\\\"}]},{\\\"id\\\":\\\"large language models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"structured prompting\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"agent mode\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"static testing and linting\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"developer prompts\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"static linter for prompts\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"HuggingFace dataset\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Github repository\\\",\\\"type\\\":\\\"Topic\\\"}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"The rise of capabilities expressed by large language models\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"large language models\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"The rise of capabilities expressed by large language models\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"structured prompting\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"The rise of capabilities expressed by large language models\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"agent mode\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"The rise of capabilities expressed by large language models\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"static testing and linting\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptSet\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"developer prompts\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptSet\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"static linter for prompts\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptSet\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"HuggingFace dataset\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"PromptSet\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Github repository\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 445,\n",
      "                \"prompt_tokens\": 999,\n",
      "                \"total_tokens\": 1444,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-757552e0-f9b3-46b5-a2e1-279923de9c8b-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"The rise of capabilities expressed by large language models\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"The rise of capabilities expressed by large language models\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"PromptSet\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"PromptSet\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"large language models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"structured prompting\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"agent mode\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"static testing and linting\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"developer prompts\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"static linter for prompts\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"HuggingFace dataset\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Github repository\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"The rise of capabilities expressed by large language models\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"large language models\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"The rise of capabilities expressed by large language models\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"structured prompting\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"The rise of capabilities expressed by large language models\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"agent mode\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"The rise of capabilities expressed by large language models\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"static testing and linting\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptSet\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"developer prompts\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptSet\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"static linter for prompts\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptSet\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"HuggingFace dataset\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"PromptSet\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Github repository\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_fBRXen2giT7XOCbx4PDBvXEz\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 999,\n",
      "              \"output_tokens\": 445,\n",
      "              \"total_tokens\": 1444\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 445,\n",
      "      \"prompt_tokens\": 999,\n",
      "      \"total_tokens\": 1444,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [5.50s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [5.50s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Interaction with Large Language Models (LLMs) is primarily carried out via\\nprompting. A prompt is a natural language instruction designed to elicit\\ncertain behaviour or output from a model. In theory, natural language prompts\\nenable non-experts to interact with and leverage LLMs. However, for complex\\ntasks and tasks with specific requirements, prompt design is not trivial.\\nCreating effective prompts requires skill and knowledge, as well as significant\\niteration in order to determine model behavior, and guide the model to\\naccomplish a particular goal. We hypothesize that the way in which users\\niterate on their prompts can provide insight into how they think prompting and\\nmodels work, as well as the kinds of support needed for more efficient prompt\\nengineering. To better understand prompt engineering practices, we analyzed\\nsessions of prompt editing behavior, categorizing the parts of prompts users\\niterated on and the types of changes they made. We discuss design implications\\nand future directions based on these prompt engineering practices.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Interaction with Large Language Models (LLMs) is primarily carried out via\\nprompting. A prompt is a natural language instruction designed to elicit\\ncertain behaviour or output from a model. In theory, natural language prompts\\nenable non-experts to interact with and leverage LLMs. However, for complex\\ntasks and tasks with specific requirements, prompt design is not trivial.\\nCreating effective prompts requires skill and knowledge, as well as significant\\niteration in order to determine model behavior, and guide the model to\\naccomplish a particular goal. We hypothesize that the way in which users\\niterate on their prompts can provide insight into how they think prompting and\\nmodels work, as well as the kinds of support needed for more efficient prompt\\nengineering. To better understand prompt engineering practices, we analyzed\\nsessions of prompt editing behavior, categorizing the parts of prompts users\\niterated on and the types of changes they made. We discuss design implications\\nand future directions based on these prompt engineering practices.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Interaction with Large Language Models (LLMs) is primarily carried out via\\nprompting. A prompt is a natural language instruction designed to elicit\\ncertain behaviour or output from a model. In theory, natural language prompts\\nenable non-experts to interact with and leverage LLMs. However, for complex\\ntasks and tasks with specific requirements, prompt design is not trivial.\\nCreating effective prompts requires skill and knowledge, as well as significant\\niteration in order to determine model behavior, and guide the model to\\naccomplish a particular goal. We hypothesize that the way in which users\\niterate on their prompts can provide insight into how they think prompting and\\nmodels work, as well as the kinds of support needed for more efficient prompt\\nengineering. To better understand prompt engineering practices, we analyzed\\nsessions of prompt editing behavior, categorizing the parts of prompts users\\niterated on and the types of changes they made. We discuss design implications\\nand future directions based on these prompt engineering practices.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [4.31s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_H02ic2ZnV1usFMSeaeFSM0Ns\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Interaction with Large Language Models (LLMs)\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"Interaction with Large Language Models (LLMs)\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Interaction with Large Language Models (LLMs) is primarily carried out via prompting. A prompt is a natural language instruction designed to elicit certain behaviour or output from a model. In theory, natural language prompts enable non-experts to interact with and leverage LLMs. However, for complex tasks and tasks with specific requirements, prompt design is not trivial. Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal. We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering. To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made. We discuss design implications and future directions based on these prompt engineering practices.\\\"}]}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Interaction with Large Language Models (LLMs)\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompting\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Interaction with Large Language Models (LLMs)\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompt design\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Interaction with Large Language Models (LLMs)\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"prompt engineering\\\",\\\"target_node_type\\\":\\\"Topic\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 356,\n",
      "                \"prompt_tokens\": 947,\n",
      "                \"total_tokens\": 1303,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f10eaa13-9bee-47f7-8317-3dc9f887e270-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Interaction with Large Language Models (LLMs)\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"Interaction with Large Language Models (LLMs)\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Interaction with Large Language Models (LLMs) is primarily carried out via prompting. A prompt is a natural language instruction designed to elicit certain behaviour or output from a model. In theory, natural language prompts enable non-experts to interact with and leverage LLMs. However, for complex tasks and tasks with specific requirements, prompt design is not trivial. Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal. We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering. To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made. We discuss design implications and future directions based on these prompt engineering practices.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Interaction with Large Language Models (LLMs)\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompting\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Interaction with Large Language Models (LLMs)\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompt design\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Interaction with Large Language Models (LLMs)\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"prompt engineering\",\n",
      "                      \"target_node_type\": \"Topic\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_H02ic2ZnV1usFMSeaeFSM0Ns\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 947,\n",
      "              \"output_tokens\": 356,\n",
      "              \"total_tokens\": 1303\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 356,\n",
      "      \"prompt_tokens\": 947,\n",
      "      \"total_tokens\": 1303,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [4.31s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [4.32s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Large language models (LLM) are perceived to offer promising potentials for\\nautomating security tasks, such as those found in security operation centers\\n(SOCs). As a first step towards evaluating this perceived potential, we\\ninvestigate the use of LLMs in software pentesting, where the main task is to\\nautomatically identify software security vulnerabilities in source code. We\\nhypothesize that an LLM-based AI agent can be improved over time for a specific\\nsecurity task as human operators interact with it. Such improvement can be\\nmade, as a first step, by engineering prompts fed to the LLM based on the\\nresponses produced, to include relevant contexts and structures so that the\\nmodel provides more accurate results. Such engineering efforts become\\nsustainable if the prompts that are engineered to produce better results on\\ncurrent tasks, also produce better results on future unknown tasks. To examine\\nthis hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains\\n2,740 hand-crafted source code test cases containing various types of\\nvulnerabilities. We divide the test cases into training and testing data, where\\nwe engineer the prompts based on the training data (only), and evaluate the\\nfinal system on the testing data. We compare the AI agent's performance on the\\ntesting data against the performance of the agent without the prompt\\nengineering. We also compare the AI agent's results against those from\\nSonarQube, a widely used static code analyzer for security testing. We built\\nand tested multiple versions of the AI agent using different off-the-shelf LLMs\\n-- Google's Gemini-pro, as well as OpenAI's GPT-3.5-Turbo and GPT-4-Turbo (with\\nboth chat completion and assistant APIs). The results show that using LLMs is a\\nviable approach to build an AI agent for software pentesting that can improve\\nthrough repeated use and prompt engineering.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Large language models (LLM) are perceived to offer promising potentials for\\nautomating security tasks, such as those found in security operation centers\\n(SOCs). As a first step towards evaluating this perceived potential, we\\ninvestigate the use of LLMs in software pentesting, where the main task is to\\nautomatically identify software security vulnerabilities in source code. We\\nhypothesize that an LLM-based AI agent can be improved over time for a specific\\nsecurity task as human operators interact with it. Such improvement can be\\nmade, as a first step, by engineering prompts fed to the LLM based on the\\nresponses produced, to include relevant contexts and structures so that the\\nmodel provides more accurate results. Such engineering efforts become\\nsustainable if the prompts that are engineered to produce better results on\\ncurrent tasks, also produce better results on future unknown tasks. To examine\\nthis hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains\\n2,740 hand-crafted source code test cases containing various types of\\nvulnerabilities. We divide the test cases into training and testing data, where\\nwe engineer the prompts based on the training data (only), and evaluate the\\nfinal system on the testing data. We compare the AI agent's performance on the\\ntesting data against the performance of the agent without the prompt\\nengineering. We also compare the AI agent's results against those from\\nSonarQube, a widely used static code analyzer for security testing. We built\\nand tested multiple versions of the AI agent using different off-the-shelf LLMs\\n-- Google's Gemini-pro, as well as OpenAI's GPT-3.5-Turbo and GPT-4-Turbo (with\\nboth chat completion and assistant APIs). The results show that using LLMs is a\\nviable approach to build an AI agent for software pentesting that can improve\\nthrough repeated use and prompt engineering.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Large language models (LLM) are perceived to offer promising potentials for\\nautomating security tasks, such as those found in security operation centers\\n(SOCs). As a first step towards evaluating this perceived potential, we\\ninvestigate the use of LLMs in software pentesting, where the main task is to\\nautomatically identify software security vulnerabilities in source code. We\\nhypothesize that an LLM-based AI agent can be improved over time for a specific\\nsecurity task as human operators interact with it. Such improvement can be\\nmade, as a first step, by engineering prompts fed to the LLM based on the\\nresponses produced, to include relevant contexts and structures so that the\\nmodel provides more accurate results. Such engineering efforts become\\nsustainable if the prompts that are engineered to produce better results on\\ncurrent tasks, also produce better results on future unknown tasks. To examine\\nthis hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains\\n2,740 hand-crafted source code test cases containing various types of\\nvulnerabilities. We divide the test cases into training and testing data, where\\nwe engineer the prompts based on the training data (only), and evaluate the\\nfinal system on the testing data. We compare the AI agent's performance on the\\ntesting data against the performance of the agent without the prompt\\nengineering. We also compare the AI agent's results against those from\\nSonarQube, a widely used static code analyzer for security testing. We built\\nand tested multiple versions of the AI agent using different off-the-shelf LLMs\\n-- Google's Gemini-pro, as well as OpenAI's GPT-3.5-Turbo and GPT-4-Turbo (with\\nboth chat completion and assistant APIs). The results show that using LLMs is a\\nviable approach to build an AI agent for software pentesting that can improve\\nthrough repeated use and prompt engineering.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [9.12s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_Fuxbom4ofXCub4pR6rkHp6O3\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Large language models\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"security tasks\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"security operation centers\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"software pentesting\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"software security vulnerabilities\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"source code\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"AI agent\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"human operators\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"prompt engineering\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"OWASP Benchmark Project 1.2\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"SonarQube\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"Google's Gemini-pro\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"OpenAI's GPT-3.5-Turbo\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"OpenAI's GPT-4-Turbo\\\",\\\"type\\\":\\\"Topic\\\"},{\\\"id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"title\\\",\\\"value\\\":\\\"LLM-based AI agent for software pentesting\\\"},{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Investigates the use of LLMs in software pentesting to identify software security vulnerabilities in source code, and evaluates the improvement of an AI agent through prompt engineering.\\\"}]}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Large language models\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"security tasks\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"security operation centers\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"software pentesting\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"software security vulnerabilities\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"source code\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"AI agent\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"human operators\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"prompt engineering\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"OWASP Benchmark Project 1.2\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"SonarQube\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"Google's Gemini-pro\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"OpenAI's GPT-3.5-Turbo\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"},{\\\"source_node_id\\\":\\\"OpenAI's GPT-4-Turbo\\\",\\\"source_node_type\\\":\\\"Topic\\\",\\\"target_node_id\\\":\\\"LLM-based AI agent for software pentesting\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"DISCUSSES\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 813,\n",
      "                \"prompt_tokens\": 1137,\n",
      "                \"total_tokens\": 1950,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-cc7ae1a4-a876-4181-a3eb-1fd1c2594b7b-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Large language models\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"security tasks\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"security operation centers\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"software pentesting\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"software security vulnerabilities\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"source code\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"AI agent\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"human operators\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"prompt engineering\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"OWASP Benchmark Project 1.2\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"SonarQube\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Google's Gemini-pro\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"OpenAI's GPT-3.5-Turbo\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"OpenAI's GPT-4-Turbo\",\n",
      "                      \"type\": \"Topic\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"title\",\n",
      "                          \"value\": \"LLM-based AI agent for software pentesting\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Investigates the use of LLMs in software pentesting to identify software security vulnerabilities in source code, and evaluates the improvement of an AI agent through prompt engineering.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Large language models\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"security tasks\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"security operation centers\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"software pentesting\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"software security vulnerabilities\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"source code\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"AI agent\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"human operators\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"prompt engineering\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"OWASP Benchmark Project 1.2\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"SonarQube\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"Google's Gemini-pro\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"OpenAI's GPT-3.5-Turbo\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"source_node_id\": \"OpenAI's GPT-4-Turbo\",\n",
      "                      \"source_node_type\": \"Topic\",\n",
      "                      \"target_node_id\": \"LLM-based AI agent for software pentesting\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"DISCUSSES\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_Fuxbom4ofXCub4pR6rkHp6O3\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1137,\n",
      "              \"output_tokens\": 813,\n",
      "              \"total_tokens\": 1950\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 813,\n",
      "      \"prompt_tokens\": 1137,\n",
      "      \"total_tokens\": 1950,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [9.13s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [9.13s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Recent efforts to enable visual navigation using large language models have\\nmainly focused on developing complex prompt systems. These systems incorporate\\ninstructions, observations, and history into massive text prompts, which are\\nthen combined with pre-trained large language models to facilitate visual\\nnavigation. In contrast, our approach aims to fine-tune large language models\\nfor visual navigation without extensive prompt engineering. Our design involves\\na simple text prompt, current observations, and a history collector model that\\ngathers information from previous observations as input. For output, our design\\nprovides a probability distribution of possible actions that the agent can take\\nduring navigation. We train our model using human demonstrations and collision\\nsignals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results\\ndemonstrate that our method outperforms state-of-the-art behavior cloning\\nmethods and effectively reduces collision rates.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Recent efforts to enable visual navigation using large language models have\\nmainly focused on developing complex prompt systems. These systems incorporate\\ninstructions, observations, and history into massive text prompts, which are\\nthen combined with pre-trained large language models to facilitate visual\\nnavigation. In contrast, our approach aims to fine-tune large language models\\nfor visual navigation without extensive prompt engineering. Our design involves\\na simple text prompt, current observations, and a history collector model that\\ngathers information from previous observations as input. For output, our design\\nprovides a probability distribution of possible actions that the agent can take\\nduring navigation. We train our model using human demonstrations and collision\\nsignals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results\\ndemonstrate that our method outperforms state-of-the-art behavior cloning\\nmethods and effectively reduces collision rates.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: # Knowledge Graph Instructions for GPT-4\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrificing accuracy. Do not add any information that is not explicitly mentioned in the text.\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **'person'**. Avoid using more specific terms like 'mathematician' or 'scientist'.- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as 'BECAME_PROFESSOR', use more general and timeless relationship types like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\\nIf an entity, such as \\\"John Doe\\\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \\\"Joe\\\", \\\"he\\\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \\\"John Doe\\\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.\\nHuman: Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: Recent efforts to enable visual navigation using large language models have\\nmainly focused on developing complex prompt systems. These systems incorporate\\ninstructions, observations, and history into massive text prompts, which are\\nthen combined with pre-trained large language models to facilitate visual\\nnavigation. In contrast, our approach aims to fine-tune large language models\\nfor visual navigation without extensive prompt engineering. Our design involves\\na simple text prompt, current observations, and a history collector model that\\ngathers information from previous observations as input. For output, our design\\nprovides a probability distribution of possible actions that the agent can take\\nduring navigation. We train our model using human demonstrations and collision\\nsignals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results\\ndemonstrate that our method outperforms state-of-the-art behavior cloning\\nmethods and effectively reduces collision rates.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [3.25s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_rCs0UCH5Dkc1iIvBwvSpKLS3\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"nodes\\\":[{\\\"id\\\":\\\"Recent efforts to enable visual navigation using large language models\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Recent efforts to enable visual navigation using large language models have mainly focused on developing complex prompt systems. These systems incorporate instructions, observations, and history into massive text prompts, which are then combined with pre-trained large language models to facilitate visual navigation.\\\"}]},{\\\"id\\\":\\\"Our approach\\\",\\\"type\\\":\\\"Paper\\\",\\\"properties\\\":[{\\\"key\\\":\\\"summary\\\",\\\"value\\\":\\\"Our approach aims to fine-tune large language models for visual navigation without extensive prompt engineering. Our design involves a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input. For output, our design provides a probability distribution of possible actions that the agent can take during navigation. We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results demonstrate that our method outperforms state-of-the-art behavior cloning methods and effectively reduces collision rates.\\\"}]}],\\\"relationships\\\":[{\\\"source_node_id\\\":\\\"Our approach\\\",\\\"source_node_type\\\":\\\"Paper\\\",\\\"target_node_id\\\":\\\"Recent efforts to enable visual navigation using large language models\\\",\\\"target_node_type\\\":\\\"Paper\\\",\\\"type\\\":\\\"RELATED_TO\\\"}]}\",\n",
      "                    \"name\": \"DynamicGraph\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ],\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 254,\n",
      "                \"prompt_tokens\": 923,\n",
      "                \"total_tokens\": 1177,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-679bb811-6651-4636-8231-9e194ead2481-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"DynamicGraph\",\n",
      "                \"args\": {\n",
      "                  \"nodes\": [\n",
      "                    {\n",
      "                      \"id\": \"Recent efforts to enable visual navigation using large language models\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Recent efforts to enable visual navigation using large language models have mainly focused on developing complex prompt systems. These systems incorporate instructions, observations, and history into massive text prompts, which are then combined with pre-trained large language models to facilitate visual navigation.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"id\": \"Our approach\",\n",
      "                      \"type\": \"Paper\",\n",
      "                      \"properties\": [\n",
      "                        {\n",
      "                          \"key\": \"summary\",\n",
      "                          \"value\": \"Our approach aims to fine-tune large language models for visual navigation without extensive prompt engineering. Our design involves a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input. For output, our design provides a probability distribution of possible actions that the agent can take during navigation. We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results demonstrate that our method outperforms state-of-the-art behavior cloning methods and effectively reduces collision rates.\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ],\n",
      "                  \"relationships\": [\n",
      "                    {\n",
      "                      \"source_node_id\": \"Our approach\",\n",
      "                      \"source_node_type\": \"Paper\",\n",
      "                      \"target_node_id\": \"Recent efforts to enable visual navigation using large language models\",\n",
      "                      \"target_node_type\": \"Paper\",\n",
      "                      \"type\": \"RELATED_TO\"\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                \"id\": \"call_rCs0UCH5Dkc1iIvBwvSpKLS3\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 923,\n",
      "              \"output_tokens\": 254,\n",
      "              \"total_tokens\": 1177\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 254,\n",
      "      \"prompt_tokens\": 923,\n",
      "      \"total_tokens\": 1177,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [3.25s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<parsed,parsing_error>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [3.26s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "Graph documents: 20\n",
      "Nodes from 1st graph doc:[Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper', properties={'title': 'Prompt design and engineering has rapidly become essential for maximizing the potential of large language models'}), Node(id='Core Concepts', type='Topic'), Node(id='Advanced Techniques Like Chain-Of-Thought And Reflection', type='Topic'), Node(id='Principles Behind Building Llm-Based Agents', type='Topic'), Node(id='Survey Of Tools For Prompt Engineers', type='Topic')]\n",
      "Relationships from 1st graph doc:[Relationship(source=Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper'), target=Node(id='Core Concepts', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper'), target=Node(id='Advanced Techniques Like Chain-Of-Thought And Reflection', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper'), target=Node(id='Principles Behind Building Llm-Based Agents', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper'), target=Node(id='Survey Of Tools For Prompt Engineers', type='Topic'), type='DISCUSSES')]\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG Setup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "graph_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "    node_properties=[\"title\", \"summary\", \"url\"],\n",
    "    allowed_relationships=[\"AUTHORED\", \"DISCUSSES\", \"RELATED_TO\"],\n",
    ")\n",
    "\n",
    "graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "\n",
    "graph.add_graph_documents(graph_documents)\n",
    "\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0:\n",
      "  Nodes: [Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper', properties={'title': 'Prompt design and engineering has rapidly become essential for maximizing the potential of large language models'}), Node(id='Core Concepts', type='Topic'), Node(id='Advanced Techniques Like Chain-Of-Thought And Reflection', type='Topic'), Node(id='Principles Behind Building Llm-Based Agents', type='Topic'), Node(id='Survey Of Tools For Prompt Engineers', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper'), target=Node(id='Core Concepts', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper'), target=Node(id='Advanced Techniques Like Chain-Of-Thought And Reflection', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper'), target=Node(id='Principles Behind Building Llm-Based Agents', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Design And Engineering Has Rapidly Become Essential For Maximizing The Potential Of Large Language Models', type='Paper'), target=Node(id='Survey Of Tools For Prompt Engineers', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 1:\n",
      "  Nodes: [Node(id='Two Ways To Unlock The Reasoning Capability Of A Large Language Model', type='Paper', properties={'title': 'Two ways to unlock the reasoning capability of a large language model'}), Node(id='Prompt Engineering', type='Topic'), Node(id='Multi-Agent Discussion', type='Topic'), Node(id='Scalable Discussion Mechanism Based On Conquer And Merge', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='Two Ways To Unlock The Reasoning Capability Of A Large Language Model', type='Paper'), target=Node(id='Prompt Engineering', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Two Ways To Unlock The Reasoning Capability Of A Large Language Model', type='Paper'), target=Node(id='Multi-Agent Discussion', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Two Ways To Unlock The Reasoning Capability Of A Large Language Model', type='Paper'), target=Node(id='Scalable Discussion Mechanism Based On Conquer And Merge', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 2:\n",
      "  Nodes: [Node(id='The Final Frontier For Simulation', type='Paper', properties={'title': 'The final frontier for simulation'}), Node(id='Agent-Based Modeling', type='Topic'), Node(id='Large Language Models', type='Topic'), Node(id='Chatgpt', type='Topic'), Node(id='Our Research', type='Paper', properties={'title': 'Our research', 'summary': 'Investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), presents two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.'}), Node(id='Park Et Al. (2023)', type='Paper')]\n",
      "  Relationships: [Relationship(source=Node(id='The Final Frontier For Simulation', type='Paper'), target=Node(id='Agent-Based Modeling', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='The Final Frontier For Simulation', type='Paper'), target=Node(id='Large Language Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Large Language Models', type='Topic'), target=Node(id='Chatgpt', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Our Research', type='Paper'), target=Node(id='Large Language Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Our Research', type='Paper'), target=Node(id='Park Et Al. (2023)', type='Paper'), type='RELATED_TO')]\n",
      "---\n",
      "Document 3:\n",
      "  Nodes: [Node(id='Ai Community', type='Topic'), Node(id='Artificial General Intelligence', type='Topic'), Node(id='Language Agents', type='Topic'), Node(id='Large Language Models', type='Topic'), Node(id='Agent Symbolic Learning', type='Topic'), Node(id='Paper On Agent Symbolic Learning', type='Paper', properties={'title': 'Agent Symbolic Learning for Self-Evolving Language Agents', 'summary': 'A systematic framework that enables language agents to optimize themselves in a data-centric way using symbolic optimizers, mimicking back-propagation and gradient descent with natural language simulacrums of weights, loss, and gradients.'}), Node(id='Self-Evolving Agents', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='Ai Community', type='Topic'), target=Node(id='Artificial General Intelligence', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Language Agents', type='Topic'), target=Node(id='Large Language Models', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Language Agents', type='Topic'), target=Node(id='Artificial General Intelligence', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Agent Symbolic Learning', type='Topic'), target=Node(id='Language Agents', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Paper On Agent Symbolic Learning', type='Paper'), target=Node(id='Agent Symbolic Learning', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Self-Evolving Agents', type='Topic'), target=Node(id='Agent Symbolic Learning', type='Topic'), type='RELATED_TO')]\n",
      "---\n",
      "Document 4:\n",
      "  Nodes: [Node(id='Reprompt', type='Paper', properties={'title': 'RePrompt: Gradient Descent for Optimizing Prompts in LLM Agents'}), Node(id='Large Language Models', type='Topic'), Node(id='Code Generation', type='Topic'), Node(id='Travel Planning', type='Topic'), Node(id='Robot Controls', type='Topic'), Node(id='Llm Agents', type='Topic'), Node(id='Automatic Prompt Engineering', type='Topic'), Node(id='Pddl Generation', type='Topic'), Node(id='Reasoning Tasks', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='Reprompt', type='Paper'), target=Node(id='Large Language Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Reprompt', type='Paper'), target=Node(id='Code Generation', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Reprompt', type='Paper'), target=Node(id='Travel Planning', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Reprompt', type='Paper'), target=Node(id='Robot Controls', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Reprompt', type='Paper'), target=Node(id='Llm Agents', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Reprompt', type='Paper'), target=Node(id='Automatic Prompt Engineering', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Reprompt', type='Paper'), target=Node(id='Pddl Generation', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Reprompt', type='Paper'), target=Node(id='Reasoning Tasks', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 5:\n",
      "  Nodes: [Node(id='Traditional Base Station Siting Methods', type='Topic'), Node(id='Drive Testing', type='Topic'), Node(id='User Feedback', type='Topic'), Node(id='Large Language Models', type='Topic'), Node(id='Prompt Engineering', type='Topic'), Node(id='Agent Engineering', type='Topic'), Node(id='Network Optimization', type='Topic'), Node(id='Well-Crafted Prompts', type='Topic'), Node(id='Autonomous Agents', type='Topic'), Node(id='Machine Language Based Llms', type='Topic'), Node(id='Human Users', type='Topic'), Node(id='Natural Language', type='Topic'), Node(id='Artificial Intelligence As A Service', type='Topic'), Node(id='Ai For More Ease', type='Topic'), Node(id='Llm-Empowered Bss Optimization Framework', type='Topic'), Node(id='Prompt-Optimized Llm', type='Topic'), Node(id='Human-In-The-Loop Llm', type='Topic'), Node(id='Llm-Empowered Autonomous Bss Agent', type='Topic'), Node(id='Cooperative Multiple Llm-Based Autonomous Bss Agents', type='Topic'), Node(id='Prompt-Assisted Llms', type='Topic'), Node(id='Llm-Based Agents', type='Topic'), Node(id='More Efficient Network Deployments', type='Topic'), Node(id='Cost-Effective Network Deployments', type='Topic'), Node(id='Reliable Network Deployments', type='Topic'), Node(id='Bss Optimization', type='Topic'), Node(id='Trivial Manual Participation', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='Traditional Base Station Siting Methods', type='Topic'), target=Node(id='Drive Testing', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Traditional Base Station Siting Methods', type='Topic'), target=Node(id='User Feedback', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Large Language Models', type='Topic'), target=Node(id='Prompt Engineering', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Large Language Models', type='Topic'), target=Node(id='Agent Engineering', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Large Language Models', type='Topic'), target=Node(id='Network Optimization', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Well-Crafted Prompts', type='Topic'), target=Node(id='Large Language Models', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Autonomous Agents', type='Topic'), target=Node(id='Large Language Models', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Machine Language Based Llms', type='Topic'), target=Node(id='Human Users', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Machine Language Based Llms', type='Topic'), target=Node(id='Natural Language', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Artificial Intelligence As A Service', type='Topic'), target=Node(id='Ai For More Ease', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Llm-Empowered Bss Optimization Framework', type='Topic'), target=Node(id='Prompt-Optimized Llm', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Llm-Empowered Bss Optimization Framework', type='Topic'), target=Node(id='Human-In-The-Loop Llm', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Llm-Empowered Bss Optimization Framework', type='Topic'), target=Node(id='Llm-Empowered Autonomous Bss Agent', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Llm-Empowered Bss Optimization Framework', type='Topic'), target=Node(id='Cooperative Multiple Llm-Based Autonomous Bss Agents', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Prompt-Assisted Llms', type='Topic'), target=Node(id='More Efficient Network Deployments', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Prompt-Assisted Llms', type='Topic'), target=Node(id='Cost-Effective Network Deployments', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Prompt-Assisted Llms', type='Topic'), target=Node(id='Reliable Network Deployments', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Llm-Based Agents', type='Topic'), target=Node(id='More Efficient Network Deployments', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Llm-Based Agents', type='Topic'), target=Node(id='Cost-Effective Network Deployments', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Llm-Based Agents', type='Topic'), target=Node(id='Reliable Network Deployments', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='More Efficient Network Deployments', type='Topic'), target=Node(id='Bss Optimization', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Cost-Effective Network Deployments', type='Topic'), target=Node(id='Bss Optimization', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Reliable Network Deployments', type='Topic'), target=Node(id='Bss Optimization', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Bss Optimization', type='Topic'), target=Node(id='Trivial Manual Participation', type='Topic'), type='RELATED_TO')]\n",
      "---\n",
      "Document 6:\n",
      "  Nodes: [Node(id='Instruction-Following Agents Must Ground Language Into Their Observation And Action Spaces', type='Paper', properties={'title': 'Instruction-following agents must ground language into their observation and action spaces'}), Node(id='Learning To Ground Language Is Challenging, Typically Requiring Domain-Specific Engineering Or Large Quantities Of Human Interaction Data', type='Paper', properties={'summary': 'Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data'}), Node(id='We Propose Using Pretrained Vision-Language Models (Vlms) To Supervise Embodied Agents', type='Paper', properties={'summary': 'We propose using pretrained vision-language models (VLMs) to supervise embodied agents'}), Node(id=\"We Combine Ideas From Model Distillation And Hindsight Experience Replay (Her), Using A Vlm To Retroactively Generate Language Describing The Agent'S Behavior\", type='Paper', properties={'summary': \"We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior\"}), Node(id='Simple Prompting Allows Us To Control The Supervision Signal, Teaching An Agent To Interact With Novel Objects Based On Their Names (E.G., Planes) Or Their Features (E.G., Colors) In A 3D Rendered Environment', type='Paper', properties={'summary': 'Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment'}), Node(id='Fewshot Prompting Lets Us Teach Abstract Category Membership, Including Pre-Existing Categories (Food Vs Toys) And Ad-Hoc Ones (Arbitrary Preferences Over Objects)', type='Paper', properties={'summary': 'Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects)'}), Node(id='Our Work Outlines A New And Effective Way To Use Internet-Scale Vlms, Repurposing The Generic Language Grounding Acquired By Such Models To Teach Task-Relevant Groundings To Embodied Agents', type='Paper', properties={'summary': 'Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents'})]\n",
      "  Relationships: [Relationship(source=Node(id='Instruction-Following Agents Must Ground Language Into Their Observation And Action Spaces', type='Paper'), target=Node(id='Learning To Ground Language Is Challenging, Typically Requiring Domain-Specific Engineering Or Large Quantities Of Human Interaction Data', type='Paper'), type='RELATED_TO'), Relationship(source=Node(id='Instruction-Following Agents Must Ground Language Into Their Observation And Action Spaces', type='Paper'), target=Node(id='We Propose Using Pretrained Vision-Language Models (Vlms) To Supervise Embodied Agents', type='Paper'), type='RELATED_TO'), Relationship(source=Node(id='Instruction-Following Agents Must Ground Language Into Their Observation And Action Spaces', type='Paper'), target=Node(id=\"We Combine Ideas From Model Distillation And Hindsight Experience Replay (Her), Using A Vlm To Retroactively Generate Language Describing The Agent'S Behavior\", type='Paper'), type='RELATED_TO'), Relationship(source=Node(id='Instruction-Following Agents Must Ground Language Into Their Observation And Action Spaces', type='Paper'), target=Node(id='Simple Prompting Allows Us To Control The Supervision Signal, Teaching An Agent To Interact With Novel Objects Based On Their Names (E.G., Planes) Or Their Features (E.G., Colors) In A 3D Rendered Environment', type='Paper'), type='RELATED_TO'), Relationship(source=Node(id='Instruction-Following Agents Must Ground Language Into Their Observation And Action Spaces', type='Paper'), target=Node(id='Fewshot Prompting Lets Us Teach Abstract Category Membership, Including Pre-Existing Categories (Food Vs Toys) And Ad-Hoc Ones (Arbitrary Preferences Over Objects)', type='Paper'), type='RELATED_TO'), Relationship(source=Node(id='Instruction-Following Agents Must Ground Language Into Their Observation And Action Spaces', type='Paper'), target=Node(id='Our Work Outlines A New And Effective Way To Use Internet-Scale Vlms, Repurposing The Generic Language Grounding Acquired By Such Models To Teach Task-Relevant Groundings To Embodied Agents', type='Paper'), type='RELATED_TO')]\n",
      "---\n",
      "Document 7:\n",
      "  Nodes: [Node(id='Helper', type='Paper', properties={'title': 'HELPER: An Embodied Agent with External Memory for Parsing Human-Robot Dialogue into Action Programs', 'summary': 'HELPER is an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting. It sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD).', 'url': 'https://helper-agent-llm.github.io'}), Node(id='Teach', type='Topic'), Node(id='Execution From Dialog History (Edh)', type='Topic'), Node(id='Trajectory From Dialogue (Tfd)', type='Topic'), Node(id='Pre-Trained And Frozen Large Language Models (Llms)', type='Topic'), Node(id=\"Robot'S Visuomotor Functions\", type='Topic'), Node(id='Retrieval-Augmented Llm Prompting', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='Helper', type='Paper'), target=Node(id='Teach', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Helper', type='Paper'), target=Node(id='Execution From Dialog History (Edh)', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Helper', type='Paper'), target=Node(id='Trajectory From Dialogue (Tfd)', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Helper', type='Paper'), target=Node(id='Pre-Trained And Frozen Large Language Models (Llms)', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Helper', type='Paper'), target=Node(id=\"Robot'S Visuomotor Functions\", type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Helper', type='Paper'), target=Node(id='Retrieval-Augmented Llm Prompting', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 8:\n",
      "  Nodes: [Node(id='Promptagent', type='Paper', properties={'title': 'PromptAgent: Autonomous Expert-Level Prompt Optimization', 'summary': 'PromptAgent is an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. It views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to navigate the expert-level prompt space. PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. It significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines across 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-specific, and general NLP tasks.'})]\n",
      "  Relationships: [Relationship(source=Node(id='Promptagent', type='Paper'), target=Node(id='Big-Bench Hard (Bbh)', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptagent', type='Paper'), target=Node(id='Domain-Specific Tasks', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptagent', type='Paper'), target=Node(id='General Nlp Tasks', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 9:\n",
      "  Nodes: [Node(id='Promptwizard', type='Paper', properties={'title': 'PromptWizard: A Novel Framework for Prompt Optimization', 'summary': 'PromptWizard leverages LLMs to iteratively synthesize and refine prompts tailored to specific tasks, optimizing both prompt instructions and in-context examples to maximize model performance. It incorporates negative examples and a critic to enhance instructions and examples with detailed reasoning steps, offering computational efficiency, adaptability, and effectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8 datasets demonstrates its superiority over existing prompt strategies.'})]\n",
      "  Relationships: [Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Large Language Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Prompting', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Prompt Engineering', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Automated Solutions', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Prompt Instructions', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='In-Context Examples', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Model Performance', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Negative Examples', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Detailed Reasoning Steps', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Computational Efficiency', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Training Data', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Smaller Llms', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptwizard', type='Paper'), target=Node(id='Prompt Optimization', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 10:\n",
      "  Nodes: [Node(id='Recent Advancements In Large Language Models (Llms) And Prompt Engineering', type='Topic'), Node(id='Chatbot Customization', type='Topic'), Node(id='Prompt Evaluation', type='Topic'), Node(id='Dataset Scale', type='Topic'), Node(id='Our Study', type='Paper', properties={'summary': 'Based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation and introduced a feature-oriented workflow for systematic prompt evaluation.'}), Node(id='Text Summarization', type='Topic'), Node(id='Summary Characteristics', type='Topic'), Node(id='Awesum', type='Topic'), Node(id='Visual Analytics System', type='Topic'), Node(id='Prompt Comparator', type='Topic'), Node(id='Bubbleset-Inspired Design', type='Topic'), Node(id='Dimensionality Reduction Techniques', type='Topic'), Node(id='Practitioners From Various Domains', type='Topic'), Node(id='Non-Technical People', type='Topic'), Node(id='Feature-Oriented Workflow', type='Topic'), Node(id='Nlg', type='Topic'), Node(id='Image-Generation Tasks', type='Topic'), Node(id='Human-Agent Interaction', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='Recent Advancements In Large Language Models (Llms) And Prompt Engineering', type='Topic'), target=Node(id='Chatbot Customization', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Prompt Evaluation', type='Topic'), target=Node(id='Dataset Scale', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Our Study', type='Paper'), target=Node(id='Prompt Evaluation', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Our Study', type='Paper'), target=Node(id='Feature-Oriented Workflow', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Feature-Oriented Workflow', type='Topic'), target=Node(id='Text Summarization', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Text Summarization', type='Topic'), target=Node(id='Summary Characteristics', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Awesum', type='Topic'), target=Node(id='Visual Analytics System', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Visual Analytics System', type='Topic'), target=Node(id='Prompt Comparator', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Prompt Comparator', type='Topic'), target=Node(id='Bubbleset-Inspired Design', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Bubbleset-Inspired Design', type='Topic'), target=Node(id='Dimensionality Reduction Techniques', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Our Study', type='Paper'), target=Node(id='Practitioners From Various Domains', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Our Study', type='Paper'), target=Node(id='Non-Technical People', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Feature-Oriented Workflow', type='Topic'), target=Node(id='Nlg', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Feature-Oriented Workflow', type='Topic'), target=Node(id='Image-Generation Tasks', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Our Study', type='Paper'), target=Node(id='Human-Agent Interaction', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 11:\n",
      "  Nodes: [Node(id='Drama Engine', type='Paper', properties={'title': 'Drama Engine', 'summary': \"This technical report presents the Drama Engine, a novel framework for agentic interaction with large language models designed for narrative purposes. The framework adapts multi-agent system principles to create dynamic, context-aware companions that can develop over time and interact with users and each other. Key features include multi-agent workflows with delegation, dynamic prompt assembly, and model-agnostic design. The Drama Engine introduces unique elements such as companion development, mood systems, and automatic context summarising. It is implemented in TypeScript. The framework's applications include multi-agent chats and virtual co-workers for creative writing. The paper discusses the system's architecture, prompt assembly process, delegation mechanisms, and moderation techniques, as well as potential ethical considerations and future extensions.\"})]\n",
      "  Relationships: [Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Multi-Agent Workflows', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Dynamic Prompt Assembly', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Model-Agnostic Design', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Companion Development', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Mood Systems', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Automatic Context Summarising', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Multi-Agent Chats', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Virtual Co-Workers For Creative Writing', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id=\"System'S Architecture\", type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Prompt Assembly Process', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Delegation Mechanisms', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Moderation Techniques', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Ethical Considerations', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Drama Engine', type='Paper'), target=Node(id='Future Extensions', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 12:\n",
      "  Nodes: [Node(id='Prompt Engineering', type='Topic', properties={'summary': 'A technique that involves augmenting a large pre-trained model with task-specific hints, known as prompts, to adapt the model to new tasks.'}), Node(id='Natural Language Processing', type='Topic'), Node(id='Vision-Language Modeling', type='Topic'), Node(id='Multimodal-To-Text Generation Models', type='Topic'), Node(id='Flamingo', type='Topic'), Node(id='Image-Text Matching Models', type='Topic'), Node(id='Clip', type='Topic'), Node(id='Text-To-Image Generation Models', type='Topic'), Node(id='Stable Diffusion', type='Topic'), Node(id='This Paper', type='Paper', properties={'title': 'A comprehensive survey of cutting-edge research in prompt engineering on vision-language models'}), Node(id='Prompting Methods', type='Topic'), Node(id='Prompting-Based Applications', type='Topic'), Node(id='Responsibility And Integrity Issues', type='Topic'), Node(id='Commonalities And Differences Between Prompting On Vision-Language Models, Language Models, And Vision Models', type='Topic'), Node(id='Challenges, Future Directions, And Research Opportunities', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='Prompt Engineering', type='Topic'), target=Node(id='Natural Language Processing', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Engineering', type='Topic'), target=Node(id='Vision-Language Modeling', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Prompt Engineering', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Multimodal-To-Text Generation Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Flamingo', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Image-Text Matching Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Clip', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Text-To-Image Generation Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Stable Diffusion', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Prompting Methods', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Prompting-Based Applications', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Responsibility And Integrity Issues', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Commonalities And Differences Between Prompting On Vision-Language Models, Language Models, And Vision Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='This Paper', type='Paper'), target=Node(id='Challenges, Future Directions, And Research Opportunities', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 13:\n",
      "  Nodes: [Node(id='Prompt Optimization', type='Topic'), Node(id='Large Language Model', type='Topic'), Node(id='Promst', type='Topic', properties={'summary': 'A new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement.', 'url': 'https://github.com/yongchao98/PROMST'}), Node(id='Paper On Promst', type='Paper', properties={'title': 'PROMST: A New LLM-driven Discrete Prompt Optimization Framework', 'summary': 'This paper introduces PROMST, a framework that uses human-designed feedback rules and a learned heuristic model to optimize prompts for multi-step tasks, significantly outperforming other methods.', 'url': 'https://yongchao98.github.io/MIT-REALM-PROMST/'})]\n",
      "  Relationships: [Relationship(source=Node(id='Prompt Optimization', type='Topic'), target=Node(id='Large Language Model', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Promst', type='Topic'), target=Node(id='Large Language Model', type='Topic'), type='RELATED_TO'), Relationship(source=Node(id='Paper On Promst', type='Paper'), target=Node(id='Promst', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 14:\n",
      "  Nodes: [Node(id='Prompt Stealing Attacks', type='Paper', properties={'title': 'Prompt Stealing Attacks', 'summary': 'The paper proposes a novel attack against LLMs, named prompt stealing attacks, which aims to steal well-designed prompts based on generated answers. The attack contains two primary modules: the parameter extractor and the prompt reconstruction.'}), Node(id='Prompt Engineering', type='Topic'), Node(id='Large Language Models', type='Topic'), Node(id='Parameter Extractor', type='Topic'), Node(id='Prompt Reconstruction', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='Prompt Stealing Attacks', type='Paper'), target=Node(id='Prompt Engineering', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Stealing Attacks', type='Paper'), target=Node(id='Large Language Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Stealing Attacks', type='Paper'), target=Node(id='Parameter Extractor', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Stealing Attacks', type='Paper'), target=Node(id='Prompt Reconstruction', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 15:\n",
      "  Nodes: [Node(id='Recent Trends In Llms As Autonomous Agents', type='Paper', properties={'title': 'Recent trends in LLMs as autonomous agents'}), Node(id='Guidance, Navigation, And Control In Space', type='Topic'), Node(id='Kerbal Space Program Differential Games (Kspdg) Challenge', type='Topic'), Node(id='Autonomous Satellite Operations', type='Topic'), Node(id='Prompt Engineering', type='Topic'), Node(id='Few-Shot Prompting', type='Topic'), Node(id='Fine-Tuning Techniques', type='Topic'), Node(id='Https://Github.Com/Arclab-Mit/Kspdg', type='Paper', properties={'url': 'https://github.com/ARCLab-MIT/kspdg'})]\n",
      "  Relationships: [Relationship(source=Node(id='Recent Trends In Llms As Autonomous Agents', type='Paper'), target=Node(id='Guidance, Navigation, And Control In Space', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Recent Trends In Llms As Autonomous Agents', type='Paper'), target=Node(id='Kerbal Space Program Differential Games (Kspdg) Challenge', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Recent Trends In Llms As Autonomous Agents', type='Paper'), target=Node(id='Autonomous Satellite Operations', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Recent Trends In Llms As Autonomous Agents', type='Paper'), target=Node(id='Prompt Engineering', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Recent Trends In Llms As Autonomous Agents', type='Paper'), target=Node(id='Few-Shot Prompting', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Recent Trends In Llms As Autonomous Agents', type='Paper'), target=Node(id='Fine-Tuning Techniques', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Recent Trends In Llms As Autonomous Agents', type='Paper'), target=Node(id='Https://Github.Com/Arclab-Mit/Kspdg', type='Paper'), type='RELATED_TO')]\n",
      "---\n",
      "Document 16:\n",
      "  Nodes: [Node(id='The Rise Of Capabilities Expressed By Large Language Models', type='Paper', properties={'title': 'The rise of capabilities expressed by large language models'}), Node(id='Promptset', type='Paper', properties={'title': 'PromptSet'}), Node(id='Large Language Models', type='Topic'), Node(id='Structured Prompting', type='Topic'), Node(id='Agent Mode', type='Topic'), Node(id='Static Testing And Linting', type='Topic'), Node(id='Developer Prompts', type='Topic'), Node(id='Static Linter For Prompts', type='Topic'), Node(id='Huggingface Dataset', type='Topic'), Node(id='Github Repository', type='Topic')]\n",
      "  Relationships: [Relationship(source=Node(id='The Rise Of Capabilities Expressed By Large Language Models', type='Paper'), target=Node(id='Large Language Models', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='The Rise Of Capabilities Expressed By Large Language Models', type='Paper'), target=Node(id='Structured Prompting', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='The Rise Of Capabilities Expressed By Large Language Models', type='Paper'), target=Node(id='Agent Mode', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='The Rise Of Capabilities Expressed By Large Language Models', type='Paper'), target=Node(id='Static Testing And Linting', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptset', type='Paper'), target=Node(id='Developer Prompts', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptset', type='Paper'), target=Node(id='Static Linter For Prompts', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptset', type='Paper'), target=Node(id='Huggingface Dataset', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Promptset', type='Paper'), target=Node(id='Github Repository', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 17:\n",
      "  Nodes: [Node(id='Interaction With Large Language Models (Llms)', type='Paper', properties={'title': 'Interaction with Large Language Models (LLMs)', 'summary': 'Interaction with Large Language Models (LLMs) is primarily carried out via prompting. A prompt is a natural language instruction designed to elicit certain behaviour or output from a model. In theory, natural language prompts enable non-experts to interact with and leverage LLMs. However, for complex tasks and tasks with specific requirements, prompt design is not trivial. Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal. We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering. To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made. We discuss design implications and future directions based on these prompt engineering practices.'})]\n",
      "  Relationships: [Relationship(source=Node(id='Interaction With Large Language Models (Llms)', type='Paper'), target=Node(id='Prompting', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Interaction With Large Language Models (Llms)', type='Paper'), target=Node(id='Prompt Design', type='Topic'), type='DISCUSSES'), Relationship(source=Node(id='Interaction With Large Language Models (Llms)', type='Paper'), target=Node(id='Prompt Engineering', type='Topic'), type='DISCUSSES')]\n",
      "---\n",
      "Document 18:\n",
      "  Nodes: [Node(id='Large Language Models', type='Topic'), Node(id='Security Tasks', type='Topic'), Node(id='Security Operation Centers', type='Topic'), Node(id='Software Pentesting', type='Topic'), Node(id='Software Security Vulnerabilities', type='Topic'), Node(id='Source Code', type='Topic'), Node(id='Ai Agent', type='Topic'), Node(id='Human Operators', type='Topic'), Node(id='Prompt Engineering', type='Topic'), Node(id='Owasp Benchmark Project 1.2', type='Topic'), Node(id='Sonarqube', type='Topic'), Node(id=\"Google'S Gemini-Pro\", type='Topic'), Node(id=\"Openai'S Gpt-3.5-Turbo\", type='Topic'), Node(id=\"Openai'S Gpt-4-Turbo\", type='Topic'), Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper', properties={'title': 'LLM-based AI agent for software pentesting', 'summary': 'Investigates the use of LLMs in software pentesting to identify software security vulnerabilities in source code, and evaluates the improvement of an AI agent through prompt engineering.'})]\n",
      "  Relationships: [Relationship(source=Node(id='Large Language Models', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Security Tasks', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Security Operation Centers', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Software Pentesting', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Software Security Vulnerabilities', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Source Code', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Ai Agent', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Human Operators', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Prompt Engineering', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Owasp Benchmark Project 1.2', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id='Sonarqube', type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id=\"Google'S Gemini-Pro\", type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id=\"Openai'S Gpt-3.5-Turbo\", type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES'), Relationship(source=Node(id=\"Openai'S Gpt-4-Turbo\", type='Topic'), target=Node(id='Llm-Based Ai Agent For Software Pentesting', type='Paper'), type='DISCUSSES')]\n",
      "---\n",
      "Document 19:\n",
      "  Nodes: [Node(id='Recent Efforts To Enable Visual Navigation Using Large Language Models', type='Paper', properties={'summary': 'Recent efforts to enable visual navigation using large language models have mainly focused on developing complex prompt systems. These systems incorporate instructions, observations, and history into massive text prompts, which are then combined with pre-trained large language models to facilitate visual navigation.'}), Node(id='Our Approach', type='Paper', properties={'summary': 'Our approach aims to fine-tune large language models for visual navigation without extensive prompt engineering. Our design involves a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input. For output, our design provides a probability distribution of possible actions that the agent can take during navigation. We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results demonstrate that our method outperforms state-of-the-art behavior cloning methods and effectively reduces collision rates.'})]\n",
      "  Relationships: [Relationship(source=Node(id='Our Approach', type='Paper'), target=Node(id='Recent Efforts To Enable Visual Navigation Using Large Language Models', type='Paper'), type='RELATED_TO')]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# After converting to graph documents\n",
    "for i, doc in enumerate(graph_documents):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"  Nodes: {doc.nodes}\")\n",
    "    print(f\"  Relationships: {doc.relationships}\")\n",
    "    # print(f\"  Source document: {doc.source_document}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Do we have articles that talk about Prompt Engineering? \",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Do we have articles that talk about Prompt Engineering? \",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\\n    \\n    Here is the user question: \\n    Do we have articles that talk about Prompt Engineering?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [9.82s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T07:45:02.455022Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 9815336792,\n",
      "          \"load_duration\": 7858905417,\n",
      "          \"prompt_eval_count\": 429,\n",
      "          \"prompt_eval_duration\": 1749011000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 204740000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T07:45:02.455022Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 9815336792,\n",
      "              \"load_duration\": 7858905417,\n",
      "              \"prompt_eval_count\": 429,\n",
      "              \"prompt_eval_duration\": 1749011000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 204740000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-87d154df-5713-4586-8c9f-ba1ae5e36858-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [9.83s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "    \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     \n",
    "    Here is the retrieved document: \n",
    "    {document}\n",
    "    \n",
    "    Here is the user question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"Do we have articles that talk about Prompt Engineering?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Do we have articles that talk about Prompt Engineering? \\n    Context: [Document(metadata={'pk': 452500211585777757, 'summary': \\\"Prompt engineering is critical for effective interaction with large language\\\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\\\nhave been limited. This study designed and implemented a prompt engineering\\\\nintervention, examining its influence on undergraduate students' AI\\\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\\\nintervention involved 27 students who participated in a 100-minute workshop\\\\nconducted during their history course at a university in Hong Kong. During the\\\\nworkshop, students were introduced to prompt engineering strategies, which they\\\\napplied to plan the course's final essay task. Multiple data sources were\\\\ncollected, including students' responses to pre- and post-workshop\\\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\\\nreflections. The study's findings revealed that students demonstrated a higher\\\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\\\nimproved prompt engineering skills because of the intervention. These findings\\\\nhave implications for AI literacy education, as they highlight the importance\\\\nof prompt engineering training for specific higher education use cases. This is\\\\na significant shift from students haphazardly and intuitively learning to\\\\nengineer prompts. Through prompt engineering education, educators can faciitate\\\\nstudents' effective navigation and leverage of LLMs to support their\\\\ncoursework.\\\", 'title': \\\"Effects of a Prompt Engineering Intervention on Undergraduate Students' AI Self-Efficacy, AI Knowledge and Prompt Engineering Ability: A Mixed Methods Study\\\", 'url': 'http://arxiv.org/abs/2408.07302v1'}, page_content=\\\"Prompt engineering is critical for effective interaction with large language\\\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\\\nhave been limited. This study designed and implemented a prompt engineering\\\\nintervention, examining its influence on undergraduate students' AI\\\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\\\nintervention involved 27 students who participated in a 100-minute workshop\\\\nconducted during their history course at a university in Hong Kong. During the\\\\nworkshop, students were introduced to prompt engineering strategies, which they\\\\napplied to plan the course's final essay task. Multiple data sources were\\\\ncollected, including students' responses to pre- and post-workshop\\\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\\\nreflections. The study's findings revealed that students demonstrated a higher\\\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\\\nimproved prompt engineering skills because of the intervention. These findings\\\\nhave implications for AI literacy education, as they highlight the importance\\\\nof prompt engineering training for specific higher education use cases. This is\\\\na significant shift from students haphazardly and intuitively learning to\\\\nengineer prompts. Through prompt engineering education, educators can faciitate\\\\nstudents' effective navigation and leverage of LLMs to support their\\\\ncoursework.\\\"), Document(metadata={'pk': 452500478732271677, 'summary': \\\"Prompt engineering is critical for effective interaction with large language\\\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\\\nhave been limited. This study designed and implemented a prompt engineering\\\\nintervention, examining its influence on undergraduate students' AI\\\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\\\nintervention involved 27 students who participated in a 100-minute workshop\\\\nconducted during their history course at a university in Hong Kong. During the\\\\nworkshop, students were introduced to prompt engineering strategies, which they\\\\napplied to plan the course's final essay task. Multiple data sources were\\\\ncollected, including students' responses to pre- and post-workshop\\\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\\\nreflections. The study's findings revealed that students demonstrated a higher\\\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\\\nimproved prompt engineering skills because of the intervention. These findings\\\\nhave implications for AI literacy education, as they highlight the importance\\\\nof prompt engineering training for specific higher education use cases. This is\\\\na significant shift from students haphazardly and intuitively learning to\\\\nengineer prompts. Through prompt engineering education, educators can faciitate\\\\nstudents' effective navigation and leverage of LLMs to support their\\\\ncoursework.\\\", 'title': \\\"Effects of a Prompt Engineering Intervention on Undergraduate Students' AI Self-Efficacy, AI Knowledge and Prompt Engineering Ability: A Mixed Methods Study\\\", 'url': 'http://arxiv.org/abs/2408.07302v1'}, page_content=\\\"Prompt engineering is critical for effective interaction with large language\\\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\\\nhave been limited. This study designed and implemented a prompt engineering\\\\nintervention, examining its influence on undergraduate students' AI\\\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\\\nintervention involved 27 students who participated in a 100-minute workshop\\\\nconducted during their history course at a university in Hong Kong. During the\\\\nworkshop, students were introduced to prompt engineering strategies, which they\\\\napplied to plan the course's final essay task. Multiple data sources were\\\\ncollected, including students' responses to pre- and post-workshop\\\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\\\nreflections. The study's findings revealed that students demonstrated a higher\\\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\\\nimproved prompt engineering skills because of the intervention. These findings\\\\nhave implications for AI literacy education, as they highlight the importance\\\\nof prompt engineering training for specific higher education use cases. This is\\\\na significant shift from students haphazardly and intuitively learning to\\\\nengineer prompts. Through prompt engineering education, educators can faciitate\\\\nstudents' effective navigation and leverage of LLMs to support their\\\\ncoursework.\\\"), Document(metadata={'pk': 452500826504036413, 'summary': \\\"Prompt engineering is critical for effective interaction with large language\\\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\\\nhave been limited. This study designed and implemented a prompt engineering\\\\nintervention, examining its influence on undergraduate students' AI\\\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\\\nintervention involved 27 students who participated in a 100-minute workshop\\\\nconducted during their history course at a university in Hong Kong. During the\\\\nworkshop, students were introduced to prompt engineering strategies, which they\\\\napplied to plan the course's final essay task. Multiple data sources were\\\\ncollected, including students' responses to pre- and post-workshop\\\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\\\nreflections. The study's findings revealed that students demonstrated a higher\\\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\\\nimproved prompt engineering skills because of the intervention. These findings\\\\nhave implications for AI literacy education, as they highlight the importance\\\\nof prompt engineering training for specific higher education use cases. This is\\\\na significant shift from students haphazardly and intuitively learning to\\\\nengineer prompts. Through prompt engineering education, educators can faciitate\\\\nstudents' effective navigation and leverage of LLMs to support their\\\\ncoursework.\\\", 'title': \\\"Effects of a Prompt Engineering Intervention on Undergraduate Students' AI Self-Efficacy, AI Knowledge and Prompt Engineering Ability: A Mixed Methods Study\\\", 'url': 'http://arxiv.org/abs/2408.07302v1'}, page_content=\\\"Prompt engineering is critical for effective interaction with large language\\\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\\\nhave been limited. This study designed and implemented a prompt engineering\\\\nintervention, examining its influence on undergraduate students' AI\\\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\\\nintervention involved 27 students who participated in a 100-minute workshop\\\\nconducted during their history course at a university in Hong Kong. During the\\\\nworkshop, students were introduced to prompt engineering strategies, which they\\\\napplied to plan the course's final essay task. Multiple data sources were\\\\ncollected, including students' responses to pre- and post-workshop\\\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\\\nreflections. The study's findings revealed that students demonstrated a higher\\\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\\\nimproved prompt engineering skills because of the intervention. These findings\\\\nhave implications for AI literacy education, as they highlight the importance\\\\nof prompt engineering training for specific higher education use cases. This is\\\\na significant shift from students haphazardly and intuitively learning to\\\\nengineer prompts. Through prompt engineering education, educators can faciitate\\\\nstudents' effective navigation and leverage of LLMs to support their\\\\ncoursework.\\\"), Document(metadata={'pk': 452500886655860797, 'summary': \\\"Prompt engineering is critical for effective interaction with large language\\\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\\\nhave been limited. This study designed and implemented a prompt engineering\\\\nintervention, examining its influence on undergraduate students' AI\\\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\\\nintervention involved 27 students who participated in a 100-minute workshop\\\\nconducted during their history course at a university in Hong Kong. During the\\\\nworkshop, students were introduced to prompt engineering strategies, which they\\\\napplied to plan the course's final essay task. Multiple data sources were\\\\ncollected, including students' responses to pre- and post-workshop\\\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\\\nreflections. The study's findings revealed that students demonstrated a higher\\\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\\\nimproved prompt engineering skills because of the intervention. These findings\\\\nhave implications for AI literacy education, as they highlight the importance\\\\nof prompt engineering training for specific higher education use cases. This is\\\\na significant shift from students haphazardly and intuitively learning to\\\\nengineer prompts. Through prompt engineering education, educators can faciitate\\\\nstudents' effective navigation and leverage of LLMs to support their\\\\ncoursework.\\\", 'title': \\\"Effects of a Prompt Engineering Intervention on Undergraduate Students' AI Self-Efficacy, AI Knowledge and Prompt Engineering Ability: A Mixed Methods Study\\\", 'url': 'http://arxiv.org/abs/2408.07302v1'}, page_content=\\\"Prompt engineering is critical for effective interaction with large language\\\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\\\nhave been limited. This study designed and implemented a prompt engineering\\\\nintervention, examining its influence on undergraduate students' AI\\\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\\\nintervention involved 27 students who participated in a 100-minute workshop\\\\nconducted during their history course at a university in Hong Kong. During the\\\\nworkshop, students were introduced to prompt engineering strategies, which they\\\\napplied to plan the course's final essay task. Multiple data sources were\\\\ncollected, including students' responses to pre- and post-workshop\\\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\\\nreflections. The study's findings revealed that students demonstrated a higher\\\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\\\nimproved prompt engineering skills because of the intervention. These findings\\\\nhave implications for AI literacy education, as they highlight the importance\\\\nof prompt engineering training for specific higher education use cases. This is\\\\na significant shift from students haphazardly and intuitively learning to\\\\nengineer prompts. Through prompt engineering education, educators can faciitate\\\\nstudents' effective navigation and leverage of LLMs to support their\\\\ncoursework.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [7.07s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The study found that the prompt engineering intervention had a positive impact on undergraduate students' AI self-efficacy, AI knowledge, and proficiency in creating effective prompts. The findings suggest that prompt engineering education is important for specific higher education use cases and can facilitate students' effective navigation and leverage of large language models (LLMs) to support their coursework.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T07:45:30.379283Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 7068640917,\n",
      "          \"load_duration\": 32755542,\n",
      "          \"prompt_eval_count\": 1026,\n",
      "          \"prompt_eval_duration\": 4288991000,\n",
      "          \"eval_count\": 70,\n",
      "          \"eval_duration\": 2744967000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The study found that the prompt engineering intervention had a positive impact on undergraduate students' AI self-efficacy, AI knowledge, and proficiency in creating effective prompts. The findings suggest that prompt engineering education is important for specific higher education use cases and can facilitate students' effective navigation and leverage of large language models (LLMs) to support their coursework.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T07:45:30.379283Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 7068640917,\n",
      "              \"load_duration\": 32755542,\n",
      "              \"prompt_eval_count\": 1026,\n",
      "              \"prompt_eval_duration\": 4288991000,\n",
      "              \"eval_count\": 70,\n",
      "              \"eval_duration\": 2744967000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a4d2f247-3e6e-43c6-bda5-67b299fcaf39-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The study found that the prompt engineering intervention had a positive impact on undergraduate students' AI self-efficacy, AI knowledge, and proficiency in creating effective prompts. The findings suggest that prompt engineering education is important for specific higher education use cases and can facilitate students' effective navigation and leverage of large language models (LLMs) to support their coursework.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [7.08s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The study found that the prompt engineering intervention had a positive impact on undergraduate students' AI self-efficacy, AI knowledge, and proficiency in creating effective prompts. The findings suggest that prompt engineering education is important for specific higher education use cases and can facilitate students' effective navigation and leverage of large language models (LLMs) to support their coursework.\"\n",
      "}\n",
      "The study found that the prompt engineering intervention had a positive impact on undergraduate students' AI self-efficacy, AI knowledge, and proficiency in creating effective prompts. The findings suggest that prompt engineering education is important for specific higher education use cases and can facilitate students' effective navigation and leverage of large language models (LLMs) to support their coursework.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"Do we have articles that talk about Prompt Engineering?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:GraphCypherQAChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:GraphCypherQAChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"schema\": \"Node properties are the following:\\nConcept {id: STRING, description: STRING, name: STRING},Technique {id: STRING},Agent {id: STRING},Profession {id: STRING},Publication {id: STRING, title: STRING},Technology {id: STRING, description: STRING},Person {id: STRING, description: STRING},Algorithm {id: STRING, field: STRING, description: STRING},Domain {id: STRING, description: STRING},Method {id: STRING, description: STRING},Node {id: STRING, name: STRING},Website {id: STRING},Domain-specific nlp task {id: STRING, description: STRING},Property {id: STRING},Topic {id: STRING, summary: STRING, title: STRING, url: STRING},Paper {id: STRING, title: STRING, summary: STRING, url: STRING},Author {id: STRING, title: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Concept)-[:RELATED_TO]->(:Concept),(:Concept)-[:RELATED_TO]->(:Agent),(:Concept)-[:RELATED_TO]->(:Technology),(:Concept)-[:RELATED_TO]->(:Website),(:Concept)-[:WROTE]->(:Technology),(:Concept)-[:APPLIED]->(:Algorithm),(:Concept)-[:INITIATED]->(:Algorithm),(:Concept)-[:ENHANCE]->(:Concept),(:Concept)-[:PRODUCE]->(:Concept),(:Concept)-[:ASSOCIATED_WITH]->(:Concept),(:Publication)-[:DESCRIBES]->(:Concept),(:Technology)-[:MENTIONS]->(:Technology),(:Technology)-[:RELATED_TO]->(:Domain),(:Technology)-[:RELATED_TO]->(:Concept),(:Technology)-[:RELATED_TO]->(:Algorithm),(:Technology)-[:USED_IN]->(:Method),(:Technology)-[:USED_FOR]->(:Algorithm),(:Person)-[:WROTE]->(:Technology),(:Person)-[:MENTIONS]->(:Concept),(:Person)-[:PROFESSOR]->(:Concept),(:Topic)-[:RELATED_TO]->(:Topic),(:Topic)-[:RELATED_TO]->(:Author),(:Topic)-[:RELATED_TO]->(:Paper),(:Topic)-[:DISCUSSES]->(:Topic),(:Topic)-[:DISCUSSES]->(:Paper),(:Topic)-[:AUTHORED]->(:Author),(:Paper)-[:DISCUSSES]->(:Topic),(:Paper)-[:RELATED_TO]->(:Topic),(:Paper)-[:RELATED_TO]->(:Paper),(:Author)-[:AUTHORED]->(:Paper),(:Author)-[:DISCUSSES]->(:Topic)\",\n",
      "  \"query\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:GraphCypherQAChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at generating Cypher queries for Neo4j.\\n    Use the following schema to generate a Cypher query that answers the given question.\\n    Make the query flexible by using case-insensitive matching and partial string matching where appropriate.\\n    Focus on searching paper titles as they contain the most relevant information.\\n    \\n    Schema:\\n    Node properties are the following:\\nConcept {id: STRING, description: STRING, name: STRING},Technique {id: STRING},Agent {id: STRING},Profession {id: STRING},Publication {id: STRING, title: STRING},Technology {id: STRING, description: STRING},Person {id: STRING, description: STRING},Algorithm {id: STRING, field: STRING, description: STRING},Domain {id: STRING, description: STRING},Method {id: STRING, description: STRING},Node {id: STRING, name: STRING},Website {id: STRING},Domain-specific nlp task {id: STRING, description: STRING},Property {id: STRING},Topic {id: STRING, summary: STRING, title: STRING, url: STRING},Paper {id: STRING, title: STRING, summary: STRING, url: STRING},Author {id: STRING, title: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Concept)-[:RELATED_TO]->(:Concept),(:Concept)-[:RELATED_TO]->(:Agent),(:Concept)-[:RELATED_TO]->(:Technology),(:Concept)-[:RELATED_TO]->(:Website),(:Concept)-[:WROTE]->(:Technology),(:Concept)-[:APPLIED]->(:Algorithm),(:Concept)-[:INITIATED]->(:Algorithm),(:Concept)-[:ENHANCE]->(:Concept),(:Concept)-[:PRODUCE]->(:Concept),(:Concept)-[:ASSOCIATED_WITH]->(:Concept),(:Publication)-[:DESCRIBES]->(:Concept),(:Technology)-[:MENTIONS]->(:Technology),(:Technology)-[:RELATED_TO]->(:Domain),(:Technology)-[:RELATED_TO]->(:Concept),(:Technology)-[:RELATED_TO]->(:Algorithm),(:Technology)-[:USED_IN]->(:Method),(:Technology)-[:USED_FOR]->(:Algorithm),(:Person)-[:WROTE]->(:Technology),(:Person)-[:MENTIONS]->(:Concept),(:Person)-[:PROFESSOR]->(:Concept),(:Topic)-[:RELATED_TO]->(:Topic),(:Topic)-[:RELATED_TO]->(:Author),(:Topic)-[:RELATED_TO]->(:Paper),(:Topic)-[:DISCUSSES]->(:Topic),(:Topic)-[:DISCUSSES]->(:Paper),(:Topic)-[:AUTHORED]->(:Author),(:Paper)-[:DISCUSSES]->(:Topic),(:Paper)-[:RELATED_TO]->(:Topic),(:Paper)-[:RELATED_TO]->(:Paper),(:Author)-[:AUTHORED]->(:Paper),(:Author)-[:DISCUSSES]->(:Topic)\\n    \\n    Question: What paper talk about Multi-Agent?\\n    \\n    Cypher Query:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:GraphCypherQAChain > chain:LLMChain > llm:ChatOpenAI] [3.86s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Certainly! Below is a Cypher query that searches for papers discussing \\\"Multi-Agent\\\" using case-insensitive and partial string matching on the paper titles:\\n\\n```cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n```\\n\\nThis query will return the titles, summaries, and URLs of papers that mention \\\"Multi-Agent\\\" in their titles, regardless of case.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Certainly! Below is a Cypher query that searches for papers discussing \\\"Multi-Agent\\\" using case-insensitive and partial string matching on the paper titles:\\n\\n```cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n```\\n\\nThis query will return the titles, summaries, and URLs of papers that mention \\\"Multi-Agent\\\" in their titles, regardless of case.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 101,\n",
      "                \"prompt_tokens\": 615,\n",
      "                \"total_tokens\": 716,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6830173e-02b4-4de2-939f-6e4063a9cc77-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 615,\n",
      "              \"output_tokens\": 101,\n",
      "              \"total_tokens\": 716\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 101,\n",
      "      \"prompt_tokens\": 615,\n",
      "      \"total_tokens\": 716,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:GraphCypherQAChain > chain:LLMChain] [3.86s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Certainly! Below is a Cypher query that searches for papers discussing \\\"Multi-Agent\\\" using case-insensitive and partial string matching on the paper titles:\\n\\n```cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n```\\n\\nThis query will return the titles, summaries, and URLs of papers that mention \\\"Multi-Agent\\\" in their titles, regardless of case.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:GraphCypherQAChain] [3.94s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ],\n",
      "  \"intermediate_steps\": [\n",
      "    {\n",
      "      \"query\": \"cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{'query': 'What paper talk about Multi-Agent?', 'result': [{'PaperTitle': 'Multi-Agent Assistant Code Generation (AgentCoder)', 'Summary': None, 'URL': None}, {'PaperTitle': 'Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)', 'Summary': None, 'URL': None}, {'PaperTitle': 'Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework', 'Summary': 'In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.', 'URL': 'https://github.com/amazon-science/comm-prompt'}], 'intermediate_steps': [{'query': 'cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\"multi-agent\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n'}]}\n"
     ]
    }
   ],
   "source": [
    "### Graph Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "cypher_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at generating Cypher queries for Neo4j.\n",
    "    Use the following schema to generate a Cypher query that answers the given question.\n",
    "    Make the query flexible by using case-insensitive matching and partial string matching where appropriate.\n",
    "    Focus on searching paper titles as they contain the most relevant information.\n",
    "    \n",
    "    Schema:\n",
    "    {schema}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Cypher Query:\"\"\",\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "# QA prompt\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following Cypher query results to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise. If topic information is not available, focus on the paper titles.\n",
    "    \n",
    "    Question: {question} \n",
    "    Cypher Query: {query}\n",
    "    Query Results: {context} \n",
    "    \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"question\", \"query\", \"context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "graph_rag_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=llm,\n",
    "    qa_llm=llm,\n",
    "    validate_cypher=True,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    return_direct=True,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    qa_prompt=qa_prompt,\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What paper talks about Multi-Agent?\"\n",
    "generation = graph_rag_chain.invoke({\"query\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Composite Vector + Graph Generations\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context from a vector store and a graph database to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Vector Context: {context} \n",
    "    Graph Context: {graph_context}\n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\", \"graph_context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Example input data\n",
    "# question = \"What main techniques are used in adversarial attacks on large language models\"\n",
    "question = \"What techniques are used for Multi-Agent? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'pk': 452500211585777722, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\nstructured approach to problem-solving and can help reduce redundancy and\\nenhance output diversity. A comprehensive evaluation across six real-world\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\nstructured approach to problem-solving and can help reduce redundancy and\\nenhance output diversity. A comprehensive evaluation across six real-world\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\noutstanding performance without requiring task-specific prompt engineering.'), Document(metadata={'pk': 452500478732271642, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\nstructured approach to problem-solving and can help reduce redundancy and\\nenhance output diversity. A comprehensive evaluation across six real-world\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\nstructured approach to problem-solving and can help reduce redundancy and\\nenhance output diversity. A comprehensive evaluation across six real-world\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\noutstanding performance without requiring task-specific prompt engineering.'), Document(metadata={'pk': 452500826504036378, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\nstructured approach to problem-solving and can help reduce redundancy and\\nenhance output diversity. A comprehensive evaluation across six real-world\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\nstructured approach to problem-solving and can help reduce redundancy and\\nenhance output diversity. A comprehensive evaluation across six real-world\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\noutstanding performance without requiring task-specific prompt engineering.'), Document(metadata={'pk': 452500886655860762, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\nstructured approach to problem-solving and can help reduce redundancy and\\nenhance output diversity. A comprehensive evaluation across six real-world\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\nstructured approach to problem-solving and can help reduce redundancy and\\nenhance output diversity. A comprehensive evaluation across six real-world\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\noutstanding performance without requiring task-specific prompt engineering.')]\n"
     ]
    }
   ],
   "source": [
    "# Get vector + graph answers\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What techniques are used for Multi-Agent?  \\n    Context: [Document(metadata={'pk': 452500211585777722, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.'), Document(metadata={'pk': 452500478732271642, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.'), Document(metadata={'pk': 452500826504036378, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.'), Document(metadata={'pk': 452500886655860762, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [14.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The techniques used for Multi-Agent are leveraging multiple large language model (LLM) agents and utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. The Captain Agent design dynamically forms and manages teams for each step of a task-solving process, allowing for a flexible yet structured approach to problem-solving. This can help reduce redundancy and enhance output diversity.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:22:15.155994Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 14698928083,\n",
      "          \"load_duration\": 4100416042,\n",
      "          \"prompt_eval_count\": 1930,\n",
      "          \"prompt_eval_duration\": 7596420000,\n",
      "          \"eval_count\": 74,\n",
      "          \"eval_duration\": 2995321000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The techniques used for Multi-Agent are leveraging multiple large language model (LLM) agents and utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. The Captain Agent design dynamically forms and manages teams for each step of a task-solving process, allowing for a flexible yet structured approach to problem-solving. This can help reduce redundancy and enhance output diversity.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:22:15.155994Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 14698928083,\n",
      "              \"load_duration\": 4100416042,\n",
      "              \"prompt_eval_count\": 1930,\n",
      "              \"prompt_eval_duration\": 7596420000,\n",
      "              \"eval_count\": 74,\n",
      "              \"eval_duration\": 2995321000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-31ceb565-ea70-4d8c-96c1-8c7d01470988-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The techniques used for Multi-Agent are leveraging multiple large language model (LLM) agents and utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. The Captain Agent design dynamically forms and manages teams for each step of a task-solving process, allowing for a flexible yet structured approach to problem-solving. This can help reduce redundancy and enhance output diversity.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [14.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The techniques used for Multi-Agent are leveraging multiple large language model (LLM) agents and utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. The Captain Agent design dynamically forms and manages teams for each step of a task-solving process, allowing for a flexible yet structured approach to problem-solving. This can help reduce redundancy and enhance output diversity.\"\n",
      "}\n",
      "The techniques used for Multi-Agent are leveraging multiple large language model (LLM) agents and utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. The Captain Agent design dynamically forms and manages teams for each step of a task-solving process, allowing for a flexible yet structured approach to problem-solving. This can help reduce redundancy and enhance output diversity.\n"
     ]
    }
   ],
   "source": [
    "vector_context = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "print(vector_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:GraphCypherQAChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What techniques are used for Multi-Agent? \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:GraphCypherQAChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What techniques are used for Multi-Agent? \",\n",
      "  \"schema\": \"Node properties are the following:\\nConcept {id: STRING, description: STRING, name: STRING},Technique {id: STRING},Agent {id: STRING},Profession {id: STRING},Publication {id: STRING, title: STRING},Technology {id: STRING, description: STRING},Person {id: STRING, description: STRING},Algorithm {id: STRING, field: STRING, description: STRING},Domain {id: STRING, description: STRING},Method {id: STRING, description: STRING},Node {id: STRING, name: STRING},Website {id: STRING},Domain-specific nlp task {id: STRING, description: STRING},Property {id: STRING},Topic {id: STRING, summary: STRING, title: STRING, url: STRING},Paper {id: STRING, title: STRING, summary: STRING, url: STRING},Author {id: STRING, title: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Concept)-[:RELATED_TO]->(:Concept),(:Concept)-[:RELATED_TO]->(:Agent),(:Concept)-[:RELATED_TO]->(:Technology),(:Concept)-[:RELATED_TO]->(:Website),(:Concept)-[:WROTE]->(:Technology),(:Concept)-[:APPLIED]->(:Algorithm),(:Concept)-[:INITIATED]->(:Algorithm),(:Concept)-[:ENHANCE]->(:Concept),(:Concept)-[:PRODUCE]->(:Concept),(:Concept)-[:ASSOCIATED_WITH]->(:Concept),(:Publication)-[:DESCRIBES]->(:Concept),(:Technology)-[:MENTIONS]->(:Technology),(:Technology)-[:RELATED_TO]->(:Domain),(:Technology)-[:RELATED_TO]->(:Concept),(:Technology)-[:RELATED_TO]->(:Algorithm),(:Technology)-[:USED_IN]->(:Method),(:Technology)-[:USED_FOR]->(:Algorithm),(:Person)-[:WROTE]->(:Technology),(:Person)-[:MENTIONS]->(:Concept),(:Person)-[:PROFESSOR]->(:Concept),(:Topic)-[:RELATED_TO]->(:Topic),(:Topic)-[:RELATED_TO]->(:Author),(:Topic)-[:RELATED_TO]->(:Paper),(:Topic)-[:DISCUSSES]->(:Topic),(:Topic)-[:DISCUSSES]->(:Paper),(:Topic)-[:AUTHORED]->(:Author),(:Paper)-[:DISCUSSES]->(:Topic),(:Paper)-[:RELATED_TO]->(:Topic),(:Paper)-[:RELATED_TO]->(:Paper),(:Author)-[:AUTHORED]->(:Paper),(:Author)-[:DISCUSSES]->(:Topic)\",\n",
      "  \"query\": \"What techniques are used for Multi-Agent? \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:GraphCypherQAChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at generating Cypher queries for Neo4j.\\n    Use the following schema to generate a Cypher query that answers the given question.\\n    Make the query flexible by using case-insensitive matching and partial string matching where appropriate.\\n    Focus on searching paper titles as they contain the most relevant information.\\n    \\n    Schema:\\n    Node properties are the following:\\nConcept {id: STRING, description: STRING, name: STRING},Technique {id: STRING},Agent {id: STRING},Profession {id: STRING},Publication {id: STRING, title: STRING},Technology {id: STRING, description: STRING},Person {id: STRING, description: STRING},Algorithm {id: STRING, field: STRING, description: STRING},Domain {id: STRING, description: STRING},Method {id: STRING, description: STRING},Node {id: STRING, name: STRING},Website {id: STRING},Domain-specific nlp task {id: STRING, description: STRING},Property {id: STRING},Topic {id: STRING, summary: STRING, title: STRING, url: STRING},Paper {id: STRING, title: STRING, summary: STRING, url: STRING},Author {id: STRING, title: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Concept)-[:RELATED_TO]->(:Concept),(:Concept)-[:RELATED_TO]->(:Agent),(:Concept)-[:RELATED_TO]->(:Technology),(:Concept)-[:RELATED_TO]->(:Website),(:Concept)-[:WROTE]->(:Technology),(:Concept)-[:APPLIED]->(:Algorithm),(:Concept)-[:INITIATED]->(:Algorithm),(:Concept)-[:ENHANCE]->(:Concept),(:Concept)-[:PRODUCE]->(:Concept),(:Concept)-[:ASSOCIATED_WITH]->(:Concept),(:Publication)-[:DESCRIBES]->(:Concept),(:Technology)-[:MENTIONS]->(:Technology),(:Technology)-[:RELATED_TO]->(:Domain),(:Technology)-[:RELATED_TO]->(:Concept),(:Technology)-[:RELATED_TO]->(:Algorithm),(:Technology)-[:USED_IN]->(:Method),(:Technology)-[:USED_FOR]->(:Algorithm),(:Person)-[:WROTE]->(:Technology),(:Person)-[:MENTIONS]->(:Concept),(:Person)-[:PROFESSOR]->(:Concept),(:Topic)-[:RELATED_TO]->(:Topic),(:Topic)-[:RELATED_TO]->(:Author),(:Topic)-[:RELATED_TO]->(:Paper),(:Topic)-[:DISCUSSES]->(:Topic),(:Topic)-[:DISCUSSES]->(:Paper),(:Topic)-[:AUTHORED]->(:Author),(:Paper)-[:DISCUSSES]->(:Topic),(:Paper)-[:RELATED_TO]->(:Topic),(:Paper)-[:RELATED_TO]->(:Paper),(:Author)-[:AUTHORED]->(:Paper),(:Author)-[:DISCUSSES]->(:Topic)\\n    \\n    Question: What techniques are used for Multi-Agent? \\n    \\n    Cypher Query:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:GraphCypherQAChain > chain:LLMChain > llm:ChatOpenAI] [2.97s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To answer the question \\\"What techniques are used for Multi-Agent?\\\" using the provided schema, we need to focus on the relationships and properties that connect techniques to the concept of \\\"Multi-Agent\\\". Specifically, we will look for `Technique` nodes that are related to the `Concept` node with the name \\\"Multi-Agent\\\".\\n\\nHere is a Cypher query that achieves this, using case-insensitive and partial string matching for flexibility:\\n\\n```cypher\\nMATCH (c:Concept)-[:RELATED_TO]->(t:Technique)\\nWHERE toLower(c.name) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN DISTINCT t.id AS TechniqueID\\n```\\n\\nThis query does the following:\\n1. Matches `Concept` nodes that are related to `Technique` nodes.\\n2. Filters the `Concept` nodes to those whose `name` property contains the string \\\"multi-agent\\\", ignoring case.\\n3. Returns the distinct IDs of the related `Technique` nodes.\\n\\nThis should provide a list of techniques used for Multi-Agent systems.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To answer the question \\\"What techniques are used for Multi-Agent?\\\" using the provided schema, we need to focus on the relationships and properties that connect techniques to the concept of \\\"Multi-Agent\\\". Specifically, we will look for `Technique` nodes that are related to the `Concept` node with the name \\\"Multi-Agent\\\".\\n\\nHere is a Cypher query that achieves this, using case-insensitive and partial string matching for flexibility:\\n\\n```cypher\\nMATCH (c:Concept)-[:RELATED_TO]->(t:Technique)\\nWHERE toLower(c.name) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN DISTINCT t.id AS TechniqueID\\n```\\n\\nThis query does the following:\\n1. Matches `Concept` nodes that are related to `Technique` nodes.\\n2. Filters the `Concept` nodes to those whose `name` property contains the string \\\"multi-agent\\\", ignoring case.\\n3. Returns the distinct IDs of the related `Technique` nodes.\\n\\nThis should provide a list of techniques used for Multi-Agent systems.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 203,\n",
      "                \"prompt_tokens\": 616,\n",
      "                \"total_tokens\": 819,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c0c0c004-bee1-457e-9eef-b81b9dfd2553-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 616,\n",
      "              \"output_tokens\": 203,\n",
      "              \"total_tokens\": 819\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 203,\n",
      "      \"prompt_tokens\": 616,\n",
      "      \"total_tokens\": 819,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:GraphCypherQAChain > chain:LLMChain] [2.97s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"To answer the question \\\"What techniques are used for Multi-Agent?\\\" using the provided schema, we need to focus on the relationships and properties that connect techniques to the concept of \\\"Multi-Agent\\\". Specifically, we will look for `Technique` nodes that are related to the `Concept` node with the name \\\"Multi-Agent\\\".\\n\\nHere is a Cypher query that achieves this, using case-insensitive and partial string matching for flexibility:\\n\\n```cypher\\nMATCH (c:Concept)-[:RELATED_TO]->(t:Technique)\\nWHERE toLower(c.name) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN DISTINCT t.id AS TechniqueID\\n```\\n\\nThis query does the following:\\n1. Matches `Concept` nodes that are related to `Technique` nodes.\\n2. Filters the `Concept` nodes to those whose `name` property contains the string \\\"multi-agent\\\", ignoring case.\\n3. Returns the distinct IDs of the related `Technique` nodes.\\n\\nThis should provide a list of techniques used for Multi-Agent systems.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:GraphCypherQAChain] [2.97s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": [],\n",
      "  \"intermediate_steps\": [\n",
      "    {\n",
      "      \"query\": \"\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{'query': 'What techniques are used for Multi-Agent? ', 'result': [], 'intermediate_steps': [{'query': ''}]}\n"
     ]
    }
   ],
   "source": [
    "graph_context = graph_rag_chain.invoke({\"query\": question})\n",
    "\n",
    "print(graph_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What techniques are used for Multi-Agent? \",\n",
      "  \"context\": \"The techniques used for Multi-Agent are leveraging multiple large language model (LLM) agents and utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. The Captain Agent design dynamically forms and manages teams for each step of a task-solving process, allowing for a flexible yet structured approach to problem-solving. This can help reduce redundancy and enhance output diversity.\",\n",
      "  \"graph_context\": {\n",
      "    \"query\": \"What techniques are used for Multi-Agent? \",\n",
      "    \"result\": [],\n",
      "    \"intermediate_steps\": [\n",
      "      {\n",
      "        \"query\": \"\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What techniques are used for Multi-Agent? \",\n",
      "  \"context\": \"The techniques used for Multi-Agent are leveraging multiple large language model (LLM) agents and utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. The Captain Agent design dynamically forms and manages teams for each step of a task-solving process, allowing for a flexible yet structured approach to problem-solving. This can help reduce redundancy and enhance output diversity.\",\n",
      "  \"graph_context\": {\n",
      "    \"query\": \"What techniques are used for Multi-Agent? \",\n",
      "    \"result\": [],\n",
      "    \"intermediate_steps\": [\n",
      "      {\n",
      "        \"query\": \"\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context from a vector store and a graph database to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What techniques are used for Multi-Agent?  \\n    Vector Context: The techniques used for Multi-Agent are leveraging multiple large language model (LLM) agents and utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. The Captain Agent design dynamically forms and manages teams for each step of a task-solving process, allowing for a flexible yet structured approach to problem-solving. This can help reduce redundancy and enhance output diversity. \\n    Graph Context: {'query': 'What techniques are used for Multi-Agent? ', 'result': [], 'intermediate_steps': [{'query': ''}]}\\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [2.81s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The techniques used for Multi-Agent include leveraging multiple large language model (LLM) agents, utilizing nested group conversations and reflection, and the Captain Agent design that dynamically forms and manages teams. This helps ensure diverse expertise and prevent stereotypical outputs, reducing redundancy and enhancing output diversity.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:22:32.294674Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2803723792,\n",
      "          \"load_duration\": 36296000,\n",
      "          \"prompt_eval_count\": 199,\n",
      "          \"prompt_eval_duration\": 910546000,\n",
      "          \"eval_count\": 56,\n",
      "          \"eval_duration\": 1855922000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The techniques used for Multi-Agent include leveraging multiple large language model (LLM) agents, utilizing nested group conversations and reflection, and the Captain Agent design that dynamically forms and manages teams. This helps ensure diverse expertise and prevent stereotypical outputs, reducing redundancy and enhancing output diversity.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:22:32.294674Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2803723792,\n",
      "              \"load_duration\": 36296000,\n",
      "              \"prompt_eval_count\": 199,\n",
      "              \"prompt_eval_duration\": 910546000,\n",
      "              \"eval_count\": 56,\n",
      "              \"eval_duration\": 1855922000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3e355120-53f1-4fb9-abe0-1339c8d61a27-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 199,\n",
      "              \"output_tokens\": 56,\n",
      "              \"total_tokens\": 255\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The techniques used for Multi-Agent include leveraging multiple large language model (LLM) agents, utilizing nested group conversations and reflection, and the Captain Agent design that dynamically forms and manages teams. This helps ensure diverse expertise and prevent stereotypical outputs, reducing redundancy and enhancing output diversity.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.81s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The techniques used for Multi-Agent include leveraging multiple large language model (LLM) agents, utilizing nested group conversations and reflection, and the Captain Agent design that dynamically forms and manages teams. This helps ensure diverse expertise and prevent stereotypical outputs, reducing redundancy and enhancing output diversity.\"\n",
      "}\n",
      "The techniques used for Multi-Agent include leveraging multiple large language model (LLM) agents, utilizing nested group conversations and reflection, and the Captain Agent design that dynamically forms and manages teams. This helps ensure diverse expertise and prevent stereotypical outputs, reducing redundancy and enhancing output diversity.\n"
     ]
    }
   ],
   "source": [
    "# Run the chain\n",
    "composite_chain = prompt | llm | StrOutputParser()\n",
    "answer = composite_chain.invoke({\"question\": question, \"context\": vector_context, \"graph_context\": graph_context})\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={'pk': 452500211585777722, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.'), Document(metadata={'pk': 452500478732271642, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.'), Document(metadata={'pk': 452500826504036378, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.'), Document(metadata={'pk': 452500886655860762, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v1'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\\\npromising approach for tackling complex tasks, while the effective design of\\\\nmultiple agents for a particular application remains an art. It is thus\\\\nintriguing to answer a critical question: Given a task, how can we build a team\\\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\\\noffers a flexible solution, realized through a novel agent design named Captain\\\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\\\nexpertise and prevent stereotypical outputs. It allows for a flexible yet\\\\nstructured approach to problem-solving and can help reduce redundancy and\\\\nenhance output diversity. A comprehensive evaluation across six real-world\\\\nscenarios demonstrates that Captain Agent significantly outperforms existing\\\\nmulti-agent methods with 21.94% improvement in average accuracy, providing\\\\noutstanding performance without requiring task-specific prompt engineering.')] \\n\\n    Here is the answer: \\n    {'query': 'What paper talk about Multi-Agent?', 'result': [{'PaperTitle': 'Multi-Agent Assistant Code Generation (AgentCoder)', 'Summary': None, 'URL': None}, {'PaperTitle': 'Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)', 'Summary': None, 'URL': None}, {'PaperTitle': 'Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework', 'Summary': 'In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.', 'URL': 'https://github.com/amazon-science/comm-prompt'}], 'intermediate_steps': [{'query': 'cypher\\\\nMATCH (p:Paper)\\\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\\\n'}]}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [17.79s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n  \\\"query\\\": \\\"What paper talk about Multi-Agent?\\\",\\n  \\\"result\\\": [\\n    {\\n      \\\"PaperTitle\\\": \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\",\\n      \\\"Summary\\\": null,\\n      \\\"URL\\\": null\\n    },\\n    {\\n      \\\"PaperTitle\\\": \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\",\\n      \\\"Summary\\\": null,\\n      \\\"URL\\\": null\\n    },\\n    {\\n      \\\"PaperTitle\\\": \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\",\\n      \\\"Summary\\\": \\\"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\\\",\\n      \\\"URL\\\": \\\"https://github.com/amazon-science/comm-prompt\\\"\\n    }\\n  ],\\n  \\\"intermediate_steps\\\": [\\n    {\\n      \\\"query\\\": \\\"cypher\\\\nMATCH (p:Paper)\\\\nWHERE toLower(p.title) CONTAINS toLower(\\\\\\\"multi-agent\\\\\\\")\\\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\\\n\\\"\\n    }\\n  ]\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:06.368874Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 17782059708,\n",
      "          \"load_duration\": 24544500,\n",
      "          \"prompt_eval_count\": 1026,\n",
      "          \"prompt_eval_duration\": 4188660000,\n",
      "          \"eval_count\": 344,\n",
      "          \"eval_duration\": 13566093000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\n  \\\"query\\\": \\\"What paper talk about Multi-Agent?\\\",\\n  \\\"result\\\": [\\n    {\\n      \\\"PaperTitle\\\": \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\",\\n      \\\"Summary\\\": null,\\n      \\\"URL\\\": null\\n    },\\n    {\\n      \\\"PaperTitle\\\": \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\",\\n      \\\"Summary\\\": null,\\n      \\\"URL\\\": null\\n    },\\n    {\\n      \\\"PaperTitle\\\": \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\",\\n      \\\"Summary\\\": \\\"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\\\",\\n      \\\"URL\\\": \\\"https://github.com/amazon-science/comm-prompt\\\"\\n    }\\n  ],\\n  \\\"intermediate_steps\\\": [\\n    {\\n      \\\"query\\\": \\\"cypher\\\\nMATCH (p:Paper)\\\\nWHERE toLower(p.title) CONTAINS toLower(\\\\\\\"multi-agent\\\\\\\")\\\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\\\n\\\"\\n    }\\n  ]\\n}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:06.368874Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 17782059708,\n",
      "              \"load_duration\": 24544500,\n",
      "              \"prompt_eval_count\": 1026,\n",
      "              \"prompt_eval_duration\": 4188660000,\n",
      "              \"eval_count\": 344,\n",
      "              \"eval_duration\": 13566093000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-d222f90e-a36f-41cd-ae89-fc30808a97fa-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1026,\n",
      "              \"output_tokens\": 344,\n",
      "              \"total_tokens\": 1370\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What paper talk about Multi-Agent?\",\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ],\n",
      "  \"intermediate_steps\": [\n",
      "    {\n",
      "      \"query\": \"cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [17.79s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What paper talk about Multi-Agent?\",\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ],\n",
      "  \"intermediate_steps\": [\n",
      "    {\n",
      "      \"query\": \"cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What paper talk about Multi-Agent?',\n",
       " 'result': [{'PaperTitle': 'Multi-Agent Assistant Code Generation (AgentCoder)',\n",
       "   'Summary': None,\n",
       "   'URL': None},\n",
       "  {'PaperTitle': 'Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)',\n",
       "   'Summary': None,\n",
       "   'URL': None},\n",
       "  {'PaperTitle': 'Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework',\n",
       "   'Summary': 'In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.',\n",
       "   'URL': 'https://github.com/amazon-science/comm-prompt'}],\n",
       " 'intermediate_steps': [{'query': 'cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\"multi-agent\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n'}]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader \n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here are the facts:\n",
    "    {documents} \n",
    "\n",
    "    Here is the answer: \n",
    "    {generation}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What techniques are used for Multi-Agent? \",\n",
      "  \"generation\": {\n",
      "    \"query\": \"What paper talk about Multi-Agent?\",\n",
      "    \"result\": [\n",
      "      {\n",
      "        \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "        \"Summary\": null,\n",
      "        \"URL\": null\n",
      "      },\n",
      "      {\n",
      "        \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "        \"Summary\": null,\n",
      "        \"URL\": null\n",
      "      },\n",
      "      {\n",
      "        \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "        \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "        \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "      }\n",
      "    ],\n",
      "    \"intermediate_steps\": [\n",
      "      {\n",
      "        \"query\": \"cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What techniques are used for Multi-Agent? \",\n",
      "  \"generation\": {\n",
      "    \"query\": \"What paper talk about Multi-Agent?\",\n",
      "    \"result\": [\n",
      "      {\n",
      "        \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "        \"Summary\": null,\n",
      "        \"URL\": null\n",
      "      },\n",
      "      {\n",
      "        \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "        \"Summary\": null,\n",
      "        \"URL\": null\n",
      "      },\n",
      "      {\n",
      "        \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "        \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "        \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "      }\n",
      "    ],\n",
      "    \"intermediate_steps\": [\n",
      "      {\n",
      "        \"query\": \"cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether an \\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \\n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n     \\n    Here is the answer:\\n    {'query': 'What paper talk about Multi-Agent?', 'result': [{'PaperTitle': 'Multi-Agent Assistant Code Generation (AgentCoder)', 'Summary': None, 'URL': None}, {'PaperTitle': 'Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)', 'Summary': None, 'URL': None}, {'PaperTitle': 'Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework', 'Summary': 'In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.', 'URL': 'https://github.com/amazon-science/comm-prompt'}], 'intermediate_steps': [{'query': 'cypher\\\\nMATCH (p:Paper)\\\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\\\n'}]} \\n\\n    Here is the question: What techniques are used for Multi-Agent?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [1.85s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:08.258314Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1852363792,\n",
      "          \"load_duration\": 16028625,\n",
      "          \"prompt_eval_count\": 406,\n",
      "          \"prompt_eval_duration\": 1602247000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 232253000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:08.258314Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1852363792,\n",
      "              \"load_duration\": 16028625,\n",
      "              \"prompt_eval_count\": 406,\n",
      "              \"prompt_eval_duration\": 1602247000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 232253000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-02619067-8deb-49da-b05a-a2e9e362e304-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 406,\n",
      "              \"output_tokens\": 7,\n",
      "              \"total_tokens\": 413\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.86s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader \n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     \n",
    "    Here is the answer:\n",
    "    {generation} \n",
    "\n",
    "    Here is the question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"llm agent memory\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"llm agent memory\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a user question to the most appropriate data source. \\n    You have three options:\\n    1. 'vectorstore': Use for questions about LLM agents, prompt engineering, and adversarial attacks.\\n    2. 'graphrag': Use for questions that involve relationships between entities, such as authors, papers, and topics, or when the question requires understanding connections between concepts.\\n    3. 'web_search': Use for all other questions or when current information is needed.\\n\\n    You do not need to be stringent with the keywords in the question related to these topics. \\n    Choose the most appropriate option based on the nature of the question.\\n\\n    Return a JSON with a single key 'datasource' and no preamble or explanation. \\n    The value should be one of: 'vectorstore', 'graphrag', or 'web_search'.\\n    \\n    Question to route: \\n    llm agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [1.09s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:10.101508Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1084724583,\n",
      "          \"load_duration\": 35418958,\n",
      "          \"prompt_eval_count\": 199,\n",
      "          \"prompt_eval_duration\": 783938000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 264485000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:10.101508Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1084724583,\n",
      "              \"load_duration\": 35418958,\n",
      "              \"prompt_eval_count\": 199,\n",
      "              \"prompt_eval_duration\": 783938000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 264485000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c3d6a94a-accd-4108-8756-ddc1385dbc0c-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.09s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at routing a user question to the most appropriate data source. \n",
    "    You have three options:\n",
    "    1. 'vectorstore': Use for questions about LLM agents, prompt engineering, and adversarial attacks.\n",
    "    2. 'graphrag': Use for questions that involve relationships between entities, such as authors, papers, and topics, or when the question requires understanding connections between concepts.\n",
    "    3. 'web_search': Use for all other questions or when current information is needed.\n",
    "\n",
    "    You do not need to be stringent with the keywords in the question related to these topics. \n",
    "    Choose the most appropriate option based on the nature of the question.\n",
    "\n",
    "    Return a JSON with a single key 'datasource' and no preamble or explanation. \n",
    "    The value should be one of: 'vectorstore', 'graphrag', or 'web_search'.\n",
    "    \n",
    "    Question to route: \n",
    "    {question}\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"llm agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement these as a control flow in LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents \n",
    "        graph_context: results from graph search\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    graph_context: str\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents and graph context\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    graph_context = state.get(\"graph_context\", \"\")\n",
    "    \n",
    "    # Composite RAG generation\n",
    "    generation = composite_chain.invoke({\n",
    "        \"question\": question, \n",
    "        \"context\": documents, \n",
    "        \"graph_context\": graph_context\n",
    "    })\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"graph_context\": graph_context}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score['score']\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "    \n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])  # Use get() with a default empty list\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "def route_question(state):\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})  \n",
    "    print(source)\n",
    "    print(source['datasource'])\n",
    "    \n",
    "    if source['datasource'] == 'graphrag':\n",
    "        print(\"---TRYING GRAPH SEARCH---\")\n",
    "        graph_result = graph_search({\"question\": question})\n",
    "        if graph_result[\"graph_context\"] != \"No results found in the graph database.\":\n",
    "            return \"graphrag\"\n",
    "        else:\n",
    "            print(\"---NO RESULTS IN GRAPH, FALLING BACK TO VECTORSTORE---\")\n",
    "            return \"retrieve\"\n",
    "    elif source['datasource'] == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO VECTORSTORE RAG---\")\n",
    "        return \"retrieve\"\n",
    "    elif source['datasource'] == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def graph_search(state):\n",
    "    \"\"\"\n",
    "    Perform GraphRAG search using Neo4j\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with graph search results\n",
    "    \"\"\"\n",
    "    print(\"---GRAPH SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Use the graph_rag_chain to perform the search\n",
    "    result = graph_rag_chain.invoke({\"query\": question})\n",
    "    \n",
    "    # Extract the relevant information from the result\n",
    "    # Adjust this based on what graph_rag_chain returns\n",
    "    graph_context = result.get(\"result\", \"\")\n",
    "    \n",
    "    # You might want to combine this with existing documents or keep it separate\n",
    "    return {\"graph_context\": graph_context, \"question\": question}\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = grade = score.get('score', '').lower()\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generatae\n",
    "workflow.add_node(\"graphrag\", graph_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set conditional entry point\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"graphrag\": \"graphrag\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"graphrag\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents,graph_context>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\"\n",
      "}\n",
      "---ROUTE QUESTION---\n",
      "What are the types of Prompt Engineering?\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a user question to the most appropriate data source. \\n    You have three options:\\n    1. 'vectorstore': Use for questions about LLM agents, prompt engineering, and adversarial attacks.\\n    2. 'graphrag': Use for questions that involve relationships between entities, such as authors, papers, and topics, or when the question requires understanding connections between concepts.\\n    3. 'web_search': Use for all other questions or when current information is needed.\\n\\n    You do not need to be stringent with the keywords in the question related to these topics. \\n    Choose the most appropriate option based on the nature of the question.\\n\\n    Return a JSON with a single key 'datasource' and no preamble or explanation. \\n    The value should be one of: 'vectorstore', 'graphrag', or 'web_search'.\\n    \\n    Question to route: \\n    What are the types of Prompt Engineering?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] [572ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:10.710856Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 569452834,\n",
      "          \"load_duration\": 15792459,\n",
      "          \"prompt_eval_count\": 203,\n",
      "          \"prompt_eval_duration\": 289360000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 262769000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:10.710856Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 569452834,\n",
      "              \"load_duration\": 15792459,\n",
      "              \"prompt_eval_count\": 203,\n",
      "              \"prompt_eval_duration\": 289360000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 262769000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0b429450-d998-4d94-a52b-25fac67b9ade-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] [574ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "{'datasource': 'vectorstore'}\n",
      "vectorstore\n",
      "---ROUTE QUESTION TO VECTORSTORE RAG---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] [574ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"retrieve\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [575ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\"\n",
      "}\n",
      "---RETRIEVE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve > chain:ChannelWrite<retrieve,question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve > chain:ChannelWrite<retrieve,question,generation,web_search,documents,graph_context>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve] [73ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: retrieve:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\\n    \\n    Here is the user question: \\n    What are the types of Prompt Engineering?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [1.92s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:12.703125Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1914052208,\n",
      "          \"load_duration\": 12349541,\n",
      "          \"prompt_eval_count\": 426,\n",
      "          \"prompt_eval_duration\": 1699175000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 201406000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:12.703125Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1914052208,\n",
      "              \"load_duration\": 12349541,\n",
      "              \"prompt_eval_count\": 426,\n",
      "              \"prompt_eval_duration\": 1699175000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 201406000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-9f1cb6ff-1bc5-4ed1-8051-429ca530426a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [1.92s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\\n    \\n    Here is the user question: \\n    What are the types of Prompt Engineering?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [257ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:12.963768Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 250581750,\n",
      "          \"load_duration\": 13485209,\n",
      "          \"prompt_eval_count\": 426,\n",
      "          \"prompt_eval_duration\": 34416000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 201591000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:12.963768Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 250581750,\n",
      "              \"load_duration\": 13485209,\n",
      "              \"prompt_eval_count\": 426,\n",
      "              \"prompt_eval_duration\": 34416000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 201591000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0a933c85-712a-4b74-b71c-44a078bc8248-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [260ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\\n    \\n    Here is the user question: \\n    What are the types of Prompt Engineering?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [262ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:13.226877Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 249381416,\n",
      "          \"load_duration\": 12706458,\n",
      "          \"prompt_eval_count\": 426,\n",
      "          \"prompt_eval_duration\": 34278000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 201184000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:13.226877Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 249381416,\n",
      "              \"load_duration\": 12706458,\n",
      "              \"prompt_eval_count\": 426,\n",
      "              \"prompt_eval_duration\": 34278000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 201184000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8222b696-1825-40ae-92b1-f8f96333d81d-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [266ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"document\": \"Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Prompt engineering is critical for effective interaction with large language\\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\\nhave been limited. This study designed and implemented a prompt engineering\\nintervention, examining its influence on undergraduate students' AI\\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\\nintervention involved 27 students who participated in a 100-minute workshop\\nconducted during their history course at a university in Hong Kong. During the\\nworkshop, students were introduced to prompt engineering strategies, which they\\napplied to plan the course's final essay task. Multiple data sources were\\ncollected, including students' responses to pre- and post-workshop\\nquestionnaires, pre- and post-workshop prompt libraries, and written\\nreflections. The study's findings revealed that students demonstrated a higher\\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\\nimproved prompt engineering skills because of the intervention. These findings\\nhave implications for AI literacy education, as they highlight the importance\\nof prompt engineering training for specific higher education use cases. This is\\na significant shift from students haphazardly and intuitively learning to\\nengineer prompts. Through prompt engineering education, educators can faciitate\\nstudents' effective navigation and leverage of LLMs to support their\\ncoursework.\\n    \\n    Here is the user question: \\n    What are the types of Prompt Engineering?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [255ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:13.487379Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 250109959,\n",
      "          \"load_duration\": 12964459,\n",
      "          \"prompt_eval_count\": 426,\n",
      "          \"prompt_eval_duration\": 34478000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 201820000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:13.487379Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 250109959,\n",
      "              \"load_duration\": 12964459,\n",
      "              \"prompt_eval_count\": 426,\n",
      "              \"prompt_eval_duration\": 34478000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 201820000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c324b18b-35b7-4bef-8de4-3c15d9c80567-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [257ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:ChannelWrite<grade_documents,question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"web_search\": \"Yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:ChannelWrite<grade_documents,question,generation,web_search,documents,graph_context>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"web_search\": \"Yes\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:decide_to_generate] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"web_search\": \"Yes\"\n",
      "}\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:decide_to_generate] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"websearch\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents] [2.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"web_search\": \"Yes\"\n",
      "}\n",
      "'Finished running: grade_documents:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"web_search\": \"Yes\",\n",
      "  \"documents\": []\n",
      "}\n",
      "---WEB SEARCH---\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > tool:tavily_search_results_json] Entering Tool run with input:\n",
      "\u001b[0m\"{'query': 'What are the types of Prompt Engineering?'}\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > tool:tavily_search_results_json] [3.14s] Exiting Tool run with output:\n",
      "\u001b[0m\"[{'url': 'https://www.upwork.com/resources/prompt-engineering-guide', 'content': \"Types of prompt engineering. Prompt engineering is writing text to feed to an AI model. However, several forms of prompt engineering can impact your success. Zero-shot prompting. This technique involves asking the model to perform a task without any examples or prior training on that specific task. It tests the model's ability to generalize ...\"}, {'url': 'https://builtin.com/artificial-intelligence/prompt-engineering', 'content': 'Below are some of the more common prompt engineering techniques used for a range of scenarios:\\nTypes of Prompt Engineering\\nHere are some of the main types of prompt engineering youâ€™ll most likely employ when using generative AI tools:\\nPrompt Engineering Best Practices\\nLetâ€™s go over some dos and donâ€™ts on writing a good prompt.\\n Key Elements of a Prompt\\nBefore diving into the creation of prompts, itâ€™s crucial to understand the building blocks of an effective prompt:\\nPrompt Engineering Techniques\\nThere are many ways to develop a prompt. Guide to Prompt Engineering\\nPrompt engineering is the process of optimizing the performance of\\xa0generative AI by tailoring the questions and processes to your specific needs.\\n Understanding the business problem and the types of personas that are needed to produce the desired output are of the utmost importance in a good prompt engineer.\\n Frequently Asked Questions\\nWhat is prompt engineering?\\nPrompt engineering is the practice of creating and tailoring input prompts or instructions to guide a language model to produce a desired response.'}, {'url': 'https://www.promptingguide.ai/introduction', 'content': 'Introduction. Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently apply and build with large language models (LLMs) for a wide variety of applications and use cases. Prompt engineering skills help to better understand the capabilities and limitations of LLMs. Researchers use prompt engineering ...'}, {'url': 'https://medium.com/@amiraryani/8-types-of-prompt-engineering-5322fff77bdf', 'content': 'Prompt engineering is a technique used to effectively communicate with large language models (LLM) like GPT-3 or GPT-4 and get the desired output. Here there are applications of 8 distinct promptâ€¦'}, {'url': 'https://www.coursera.org/articles/what-is-prompt-engineering', 'content': 'Examples of prompt engineering\\nHere are a few examples of prompt engineering to give you a better understanding of what it is and how you might engineer a prompt with a text and image model.\\n Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals.\\n$1 unlocks unlimited opportunities\\nCoursera Footer\\nPopular AI Content\\nPopular Programs\\nPopular Skills\\nPopular Career Resources\\nCoursera\\nCommunity\\nMore You can do the same for text-to-image models like DALL-E.\\nWhy is it important in generative AI?\\nPrompt engineering is important for AI engineers to create better services, such as chatbots that handle customer service tasks or generate legal contracts. If you do need to include â€œtoneâ€ in your prompt, should you write â€œin a professional toneâ€ or â€œin a formal toneâ€?\\n Whether youâ€™re inputting prompts in ChatGPT to help you write your resume or using DALL-E to generate a photo for a presentation, anybody can be a prompt engineer.\\xa0'}]\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents,graph_context>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] [3.14s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: websearch:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context from a vector store and a graph database to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of Prompt Engineering? \\n    Vector Context: [Document(page_content=\\\"Types of prompt engineering. Prompt engineering is writing text to feed to an AI model. However, several forms of prompt engineering can impact your success. Zero-shot prompting. This technique involves asking the model to perform a task without any examples or prior training on that specific task. It tests the model's ability to generalize ...\\\\nBelow are some of the more common prompt engineering techniques used for a range of scenarios:\\\\nTypes of Prompt Engineering\\\\nHere are some of the main types of prompt engineering youâ€™ll most likely employ when using generative AI tools:\\\\nPrompt Engineering Best Practices\\\\nLetâ€™s go over some dos and donâ€™ts on writing a good prompt.\\\\n Key Elements of a Prompt\\\\nBefore diving into the creation of prompts, itâ€™s crucial to understand the building blocks of an effective prompt:\\\\nPrompt Engineering Techniques\\\\nThere are many ways to develop a prompt. Guide to Prompt Engineering\\\\nPrompt engineering is the process of optimizing the performance of\\\\xa0generative AI by tailoring the questions and processes to your specific needs.\\\\n Understanding the business problem and the types of personas that are needed to produce the desired output are of the utmost importance in a good prompt engineer.\\\\n Frequently Asked Questions\\\\nWhat is prompt engineering?\\\\nPrompt engineering is the practice of creating and tailoring input prompts or instructions to guide a language model to produce a desired response.\\\\nIntroduction. Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently apply and build with large language models (LLMs) for a wide variety of applications and use cases. Prompt engineering skills help to better understand the capabilities and limitations of LLMs. Researchers use prompt engineering ...\\\\nPrompt engineering is a technique used to effectively communicate with large language models (LLM) like GPT-3 or GPT-4 and get the desired output. Here there are applications of 8 distinct promptâ€¦\\\\nExamples of prompt engineering\\\\nHere are a few examples of prompt engineering to give you a better understanding of what it is and how you might engineer a prompt with a text and image model.\\\\n Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals.\\\\n$1 unlocks unlimited opportunities\\\\nCoursera Footer\\\\nPopular AI Content\\\\nPopular Programs\\\\nPopular Skills\\\\nPopular Career Resources\\\\nCoursera\\\\nCommunity\\\\nMore You can do the same for text-to-image models like DALL-E.\\\\nWhy is it important in generative AI?\\\\nPrompt engineering is important for AI engineers to create better services, such as chatbots that handle customer service tasks or generate legal contracts. If you do need to include â€œtoneâ€ in your prompt, should you write â€œin a professional toneâ€ or â€œin a formal toneâ€?\\\\n Whether youâ€™re inputting prompts in ChatGPT to help you write your resume or using DALL-E to generate a photo for a presentation, anybody can be a prompt engineer.\\\\xa0\\\")] \\n    Graph Context: \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [5.73s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"There are several types of Prompt Engineering, including Zero-shot prompting, which involves asking the model to perform a task without any examples or prior training on that specific task. Other common techniques include Guided Prompting and Instructed Prompting, where the model is provided with guidance or instructions to complete a task. Additionally, there's also Meta-Prompting, which involves creating prompts for other prompts.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:22.359878Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 5721794417,\n",
      "          \"load_duration\": 35658750,\n",
      "          \"prompt_eval_count\": 698,\n",
      "          \"prompt_eval_duration\": 2886707000,\n",
      "          \"eval_count\": 81,\n",
      "          \"eval_duration\": 2797021000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"There are several types of Prompt Engineering, including Zero-shot prompting, which involves asking the model to perform a task without any examples or prior training on that specific task. Other common techniques include Guided Prompting and Instructed Prompting, where the model is provided with guidance or instructions to complete a task. Additionally, there's also Meta-Prompting, which involves creating prompts for other prompts.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:22.359878Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 5721794417,\n",
      "              \"load_duration\": 35658750,\n",
      "              \"prompt_eval_count\": 698,\n",
      "              \"prompt_eval_duration\": 2886707000,\n",
      "              \"eval_count\": 81,\n",
      "              \"eval_duration\": 2797021000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-aae870c3-b9da-4950-baa5-5cf8b673604d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 698,\n",
      "              \"output_tokens\": 81,\n",
      "              \"total_tokens\": 779\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"There are several types of Prompt Engineering, including Zero-shot prompting, which involves asking the model to perform a task without any examples or prior training on that specific task. Other common techniques include Guided Prompting and Instructed Prompting, where the model is provided with guidance or instructions to complete a task. Additionally, there's also Meta-Prompting, which involves creating prompts for other prompts.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [5.73s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"There are several types of Prompt Engineering, including Zero-shot prompting, which involves asking the model to perform a task without any examples or prior training on that specific task. Other common techniques include Guided Prompting and Instructed Prompting, where the model is provided with guidance or instructions to complete a task. Additionally, there's also Meta-Prompting, which involves creating prompts for other prompts.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents,graph_context>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"Types of prompt engineering. Prompt engineering is writing text to feed to an AI model. However, several forms of prompt engineering can impact your success. Zero-shot prompting. This technique involves asking the model to perform a task without any examples or prior training on that specific task. It tests the model's ability to generalize ...\\\\nBelow are some of the more common prompt engineering techniques used for a range of scenarios:\\\\nTypes of Prompt Engineering\\\\nHere are some of the main types of prompt engineering youâ€™ll most likely employ when using generative AI tools:\\\\nPrompt Engineering Best Practices\\\\nLetâ€™s go over some dos and donâ€™ts on writing a good prompt.\\\\n Key Elements of a Prompt\\\\nBefore diving into the creation of prompts, itâ€™s crucial to understand the building blocks of an effective prompt:\\\\nPrompt Engineering Techniques\\\\nThere are many ways to develop a prompt. Guide to Prompt Engineering\\\\nPrompt engineering is the process of optimizing the performance of\\\\xa0generative AI by tailoring the questions and processes to your specific needs.\\\\n Understanding the business problem and the types of personas that are needed to produce the desired output are of the utmost importance in a good prompt engineer.\\\\n Frequently Asked Questions\\\\nWhat is prompt engineering?\\\\nPrompt engineering is the practice of creating and tailoring input prompts or instructions to guide a language model to produce a desired response.\\\\nIntroduction. Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently apply and build with large language models (LLMs) for a wide variety of applications and use cases. Prompt engineering skills help to better understand the capabilities and limitations of LLMs. Researchers use prompt engineering ...\\\\nPrompt engineering is a technique used to effectively communicate with large language models (LLM) like GPT-3 or GPT-4 and get the desired output. Here there are applications of 8 distinct promptâ€¦\\\\nExamples of prompt engineering\\\\nHere are a few examples of prompt engineering to give you a better understanding of what it is and how you might engineer a prompt with a text and image model.\\\\n Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals.\\\\n$1 unlocks unlimited opportunities\\\\nCoursera Footer\\\\nPopular AI Content\\\\nPopular Programs\\\\nPopular Skills\\\\nPopular Career Resources\\\\nCoursera\\\\nCommunity\\\\nMore You can do the same for text-to-image models like DALL-E.\\\\nWhy is it important in generative AI?\\\\nPrompt engineering is important for AI engineers to create better services, such as chatbots that handle customer service tasks or generate legal contracts. If you do need to include â€œtoneâ€ in your prompt, should you write â€œin a professional toneâ€ or â€œin a formal toneâ€?\\\\n Whether youâ€™re inputting prompts in ChatGPT to help you write your resume or using DALL-E to generate a photo for a presentation, anybody can be a prompt engineer.\\\\xa0\\\")] \\n\\n    Here is the answer: \\n    There are several types of Prompt Engineering, including Zero-shot prompting, which involves asking the model to perform a task without any examples or prior training on that specific task. Other common techniques include Guided Prompting and Instructed Prompting, where the model is provided with guidance or instructions to complete a task. Additionally, there's also Meta-Prompting, which involves creating prompts for other prompts.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [3.23s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:25.595661Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3227405459,\n",
      "          \"load_duration\": 12009042,\n",
      "          \"prompt_eval_count\": 781,\n",
      "          \"prompt_eval_duration\": 3004935000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 209576000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:25.595661Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3227405459,\n",
      "              \"load_duration\": 12009042,\n",
      "              \"prompt_eval_count\": 781,\n",
      "              \"prompt_eval_duration\": 3004935000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 209576000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-89ec25e4-5351-409b-b892-db4d7d1cf1e1-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 781,\n",
      "              \"output_tokens\": 7,\n",
      "              \"total_tokens\": 788\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [3.23s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"generation\": \"There are several types of Prompt Engineering, including Zero-shot prompting, which involves asking the model to perform a task without any examples or prior training on that specific task. Other common techniques include Guided Prompting and Instructed Prompting, where the model is provided with guidance or instructions to complete a task. Additionally, there's also Meta-Prompting, which involves creating prompts for other prompts.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of Prompt Engineering?\",\n",
      "  \"generation\": \"There are several types of Prompt Engineering, including Zero-shot prompting, which involves asking the model to perform a task without any examples or prior training on that specific task. Other common techniques include Guided Prompting and Instructed Prompting, where the model is provided with guidance or instructions to complete a task. Additionally, there's also Meta-Prompting, which involves creating prompts for other prompts.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether an \\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \\n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n     \\n    Here is the answer:\\n    There are several types of Prompt Engineering, including Zero-shot prompting, which involves asking the model to perform a task without any examples or prior training on that specific task. Other common techniques include Guided Prompting and Instructed Prompting, where the model is provided with guidance or instructions to complete a task. Additionally, there's also Meta-Prompting, which involves creating prompts for other prompts. \\n\\n    Here is the question: What are the types of Prompt Engineering?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [925ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:26.524823Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 920757792,\n",
      "          \"load_duration\": 12883375,\n",
      "          \"prompt_eval_count\": 177,\n",
      "          \"prompt_eval_duration\": 713974000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 193097000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:26.524823Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 920757792,\n",
      "              \"load_duration\": 12883375,\n",
      "              \"prompt_eval_count\": 177,\n",
      "              \"prompt_eval_duration\": 713974000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 193097000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ea1538ca-10d9-4f35-a26f-43a3df7134f5-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 177,\n",
      "              \"output_tokens\": 7,\n",
      "              \"total_tokens\": 184\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [932ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [4.17s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"useful\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [9.90s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph] [16.40s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "('There are several types of Prompt Engineering, including Zero-shot '\n",
      " 'prompting, which involves asking the model to perform a task without any '\n",
      " 'examples or prior training on that specific task. Other common techniques '\n",
      " 'include Guided Prompting and Instructed Prompting, where the model is '\n",
      " 'provided with guidance or instructions to complete a task. Additionally, '\n",
      " \"there's also Meta-Prompting, which involves creating prompts for other \"\n",
      " 'prompts.')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What are the types of Prompt Engineering?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/8d449b67-6bc4-4ecf-9153-759cd21df24f/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents,graph_context>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "---ROUTE QUESTION---\n",
      "Did Emmanuel Macron visit Germany recently?\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a user question to the most appropriate data source. \\n    You have three options:\\n    1. 'vectorstore': Use for questions about LLM agents, prompt engineering, and adversarial attacks.\\n    2. 'graphrag': Use for questions that involve relationships between entities, such as authors, papers, and topics, or when the question requires understanding connections between concepts.\\n    3. 'web_search': Use for all other questions or when current information is needed.\\n\\n    You do not need to be stringent with the keywords in the question related to these topics. \\n    Choose the most appropriate option based on the nature of the question.\\n\\n    Return a JSON with a single key 'datasource' and no preamble or explanation. \\n    The value should be one of: 'vectorstore', 'graphrag', or 'web_search'.\\n    \\n    Question to route: \\n    Did Emmanuel Macron visit Germany recently?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] [1.15s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"web_search\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T07:53:48.378325Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1141196666,\n",
      "          \"load_duration\": 12223041,\n",
      "          \"prompt_eval_count\": 202,\n",
      "          \"prompt_eval_duration\": 867027000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 261124000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"web_search\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T07:53:48.378325Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1141196666,\n",
      "              \"load_duration\": 12223041,\n",
      "              \"prompt_eval_count\": 202,\n",
      "              \"prompt_eval_duration\": 867027000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 261124000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0a64736e-0b0b-4238-aff0-973a7dd3297e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"web_search\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] [1.15s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"web_search\"\n",
      "}\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] [1.15s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"websearch\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [1.15s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "---WEB SEARCH---\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > tool:tavily_search_results_json] Entering Tool run with input:\n",
      "\u001b[0m\"{'query': 'Did Emmanuel Macron visit Germany recently?'}\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > tool:tavily_search_results_json] [2.85s] Exiting Tool run with output:\n",
      "\u001b[0m\"[{'url': 'https://www.straitstimes.com/world/europe/macron-heads-to-germany-in-first-french-presidential-state-visit-in-24-years', 'content': 'PARIS/BERLIN - French President Emmanuel Macron arrived in Berlin on May 26 on the first state visit to Germany by a French president in a quarter of a century, seeking to ease recent tensions and ...'}, {'url': 'https://www.usnews.com/news/world/articles/2024-05-26/macron-heads-to-germany-in-first-french-presidential-state-visit-in-24-years', 'content': \"Macron's trip to the capital Berlin, Dresden in the east and Muenster in the west is the first French presidential state visit to Germany in 24 years. The visit will be watched as a checkup on the ...\"}, {'url': 'https://www.lemonde.fr/en/international/article/2024/05/26/macron-begins-first-state-visit-to-germany-by-a-french-president-in-24-years_6672731_4.html', 'content': 'President Emmanuel Macron on Sunday, May 26, started the first state visit to Germany by a French head of state in 24 years, a three-day trip meant to underline the strong ties between the ...'}, {'url': 'https://www.reuters.com/world/europe/macron-heads-germany-first-french-presidential-state-visit-24-years-2024-05-26/', 'content': \"French President Emmanuel Macron landed in Germany on Sunday for a three-day state visit followed by a bilateral cabinet meeting as the European Union's two biggest powers seek to show unity ahead ...\"}, {'url': 'https://www.independent.co.uk/news/emmanuel-macron-ap-germany-berlin-french-b2551771.html', 'content': 'Find out more. President Emmanuel Macron arrived in Germany Sunday for the first state visit by a French head of state in 24 years, a three-day trip meant to underline the strong ties between the ...'}]\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents,graph_context>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] [2.85s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: websearch:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Did Emmanuel Macron visit Germany recently? \\n    Context: [Document(page_content=\\\"PARIS/BERLIN - French President Emmanuel Macron arrived in Berlin on May 26 on the first state visit to Germany by a French president in a quarter of a century, seeking to ease recent tensions and ...\\\\nMacron's trip to the capital Berlin, Dresden in the east and Muenster in the west is the first French presidential state visit to Germany in 24 years. The visit will be watched as a checkup on the ...\\\\nPresident Emmanuel Macron on Sunday, May 26, started the first state visit to Germany by a French head of state in 24 years, a three-day trip meant to underline the strong ties between the ...\\\\nFrench President Emmanuel Macron landed in Germany on Sunday for a three-day state visit followed by a bilateral cabinet meeting as the European Union's two biggest powers seek to show unity ahead ...\\\\nFind out more. President Emmanuel Macron arrived in Germany Sunday for the first state visit by a French head of state in 24 years, a three-day trip meant to underline the strong ties between the ...\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOpenAI] [841ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, Emmanuel Macron visited Germany recently, arriving on May 26 for a three-day state visit.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, Emmanuel Macron visited Germany recently, arriving on May 26 for a three-day state visit.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 20,\n",
      "                \"prompt_tokens\": 295,\n",
      "                \"total_tokens\": 315,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-22096098-0957-44ec-99e1-d97635ecd621-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 295,\n",
      "              \"output_tokens\": 20,\n",
      "              \"total_tokens\": 315\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 20,\n",
      "      \"prompt_tokens\": 295,\n",
      "      \"total_tokens\": 315,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Yes, Emmanuel Macron visited Germany recently, arriving on May 26 for a three-day state visit.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [843ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Yes, Emmanuel Macron visited Germany recently, arriving on May 26 for a three-day state visit.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents,graph_context>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"PARIS/BERLIN - French President Emmanuel Macron arrived in Berlin on May 26 on the first state visit to Germany by a French president in a quarter of a century, seeking to ease recent tensions and ...\\\\nMacron's trip to the capital Berlin, Dresden in the east and Muenster in the west is the first French presidential state visit to Germany in 24 years. The visit will be watched as a checkup on the ...\\\\nPresident Emmanuel Macron on Sunday, May 26, started the first state visit to Germany by a French head of state in 24 years, a three-day trip meant to underline the strong ties between the ...\\\\nFrench President Emmanuel Macron landed in Germany on Sunday for a three-day state visit followed by a bilateral cabinet meeting as the European Union's two biggest powers seek to show unity ahead ...\\\\nFind out more. President Emmanuel Macron arrived in Germany Sunday for the first state visit by a French head of state in 24 years, a three-day trip meant to underline the strong ties between the ...\\\")] \\n\\n    Here is the answer: \\n    Yes, Emmanuel Macron visited Germany recently, arriving on May 26 for a three-day state visit.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [1.75s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T07:53:53.823282Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1737587500,\n",
      "          \"load_duration\": 40195542,\n",
      "          \"prompt_eval_count\": 336,\n",
      "          \"prompt_eval_duration\": 1495920000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 198463000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T07:53:53.823282Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1737587500,\n",
      "              \"load_duration\": 40195542,\n",
      "              \"prompt_eval_count\": 336,\n",
      "              \"prompt_eval_duration\": 1495920000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 198463000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-70d87e4f-0585-452b-a032-12b27297d186-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 336,\n",
      "              \"output_tokens\": 7,\n",
      "              \"total_tokens\": 343\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [1.75s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\",\n",
      "  \"generation\": \"Yes, Emmanuel Macron visited Germany recently, arriving on May 26 for a three-day state visit.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\",\n",
      "  \"generation\": \"Yes, Emmanuel Macron visited Germany recently, arriving on May 26 for a three-day state visit.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether an \\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \\n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n     \\n    Here is the answer:\\n    Yes, Emmanuel Macron visited Germany recently, arriving on May 26 for a three-day state visit. \\n\\n    Here is the question: Did Emmanuel Macron visit Germany recently?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [692ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T07:53:54.518487Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 687936875,\n",
      "          \"load_duration\": 11864417,\n",
      "          \"prompt_eval_count\": 116,\n",
      "          \"prompt_eval_duration\": 482178000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 193100000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T07:53:54.518487Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 687936875,\n",
      "              \"load_duration\": 11864417,\n",
      "              \"prompt_eval_count\": 116,\n",
      "              \"prompt_eval_duration\": 482178000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 193100000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-72b74f24-7728-4908-a5ca-dd3ee2ad2157-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 116,\n",
      "              \"output_tokens\": 7,\n",
      "              \"total_tokens\": 123\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [696ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.44s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"useful\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [3.29s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph] [7.29s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "('Yes, Emmanuel Macron visited Germany recently, arriving on May 26 for a '\n",
      " 'three-day state visit.')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Did Emmanuel Macron visit Germany recently?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents,graph_context>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "---ROUTE QUESTION---\n",
      "What paper talk about Multi-Agent?\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a user question to the most appropriate data source. \\n    You have three options:\\n    1. 'vectorstore': Use for questions about LLM agents, prompt engineering, and adversarial attacks.\\n    2. 'graphrag': Use for questions that involve relationships between entities, such as authors, papers, and topics, or when the question requires understanding connections between concepts.\\n    3. 'web_search': Use for all other questions or when current information is needed.\\n\\n    You do not need to be stringent with the keywords in the question related to these topics. \\n    Choose the most appropriate option based on the nature of the question.\\n\\n    Return a JSON with a single key 'datasource' and no preamble or explanation. \\n    The value should be one of: 'vectorstore', 'graphrag', or 'web_search'.\\n    \\n    Question to route: \\n    What paper talk about Multi-Agent?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] [1.13s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"graphrag\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:27.671046Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1121375042,\n",
      "          \"load_duration\": 13325792,\n",
      "          \"prompt_eval_count\": 202,\n",
      "          \"prompt_eval_duration\": 845329000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 261280000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"graphrag\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:27.671046Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1121375042,\n",
      "              \"load_duration\": 13325792,\n",
      "              \"prompt_eval_count\": 202,\n",
      "              \"prompt_eval_duration\": 845329000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 261280000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e6d1d0ef-764c-4d80-9781-ecbfa724e6d1-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"graphrag\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] [1.13s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"graphrag\"\n",
      "}\n",
      "{'datasource': 'graphrag'}\n",
      "graphrag\n",
      "---TRYING GRAPH SEARCH---\n",
      "---GRAPH SEARCH---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:GraphCypherQAChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:GraphCypherQAChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"schema\": \"Node properties are the following:\\nConcept {id: STRING, description: STRING, name: STRING},Technique {id: STRING},Agent {id: STRING},Profession {id: STRING},Publication {id: STRING, title: STRING},Technology {id: STRING, description: STRING},Person {id: STRING, description: STRING},Algorithm {id: STRING, field: STRING, description: STRING},Domain {id: STRING, description: STRING},Method {id: STRING, description: STRING},Node {id: STRING, name: STRING},Website {id: STRING},Domain-specific nlp task {id: STRING, description: STRING},Property {id: STRING},Topic {id: STRING, summary: STRING, title: STRING, url: STRING},Paper {id: STRING, title: STRING, summary: STRING, url: STRING},Author {id: STRING, title: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Concept)-[:RELATED_TO]->(:Concept),(:Concept)-[:RELATED_TO]->(:Agent),(:Concept)-[:RELATED_TO]->(:Technology),(:Concept)-[:RELATED_TO]->(:Website),(:Concept)-[:WROTE]->(:Technology),(:Concept)-[:APPLIED]->(:Algorithm),(:Concept)-[:INITIATED]->(:Algorithm),(:Concept)-[:ENHANCE]->(:Concept),(:Concept)-[:PRODUCE]->(:Concept),(:Concept)-[:ASSOCIATED_WITH]->(:Concept),(:Publication)-[:DESCRIBES]->(:Concept),(:Technology)-[:MENTIONS]->(:Technology),(:Technology)-[:RELATED_TO]->(:Domain),(:Technology)-[:RELATED_TO]->(:Concept),(:Technology)-[:RELATED_TO]->(:Algorithm),(:Technology)-[:USED_IN]->(:Method),(:Technology)-[:USED_FOR]->(:Algorithm),(:Person)-[:WROTE]->(:Technology),(:Person)-[:MENTIONS]->(:Concept),(:Person)-[:PROFESSOR]->(:Concept),(:Topic)-[:RELATED_TO]->(:Topic),(:Topic)-[:RELATED_TO]->(:Author),(:Topic)-[:RELATED_TO]->(:Paper),(:Topic)-[:DISCUSSES]->(:Topic),(:Topic)-[:DISCUSSES]->(:Paper),(:Topic)-[:AUTHORED]->(:Author),(:Paper)-[:DISCUSSES]->(:Topic),(:Paper)-[:RELATED_TO]->(:Topic),(:Paper)-[:RELATED_TO]->(:Paper),(:Author)-[:AUTHORED]->(:Paper),(:Author)-[:DISCUSSES]->(:Topic)\",\n",
      "  \"query\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:GraphCypherQAChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at generating Cypher queries for Neo4j.\\n    Use the following schema to generate a Cypher query that answers the given question.\\n    Make the query flexible by using case-insensitive matching and partial string matching where appropriate.\\n    Focus on searching paper titles as they contain the most relevant information.\\n    \\n    Schema:\\n    Node properties are the following:\\nConcept {id: STRING, description: STRING, name: STRING},Technique {id: STRING},Agent {id: STRING},Profession {id: STRING},Publication {id: STRING, title: STRING},Technology {id: STRING, description: STRING},Person {id: STRING, description: STRING},Algorithm {id: STRING, field: STRING, description: STRING},Domain {id: STRING, description: STRING},Method {id: STRING, description: STRING},Node {id: STRING, name: STRING},Website {id: STRING},Domain-specific nlp task {id: STRING, description: STRING},Property {id: STRING},Topic {id: STRING, summary: STRING, title: STRING, url: STRING},Paper {id: STRING, title: STRING, summary: STRING, url: STRING},Author {id: STRING, title: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Concept)-[:RELATED_TO]->(:Concept),(:Concept)-[:RELATED_TO]->(:Agent),(:Concept)-[:RELATED_TO]->(:Technology),(:Concept)-[:RELATED_TO]->(:Website),(:Concept)-[:WROTE]->(:Technology),(:Concept)-[:APPLIED]->(:Algorithm),(:Concept)-[:INITIATED]->(:Algorithm),(:Concept)-[:ENHANCE]->(:Concept),(:Concept)-[:PRODUCE]->(:Concept),(:Concept)-[:ASSOCIATED_WITH]->(:Concept),(:Publication)-[:DESCRIBES]->(:Concept),(:Technology)-[:MENTIONS]->(:Technology),(:Technology)-[:RELATED_TO]->(:Domain),(:Technology)-[:RELATED_TO]->(:Concept),(:Technology)-[:RELATED_TO]->(:Algorithm),(:Technology)-[:USED_IN]->(:Method),(:Technology)-[:USED_FOR]->(:Algorithm),(:Person)-[:WROTE]->(:Technology),(:Person)-[:MENTIONS]->(:Concept),(:Person)-[:PROFESSOR]->(:Concept),(:Topic)-[:RELATED_TO]->(:Topic),(:Topic)-[:RELATED_TO]->(:Author),(:Topic)-[:RELATED_TO]->(:Paper),(:Topic)-[:DISCUSSES]->(:Topic),(:Topic)-[:DISCUSSES]->(:Paper),(:Topic)-[:AUTHORED]->(:Author),(:Paper)-[:DISCUSSES]->(:Topic),(:Paper)-[:RELATED_TO]->(:Topic),(:Paper)-[:RELATED_TO]->(:Paper),(:Author)-[:AUTHORED]->(:Paper),(:Author)-[:DISCUSSES]->(:Topic)\\n    \\n    Question: What paper talk about Multi-Agent?\\n    \\n    Cypher Query:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:GraphCypherQAChain > chain:LLMChain > llm:ChatOpenAI] [5.05s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Sure, here is a Cypher query that searches for papers that talk about \\\"Multi-Agent\\\" using case-insensitive and partial string matching:\\n\\n```cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n```\\n\\nThis query will return the titles, summaries, and URLs of papers that mention \\\"Multi-Agent\\\" in their titles, regardless of case.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Sure, here is a Cypher query that searches for papers that talk about \\\"Multi-Agent\\\" using case-insensitive and partial string matching:\\n\\n```cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n```\\n\\nThis query will return the titles, summaries, and URLs of papers that mention \\\"Multi-Agent\\\" in their titles, regardless of case.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 99,\n",
      "                \"prompt_tokens\": 615,\n",
      "                \"total_tokens\": 714,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ab7dd48e-d5b8-4365-a6f9-548623408483-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 615,\n",
      "              \"output_tokens\": 99,\n",
      "              \"total_tokens\": 714\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 99,\n",
      "      \"prompt_tokens\": 615,\n",
      "      \"total_tokens\": 714,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:GraphCypherQAChain > chain:LLMChain] [5.05s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Sure, here is a Cypher query that searches for papers that talk about \\\"Multi-Agent\\\" using case-insensitive and partial string matching:\\n\\n```cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n```\\n\\nThis query will return the titles, summaries, and URLs of papers that mention \\\"Multi-Agent\\\" in their titles, regardless of case.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:GraphCypherQAChain] [5.17s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ],\n",
      "  \"intermediate_steps\": [\n",
      "    {\n",
      "      \"query\": \"cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] [6.30s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"graphrag\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [6.30s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "---GRAPH SEARCH---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag > chain:GraphCypherQAChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag > chain:GraphCypherQAChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"schema\": \"Node properties are the following:\\nConcept {id: STRING, description: STRING, name: STRING},Technique {id: STRING},Agent {id: STRING},Profession {id: STRING},Publication {id: STRING, title: STRING},Technology {id: STRING, description: STRING},Person {id: STRING, description: STRING},Algorithm {id: STRING, field: STRING, description: STRING},Domain {id: STRING, description: STRING},Method {id: STRING, description: STRING},Node {id: STRING, name: STRING},Website {id: STRING},Domain-specific nlp task {id: STRING, description: STRING},Property {id: STRING},Topic {id: STRING, summary: STRING, title: STRING, url: STRING},Paper {id: STRING, title: STRING, summary: STRING, url: STRING},Author {id: STRING, title: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Concept)-[:RELATED_TO]->(:Concept),(:Concept)-[:RELATED_TO]->(:Agent),(:Concept)-[:RELATED_TO]->(:Technology),(:Concept)-[:RELATED_TO]->(:Website),(:Concept)-[:WROTE]->(:Technology),(:Concept)-[:APPLIED]->(:Algorithm),(:Concept)-[:INITIATED]->(:Algorithm),(:Concept)-[:ENHANCE]->(:Concept),(:Concept)-[:PRODUCE]->(:Concept),(:Concept)-[:ASSOCIATED_WITH]->(:Concept),(:Publication)-[:DESCRIBES]->(:Concept),(:Technology)-[:MENTIONS]->(:Technology),(:Technology)-[:RELATED_TO]->(:Domain),(:Technology)-[:RELATED_TO]->(:Concept),(:Technology)-[:RELATED_TO]->(:Algorithm),(:Technology)-[:USED_IN]->(:Method),(:Technology)-[:USED_FOR]->(:Algorithm),(:Person)-[:WROTE]->(:Technology),(:Person)-[:MENTIONS]->(:Concept),(:Person)-[:PROFESSOR]->(:Concept),(:Topic)-[:RELATED_TO]->(:Topic),(:Topic)-[:RELATED_TO]->(:Author),(:Topic)-[:RELATED_TO]->(:Paper),(:Topic)-[:DISCUSSES]->(:Topic),(:Topic)-[:DISCUSSES]->(:Paper),(:Topic)-[:AUTHORED]->(:Author),(:Paper)-[:DISCUSSES]->(:Topic),(:Paper)-[:RELATED_TO]->(:Topic),(:Paper)-[:RELATED_TO]->(:Paper),(:Author)-[:AUTHORED]->(:Paper),(:Author)-[:DISCUSSES]->(:Topic)\",\n",
      "  \"query\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag > chain:GraphCypherQAChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at generating Cypher queries for Neo4j.\\n    Use the following schema to generate a Cypher query that answers the given question.\\n    Make the query flexible by using case-insensitive matching and partial string matching where appropriate.\\n    Focus on searching paper titles as they contain the most relevant information.\\n    \\n    Schema:\\n    Node properties are the following:\\nConcept {id: STRING, description: STRING, name: STRING},Technique {id: STRING},Agent {id: STRING},Profession {id: STRING},Publication {id: STRING, title: STRING},Technology {id: STRING, description: STRING},Person {id: STRING, description: STRING},Algorithm {id: STRING, field: STRING, description: STRING},Domain {id: STRING, description: STRING},Method {id: STRING, description: STRING},Node {id: STRING, name: STRING},Website {id: STRING},Domain-specific nlp task {id: STRING, description: STRING},Property {id: STRING},Topic {id: STRING, summary: STRING, title: STRING, url: STRING},Paper {id: STRING, title: STRING, summary: STRING, url: STRING},Author {id: STRING, title: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Concept)-[:RELATED_TO]->(:Concept),(:Concept)-[:RELATED_TO]->(:Agent),(:Concept)-[:RELATED_TO]->(:Technology),(:Concept)-[:RELATED_TO]->(:Website),(:Concept)-[:WROTE]->(:Technology),(:Concept)-[:APPLIED]->(:Algorithm),(:Concept)-[:INITIATED]->(:Algorithm),(:Concept)-[:ENHANCE]->(:Concept),(:Concept)-[:PRODUCE]->(:Concept),(:Concept)-[:ASSOCIATED_WITH]->(:Concept),(:Publication)-[:DESCRIBES]->(:Concept),(:Technology)-[:MENTIONS]->(:Technology),(:Technology)-[:RELATED_TO]->(:Domain),(:Technology)-[:RELATED_TO]->(:Concept),(:Technology)-[:RELATED_TO]->(:Algorithm),(:Technology)-[:USED_IN]->(:Method),(:Technology)-[:USED_FOR]->(:Algorithm),(:Person)-[:WROTE]->(:Technology),(:Person)-[:MENTIONS]->(:Concept),(:Person)-[:PROFESSOR]->(:Concept),(:Topic)-[:RELATED_TO]->(:Topic),(:Topic)-[:RELATED_TO]->(:Author),(:Topic)-[:RELATED_TO]->(:Paper),(:Topic)-[:DISCUSSES]->(:Topic),(:Topic)-[:DISCUSSES]->(:Paper),(:Topic)-[:AUTHORED]->(:Author),(:Paper)-[:DISCUSSES]->(:Topic),(:Paper)-[:RELATED_TO]->(:Topic),(:Paper)-[:RELATED_TO]->(:Paper),(:Author)-[:AUTHORED]->(:Paper),(:Author)-[:DISCUSSES]->(:Topic)\\n    \\n    Question: What paper talk about Multi-Agent?\\n    \\n    Cypher Query:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag > chain:GraphCypherQAChain > chain:LLMChain > llm:ChatOpenAI] [1.69s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Sure, here is a Cypher query that searches for papers that talk about \\\"Multi-Agent\\\" using case-insensitive and partial string matching:\\n\\n```cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n```\\n\\nThis query will return the titles, summaries, and URLs of papers that mention \\\"Multi-Agent\\\" in their titles, regardless of case.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Sure, here is a Cypher query that searches for papers that talk about \\\"Multi-Agent\\\" using case-insensitive and partial string matching:\\n\\n```cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n```\\n\\nThis query will return the titles, summaries, and URLs of papers that mention \\\"Multi-Agent\\\" in their titles, regardless of case.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 99,\n",
      "                \"prompt_tokens\": 615,\n",
      "                \"total_tokens\": 714,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_25624ae3a5\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5e12c74d-28f6-4dfd-90b1-78119e4c8c33-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 615,\n",
      "              \"output_tokens\": 99,\n",
      "              \"total_tokens\": 714\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 99,\n",
      "      \"prompt_tokens\": 615,\n",
      "      \"total_tokens\": 714,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_25624ae3a5\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag > chain:GraphCypherQAChain > chain:LLMChain] [1.69s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Sure, here is a Cypher query that searches for papers that talk about \\\"Multi-Agent\\\" using case-insensitive and partial string matching:\\n\\n```cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n```\\n\\nThis query will return the titles, summaries, and URLs of papers that mention \\\"Multi-Agent\\\" in their titles, regardless of case.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag > chain:GraphCypherQAChain] [1.73s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ],\n",
      "  \"intermediate_steps\": [\n",
      "    {\n",
      "      \"query\": \"cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\\\"multi-agent\\\")\\nRETURN p.title AS PaperTitle, p.summary AS Summary, p.url AS URL\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag > chain:ChannelWrite<graphrag,question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ],\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag > chain:ChannelWrite<graphrag,question,generation,web_search,documents,graph_context>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ],\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:graphrag] [1.73s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ],\n",
      "  \"question\": \"What paper talk about Multi-Agent?\"\n",
      "}\n",
      "'Finished running: graphrag:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"context\": [],\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"context\": [],\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context from a vector store and a graph database to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What paper talk about Multi-Agent? \\n    Vector Context: [] \\n    Graph Context: [{'PaperTitle': 'Multi-Agent Assistant Code Generation (AgentCoder)', 'Summary': None, 'URL': None}, {'PaperTitle': 'Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)', 'Summary': None, 'URL': None}, {'PaperTitle': 'Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework', 'Summary': 'In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.', 'URL': 'https://github.com/amazon-science/comm-prompt'}]\\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.58s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:38.155443Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3577023708,\n",
      "          \"load_duration\": 39996625,\n",
      "          \"prompt_eval_count\": 338,\n",
      "          \"prompt_eval_duration\": 1514809000,\n",
      "          \"eval_count\": 62,\n",
      "          \"eval_duration\": 2020694000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:38.155443Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3577023708,\n",
      "              \"load_duration\": 39996625,\n",
      "              \"prompt_eval_count\": 338,\n",
      "              \"prompt_eval_duration\": 1514809000,\n",
      "              \"eval_count\": 62,\n",
      "              \"eval_duration\": 2020694000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c6b5cf48-0bd7-49a5-9faa-6dea1cc9a9b6-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 338,\n",
      "              \"output_tokens\": 62,\n",
      "              \"total_tokens\": 400\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.59s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents,graph_context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"generation\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\",\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents,graph_context>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"generation\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\",\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"generation\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\",\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"generation\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"generation\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [] \\n\\n    Here is the answer: \\n    The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [815ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:38.971991Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 809643500,\n",
      "          \"load_duration\": 12192542,\n",
      "          \"prompt_eval_count\": 165,\n",
      "          \"prompt_eval_duration\": 603775000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 192242000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:38.971991Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 809643500,\n",
      "              \"load_duration\": 12192542,\n",
      "              \"prompt_eval_count\": 165,\n",
      "              \"prompt_eval_duration\": 603775000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 192242000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e6d664ca-55db-474f-a85d-f41163e4fd79-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 165,\n",
      "              \"output_tokens\": 7,\n",
      "              \"total_tokens\": 172\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [816ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"generation\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"generation\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether an \\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \\n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n     \\n    Here is the answer:\\n    The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\". \\n\\n    Here is the question: What paper talk about Multi-Agent?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [808ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2024-09-13T08:23:39.784983Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 806677875,\n",
      "          \"load_duration\": 11877833,\n",
      "          \"prompt_eval_count\": 157,\n",
      "          \"prompt_eval_duration\": 600102000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 193857000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2024-09-13T08:23:39.784983Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 806677875,\n",
      "              \"load_duration\": 11877833,\n",
      "              \"prompt_eval_count\": 157,\n",
      "              \"prompt_eval_duration\": 600102000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 193857000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f9298b2d-774e-4c38-b644-1ea5b9998427-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 157,\n",
      "              \"output_tokens\": 7,\n",
      "              \"total_tokens\": 164\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [809ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [1.63s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"useful\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [5.21s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"documents\": [],\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"generation\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\",\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "'Finished running: generate:'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph] [13.24s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What paper talk about Multi-Agent?\",\n",
      "  \"generation\": \"The papers that talk about Multi-Agent are \\\"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\\\" and possibly others like \\\"Multi-Agent Assistant Code Generation (AgentCoder)\\\" and \\\"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\\\".\",\n",
      "  \"documents\": [],\n",
      "  \"graph_context\": [\n",
      "    {\n",
      "      \"PaperTitle\": \"Multi-Agent Assistant Code Generation (AgentCoder)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)\",\n",
      "      \"Summary\": null,\n",
      "      \"URL\": null\n",
      "    },\n",
      "    {\n",
      "      \"PaperTitle\": \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework\",\n",
      "      \"Summary\": \"In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.\",\n",
      "      \"URL\": \"https://github.com/amazon-science/comm-prompt\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "('The papers that talk about Multi-Agent are \"Collaborative Multi-Agent, '\n",
      " 'Multi-Reasoning-Path (CoMM) Prompting Framework\" and possibly others like '\n",
      " '\"Multi-Agent Assistant Code Generation (AgentCoder)\" and \"Framework for '\n",
      " 'Automatically Generating Process Models with Multi-Agent Orchestration '\n",
      " '(MAO)\".')\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What paper talk about Multi-Agent?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
